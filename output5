Generating LSTDQ demonstrations...
2182 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.178	0.257	0.949	-0.019	0.085	0.193	0.118	-0.236	-1.378	-0.158	0.104	-0.498	-0.007	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.063	0.336	1.051	0.013	0.038	0.203	-0.167	-0.315	-1.136	-0.089	0.097	-0.455	-0.037	
Sampling 20 reward functions...
	Sampled reward functions:
	 (0)	0.002	-0.096	0.736	-0.376	0.011	0.212	-0.014	-0.069	-1.085	0.361	0.029	-0.108	-0.006	
	 (1)	0.059	-0.039	0.982	-0.132	0.027	0.100	0.014	-0.036	-1.191	0.180	-0.032	1.020	0.023	
	 (2)	0.148	-0.176	1.330	-0.222	-0.097	0.440	-0.100	0.234	-0.710	0.147	0.011	0.511	-0.024	
	 (3)	-0.306	0.088	0.483	0.649	0.114	-1.392	0.196	-0.236	-1.243	-0.306	0.037	3.433	-0.010	
	 (4)	-0.061	0.111	0.770	-0.067	-0.045	0.146	-0.014	-0.010	-0.840	0.141	0.036	-2.637	0.016	
	 (5)	-0.206	-0.166	1.048	0.039	0.086	1.217	0.246	0.044	-0.746	-0.179	-0.117	-0.734	0.083	
	 (6)	-0.185	-0.150	0.671	0.107	0.246	0.296	0.308	0.355	-0.882	-0.640	-0.304	-4.447	0.202	
	 (7)	0.001	0.152	1.055	0.115	-0.182	0.372	-0.025	-0.294	-1.471	-0.223	0.335	-1.461	0.035	
	 (8)	0.533	0.835	1.969	-0.306	-0.420	0.137	-0.510	-1.044	-1.321	0.164	0.444	6.086	-0.762	
	 (9)	-0.142	-0.260	1.092	0.217	0.121	-0.352	0.139	-0.005	-0.634	0.240	-0.180	2.492	0.278	
	 (10)	-0.014	0.010	1.162	0.181	-0.022	0.486	0.118	-0.162	-0.845	-0.280	0.050	-0.133	-0.032	
	 (11)	-0.052	0.038	1.482	-0.105	-0.162	0.471	0.014	0.194	-0.709	0.127	0.164	0.159	-0.050	
	 (12)	0.127	-0.265	0.884	-0.062	0.014	-0.153	-0.144	0.241	-0.882	0.192	0.040	2.524	0.043	
	 (13)	0.019	0.262	1.216	-0.067	-0.142	-0.573	0.023	0.157	-0.970	0.202	-0.080	-0.727	0.170	
	 (14)	0.299	0.308	2.445	-0.268	-0.165	-0.066	-0.220	-0.104	-1.379	-0.059	0.132	-0.148	-0.451	
	 (15)	0.088	-0.073	1.907	0.247	0.008	0.504	-0.242	-0.929	-1.049	0.571	0.062	3.679	-0.063	
	 (16)	-0.723	-1.178	0.636	0.175	0.941	0.669	0.534	0.076	-0.501	0.648	-0.353	1.834	1.264	
	 (17)	-0.205	-0.229	0.925	0.174	0.100	0.294	0.113	0.118	-0.204	-0.095	-0.121	1.071	0.244	
	 (18)	0.064	-0.133	1.414	-0.055	-0.123	-0.409	-0.058	0.131	-1.138	-0.259	0.196	-0.178	-0.053	
	 (19)	-0.167	-0.242	1.271	0.463	0.034	-0.060	0.257	1.281	-0.330	-0.835	-0.133	-4.173	0.158	
LSPI converged after 5 iterations in 3.190 seconds.
Creating 20 corresponding optimal policies...
 in 3.420 seconds.
 in 3.420 seconds.
 in 3.400 seconds.
 in 2.840 seconds.
 in 3.410 seconds.
 in 3.410 seconds.
 in 3.400 seconds.
 in 3.500 seconds.
 in 3.470 seconds.
 in 3.510 seconds.
 in 3.460 seconds.
 in 3.450 seconds.
 in 4.010 seconds.
 in 3.410 seconds.
 in 3.410 seconds.
 in 3.500 seconds.
 in 2.840 seconds.
 in 2.910 seconds.
 in 2.840 seconds.
 in 3.470 seconds.
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.970 }	0.213	0.300	0.943	-0.029	0.084	0.316	0.325	-0.205	-1.526	-0.356	0.107	-0.770	-0.008	
	{ -0.133 }	 (random)
	{ -0.134 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.503 }	0.021	0.154	0.565	-0.348	0.062	0.103	0.410	-0.237	-1.675	-0.137	0.154	-1.032	-0.034	
 (1)	{ 0.917 }	0.289	0.202	0.742	-0.304	0.124	0.189	0.501	-0.240	-1.927	-0.354	0.083	-0.497	0.022	
 (2)	{ 0.942 }	0.335	0.260	1.228	-0.323	-0.103	0.346	0.380	0.040	-1.511	-0.231	0.160	-0.487	-0.049	
 (3)	{ 0.493 }	0.034	0.060	0.091	0.335	0.276	-0.993	0.581	-0.412	-2.017	-0.660	0.215	1.587	-0.087	
 (4)	{ 0.867 }	0.073	0.433	0.658	0.027	-0.041	0.173	0.231	-0.237	-1.344	-0.256	0.157	-2.493	0.021	
 (5)	{ 0.813 }	0.031	0.232	0.769	0.121	0.258	1.229	0.854	-0.300	-1.554	-1.001	-0.078	-1.705	0.133	
 (6)	{ 0.797 }	0.150	-0.029	0.297	-0.022	0.554	0.622	0.891	-0.149	-1.510	-1.587	-0.362	-4.392	0.305	
 (7)	{ 0.883 }	0.414	0.517	0.772	0.010	-0.256	0.494	0.477	-0.729	-2.553	-0.965	0.744	-2.089	0.035	
 (8)	{ 0.589 }	1.079	1.225	1.841	-0.732	-0.570	-0.068	0.323	-1.137	-2.990	-0.205	0.962	3.759	-1.272	
 (9)	{ 0.841 }	0.107	-0.049	0.752	0.038	0.297	0.010	0.539	-0.086	-1.243	-0.190	-0.159	0.836	0.430	
 (10)	{ 0.783 }	0.331	0.292	0.881	-0.026	0.065	0.587	0.784	-0.471	-1.797	-1.168	0.235	-1.191	-0.066	
 (11)	{ 0.614 }	0.241	0.499	1.314	-0.254	-0.224	0.415	0.853	0.000	-1.889	-0.605	0.453	-0.860	-0.083	
 (12)	{ 0.891 }	0.363	-0.096	0.627	-0.262	0.073	-0.044	0.348	0.094	-1.704	-0.092	0.229	0.982	0.029	
 (13)	{ 0.826 }	0.211	0.524	1.197	-0.216	-0.180	-0.302	0.624	-0.023	-1.461	-0.443	-0.034	-1.718	0.351	
 (14)	{ 0.825 }	0.816	1.048	2.080	-0.523	-0.073	-0.044	0.561	-0.567	-3.175	-0.868	0.535	-1.469	-0.845	
 (15)	{ 0.862 }	0.508	0.548	1.465	0.154	0.123	0.671	0.050	-1.088	-2.249	0.089	0.334	1.741	-0.169	
 (16)	{ 0.730 }	0.209	-1.007	-1.039	-0.162	1.734	1.670	1.666	-0.248	-2.490	-1.035	-0.142	-1.382	1.829	
 (17)	{ 0.638 }	0.007	0.082	0.618	0.152	0.250	0.546	0.360	-0.026	-0.796	-0.341	-0.076	0.225	0.367	
 (18)	{ 0.864 }	0.311	0.418	1.184	-0.153	-0.158	-0.311	0.318	-0.203	-2.246	-0.582	0.544	-0.982	-0.157	
 (19)	{ 0.828 }	0.352	-0.015	0.948	0.079	0.222	0.177	1.403	0.851	-1.557	-1.884	-0.045	-4.550	0.226	
Scores for the experts true policies:
	{ 0.948 }	 (expert 0)
	{ 0.945 }	 (expert 1)
	{ 0.945 }	 (expert 2)
	{ 0.598 }	 (expert 3)
Summed loss of reward-optimal policies w.r.t. true rho values rho_m == MDP_m == V_m == (expert weights)
	0.452	0.511	0.536	0.910	0.638	0.875	1.490	1.040	2.265	0.603	0.842	1.266	0.918	0.692	1.573	0.921	2.959	0.386	0.626	1.873	
	0.544	0.543	0.528	0.790	0.788	0.986	1.619	1.128	2.184	0.534	0.905	1.276	0.813	0.786	1.592	0.880	2.995	0.356	0.627	1.929	
	0.595	0.905	0.909	1.193	0.711	1.149	1.583	1.316	2.574	0.962	1.202	1.626	1.249	0.965	1.938	1.271	3.311	0.678	0.951	2.216	
	0.570	0.606	0.579	0.831	0.780	1.036	1.586	1.153	2.266	0.615	0.943	1.340	0.844	0.818	1.646	0.999	3.047	0.464	0.649	1.910	
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.257	0.462	0.441	0.525	0.264	0.732	0.786	0.870	1.139	0.524	0.849	0.685	0.438	0.532	0.784	0.818	1.432	0.313	0.463	1.199	
0.318	0.480	0.470	0.482	0.245	0.773	0.836	0.818	1.038	0.550	0.872	0.643	0.458	0.560	0.796	0.765	1.391	0.348	0.448	1.267	
0.299	0.488	0.475	0.468	0.239	0.821	0.866	0.833	1.085	0.539	0.887	0.647	0.439	0.546	0.806	0.807	1.435	0.369	0.453	1.244	
0.310	0.444	0.434	0.497	0.235	0.732	0.807	0.856	0.995	0.477	0.830	0.634	0.424	0.526	0.734	0.666	1.392	0.325	0.442	1.227	
0.297	0.472	0.463	0.474	0.244	0.748	0.838	0.841	1.021	0.506	0.861	0.640	0.448	0.559	0.779	0.713	1.454	0.333	0.480	1.259	
0.271	0.476	0.466	0.520	0.235	0.742	0.824	0.873	1.077	0.511	0.865	0.651	0.447	0.554	0.791	0.728	1.393	0.340	0.455	1.256	
0.300	0.498	0.496	0.572	0.258	0.788	0.857	0.827	1.079	0.562	0.896	0.651	0.462	0.568	0.825	0.799	1.400	0.371	0.441	1.274	
0.303	0.475	0.461	0.503	0.242	0.784	0.852	0.844	1.055	0.546	0.869	0.652	0.438	0.541	0.787	0.788	1.437	0.371	0.441	1.252	
0.286	0.495	0.474	0.537	0.238	0.782	0.859	0.854	1.070	0.553	0.899	0.650	0.447	0.559	0.816	0.806	1.480	0.361	0.452	1.240	
0.306	0.479	0.476	0.480	0.249	0.788	0.850	0.831	1.019	0.549	0.862	0.638	0.454	0.539	0.785	0.767	1.404	0.362	0.450	1.245	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.116	0.005	0.007	0.004	0.796	0.000	0.000	0.000	0.000	0.003	0.000	0.001	0.010	0.002	0.000	0.000	0.000	0.047	0.008	0.000	
LSPI converged after 5 iterations in 3.410 seconds.
MEAN LOSS: 0.608
MEAN RF POLICY* IN MDP_m (loss): 0.522

Loss matrix
0.358	0.381	0.363	0.752	0.184	0.555	0.522	0.899	1.467	0.429	0.657	0.763	0.572	0.503	0.670	0.719	1.699	0.269	0.506	1.074	
0.429	0.465	0.608	0.741	0.303	0.650	0.725	1.023	1.309	0.684	0.742	0.900	0.722	0.782	0.875	0.827	1.958	0.579	0.704	1.390	
0.428	0.469	0.586	0.763	0.297	0.648	0.733	0.968	1.278	0.676	0.739	0.886	0.715	0.756	0.848	0.786	1.963	0.557	0.664	1.399	
0.429	0.457	0.569	0.763	0.307	0.647	0.765	1.023	1.245	0.707	0.732	0.877	0.704	0.786	0.805	0.858	2.061	0.591	0.679	1.398	
0.431	0.479	0.611	0.783	0.295	0.630	0.757	1.041	1.348	0.722	0.750	0.891	0.748	0.800	0.899	0.888	1.934	0.557	0.725	1.407	
0.436	0.461	0.618	0.764	0.306	0.614	0.759	1.046	1.256	0.719	0.728	0.894	0.749	0.824	0.834	0.858	1.943	0.587	0.739	1.418	
0.423	0.475	0.636	0.756	0.286	0.647	0.730	0.956	1.249	0.757	0.743	0.882	0.758	0.807	0.864	0.856	2.005	0.621	0.717	1.428	
0.444	0.460	0.635	0.733	0.319	0.618	0.734	1.080	1.290	0.710	0.725	0.926	0.751	0.828	0.873	0.845	1.995	0.594	0.761	1.434	
0.447	0.489	0.617	0.788	0.337	0.676	0.755	1.036	1.298	0.736	0.764	0.921	0.734	0.820	0.878	0.905	2.047	0.615	0.703	1.451	
0.450	0.470	0.614	0.775	0.323	0.612	0.762	1.073	1.270	0.726	0.732	0.910	0.750	0.848	0.845	0.846	1.998	0.589	0.748	1.412	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.036	0.020	0.005	0.000	0.881	0.002	0.001	0.000	0.000	0.002	0.000	0.000	0.001	0.001	0.000	0.000	0.000	0.051	0.001	0.000	
LSPI converged after 5 iterations in 3.490 seconds.
MEAN LOSS: 0.752
MEAN RF POLICY* IN MDP_m (loss): 0.716

Loss matrix
0.405	0.388	0.378	0.670	0.226	0.529	0.466	0.829	1.457	0.482	0.586	0.765	0.575	0.485	0.628	0.773	1.885	0.346	0.408	0.959	
0.465	0.475	0.682	0.582	0.334	0.683	0.600	0.995	1.293	0.744	0.681	0.980	0.744	0.863	0.751	0.879	2.045	0.665	0.692	1.200	
0.469	0.465	0.678	0.552	0.329	0.667	0.573	0.959	1.281	0.702	0.658	0.986	0.738	0.835	0.745	0.827	1.895	0.623	0.673	1.196	
0.487	0.476	0.712	0.549	0.343	0.679	0.570	1.050	1.369	0.720	0.691	1.025	0.757	0.869	0.788	0.913	1.879	0.636	0.738	1.204	
0.469	0.458	0.651	0.554	0.338	0.676	0.611	1.003	1.242	0.707	0.653	0.967	0.722	0.838	0.701	0.841	2.115	0.656	0.663	1.202	
0.477	0.496	0.685	0.621	0.319	0.666	0.597	0.976	1.389	0.722	0.705	0.982	0.762	0.822	0.844	0.902	1.941	0.611	0.716	1.215	
0.469	0.470	0.670	0.575	0.334	0.669	0.584	0.980	1.318	0.703	0.672	0.982	0.732	0.843	0.762	0.852	1.946	0.631	0.673	1.194	
0.481	0.480	0.639	0.591	0.334	0.690	0.584	1.003	1.340	0.715	0.687	0.974	0.711	0.813	0.745	0.917	2.122	0.635	0.645	1.158	
0.486	0.477	0.664	0.561	0.347	0.701	0.603	1.023	1.307	0.715	0.683	0.992	0.723	0.847	0.743	0.886	2.059	0.647	0.670	1.160	
0.468	0.479	0.637	0.614	0.316	0.665	0.597	0.960	1.332	0.696	0.684	0.956	0.723	0.796	0.776	0.851	2.036	0.597	0.655	1.186	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.018	0.020	0.005	0.004	0.937	0.001	0.003	0.000	0.000	0.001	0.001	0.000	0.000	0.001	0.000	0.000	0.000	0.006	0.002	0.000	
LSPI converged after 5 iterations in 3.460 seconds.
MEAN LOSS: 0.721
MEAN RF POLICY* IN MDP_m (loss): 0.688

Loss matrix
0.582	0.554	0.581	0.655	0.502	0.888	0.765	1.075	1.615	0.588	0.780	0.997	0.551	0.807	0.972	0.921	1.564	0.419	0.544	0.939	
0.786	0.892	0.765	0.979	0.664	1.150	1.212	1.263	1.658	0.787	1.007	1.056	0.780	1.013	1.315	1.048	1.679	0.467	0.748	1.265	
0.877	0.997	0.795	1.050	0.738	1.284	1.374	1.307	1.726	0.834	1.075	1.087	0.800	1.064	1.447	1.119	1.754	0.486	0.758	1.300	
0.817	0.933	0.756	0.990	0.679	1.183	1.275	1.265	1.678	0.805	1.029	1.051	0.779	1.017	1.347	1.066	1.726	0.466	0.733	1.285	
0.799	0.901	0.723	0.991	0.664	1.143	1.213	1.294	1.656	0.784	1.003	1.046	0.770	0.987	1.272	1.026	1.756	0.448	0.721	1.246	
0.825	0.959	0.747	1.095	0.668	1.219	1.338	1.196	1.660	0.834	1.024	1.030	0.788	1.002	1.378	1.056	1.770	0.472	0.711	1.286	
0.854	0.978	0.766	1.074	0.713	1.252	1.371	1.311	1.705	0.824	1.056	1.069	0.801	1.040	1.406	1.085	1.794	0.474	0.757	1.284	
0.783	0.904	0.721	1.026	0.657	1.150	1.253	1.276	1.645	0.792	1.001	1.025	0.770	0.997	1.285	1.027	1.752	0.450	0.721	1.246	
0.834	0.951	0.762	1.069	0.689	1.195	1.294	1.264	1.696	0.793	1.037	1.061	0.804	1.012	1.387	1.029	1.669	0.446	0.764	1.300	
0.787	0.910	0.714	1.058	0.638	1.148	1.275	1.219	1.633	0.775	0.991	1.014	0.778	0.947	1.321	0.972	1.692	0.428	0.719	1.267	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.001	0.001	0.001	0.000	0.006	0.000	0.000	0.000	0.000	0.001	0.000	0.000	0.001	0.000	0.000	0.000	0.000	0.987	0.002	0.000	
LSPI converged after 4 iterations in 2.890 seconds.
MEAN LOSS: 0.467
MEAN RF POLICY* IN MDP_m (loss): 0.459

Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.246	0.441	0.406	0.500	0.184	0.650	0.751	0.740	0.918	0.449	0.762	0.605	0.397	0.511	0.740	0.581	1.251	0.290	0.412	1.165	
0.262	0.448	0.417	0.496	0.180	0.664	0.768	0.681	0.893	0.470	0.754	0.584	0.409	0.510	0.748	0.576	1.224	0.308	0.411	1.165	
0.260	0.456	0.418	0.502	0.178	0.669	0.769	0.696	0.918	0.466	0.770	0.589	0.403	0.519	0.760	0.596	1.218	0.308	0.405	1.166	
0.256	0.456	0.430	0.503	0.180	0.672	0.773	0.689	0.910	0.474	0.765	0.593	0.410	0.523	0.771	0.587	1.214	0.314	0.408	1.181	
0.256	0.455	0.422	0.511	0.182	0.670	0.778	0.692	0.903	0.477	0.767	0.590	0.407	0.522	0.763	0.587	1.213	0.315	0.412	1.176	
0.257	0.454	0.421	0.502	0.175	0.669	0.776	0.687	0.903	0.474	0.765	0.584	0.405	0.519	0.760	0.589	1.223	0.313	0.400	1.173	
0.256	0.452	0.423	0.499	0.180	0.679	0.780	0.684	0.900	0.471	0.762	0.592	0.404	0.517	0.760	0.581	1.233	0.317	0.406	1.177	
0.252	0.459	0.422	0.523	0.180	0.666	0.777	0.699	0.913	0.472	0.774	0.590	0.410	0.530	0.764	0.597	1.212	0.313	0.430	1.179	
0.258	0.455	0.425	0.504	0.183	0.676	0.776	0.700	0.913	0.474	0.768	0.598	0.408	0.527	0.770	0.580	1.223	0.316	0.408	1.185	
0.264	0.453	0.427	0.497	0.184	0.675	0.774	0.693	0.909	0.470	0.767	0.596	0.405	0.522	0.770	0.593	1.228	0.314	0.405	1.181	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.125	0.005	0.008	0.002	0.781	0.000	0.000	0.000	0.000	0.004	0.000	0.001	0.012	0.002	0.000	0.001	0.000	0.049	0.009	0.000	
LSPI converged after 5 iterations in 3.430 seconds.
MEAN LOSS: 0.606
MEAN RF POLICY* IN MDP_m (loss): 0.514

Loss matrix
0.322	0.344	0.320	0.629	0.229	0.511	0.526	0.757	1.107	0.439	0.555	0.619	0.476	0.445	0.521	0.661	1.592	0.272	0.431	0.872	
0.476	0.555	0.493	0.803	0.309	0.711	0.778	0.803	1.178	0.593	0.695	0.707	0.619	0.614	0.822	0.720	1.514	0.355	0.567	1.099	
0.470	0.554	0.499	0.803	0.308	0.710	0.776	0.804	1.146	0.613	0.690	0.706	0.630	0.627	0.807	0.719	1.541	0.368	0.572	1.111	
0.476	0.560	0.486	0.813	0.312	0.706	0.782	0.828	1.167	0.615	0.691	0.697	0.624	0.624	0.803	0.749	1.574	0.365	0.567	1.088	
0.471	0.548	0.479	0.811	0.305	0.707	0.781	0.814	1.163	0.591	0.684	0.696	0.618	0.607	0.796	0.714	1.534	0.350	0.564	1.088	
0.478	0.557	0.489	0.811	0.313	0.702	0.773	0.816	1.159	0.602	0.688	0.703	0.625	0.621	0.804	0.717	1.542	0.358	0.572	1.096	
0.462	0.540	0.481	0.800	0.298	0.695	0.765	0.790	1.149	0.597	0.676	0.694	0.615	0.600	0.791	0.715	1.537	0.356	0.556	1.093	
0.481	0.555	0.496	0.793	0.309	0.708	0.759	0.800	1.165	0.608	0.684	0.705	0.623	0.616	0.803	0.731	1.541	0.364	0.563	1.088	
0.474	0.552	0.489	0.803	0.309	0.707	0.767	0.808	1.165	0.602	0.688	0.706	0.622	0.611	0.804	0.725	1.534	0.359	0.564	1.097	
0.489	0.574	0.502	0.809	0.317	0.732	0.803	0.815	1.176	0.615	0.702	0.705	0.627	0.628	0.832	0.738	1.548	0.365	0.563	1.114	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.016	0.008	0.014	0.000	0.850	0.001	0.000	0.000	0.000	0.003	0.001	0.000	0.002	0.002	0.000	0.000	0.000	0.097	0.005	0.000	
LSPI converged after 5 iterations in 3.410 seconds.
MEAN LOSS: 0.737
MEAN RF POLICY* IN MDP_m (loss): 0.683

Loss matrix
0.343	0.333	0.302	0.638	0.178	0.481	0.396	0.607	1.124	0.395	0.489	0.616	0.459	0.370	0.592	0.578	1.563	0.272	0.375	0.791	
0.366	0.378	0.399	0.607	0.207	0.545	0.463	0.645	1.065	0.503	0.536	0.676	0.514	0.486	0.637	0.626	1.651	0.375	0.448	0.878	
0.365	0.380	0.412	0.608	0.205	0.547	0.476	0.620	1.059	0.503	0.535	0.682	0.519	0.491	0.662	0.599	1.609	0.374	0.456	0.913	
0.371	0.385	0.414	0.603	0.211	0.550	0.468	0.646	1.078	0.512	0.540	0.692	0.522	0.500	0.660	0.627	1.615	0.384	0.463	0.891	
0.371	0.385	0.410	0.602	0.209	0.555	0.473	0.634	1.061	0.515	0.538	0.684	0.519	0.493	0.649	0.629	1.648	0.385	0.453	0.892	
0.368	0.381	0.403	0.609	0.204	0.548	0.473	0.625	1.070	0.500	0.537	0.678	0.518	0.477	0.667	0.615	1.606	0.375	0.450	0.887	
0.364	0.380	0.403	0.609	0.204	0.548	0.472	0.631	1.067	0.500	0.537	0.679	0.516	0.485	0.652	0.615	1.628	0.372	0.449	0.895	
0.363	0.379	0.401	0.615	0.204	0.539	0.471	0.630	1.061	0.500	0.533	0.674	0.517	0.486	0.647	0.604	1.634	0.371	0.453	0.891	
0.363	0.383	0.421	0.602	0.205	0.559	0.481	0.629	1.052	0.515	0.541	0.690	0.523	0.503	0.657	0.615	1.608	0.386	0.461	0.922	
0.365	0.377	0.405	0.610	0.199	0.539	0.464	0.623	1.063	0.499	0.530	0.676	0.522	0.480	0.650	0.605	1.607	0.370	0.457	0.900	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.025	0.019	0.016	0.001	0.878	0.002	0.004	0.001	0.000	0.003	0.002	0.000	0.002	0.004	0.000	0.001	0.000	0.035	0.006	0.000	
LSPI converged after 5 iterations in 3.430 seconds.
MEAN LOSS: 0.725
MEAN RF POLICY* IN MDP_m (loss): 0.664

Loss matrix
0.555	0.582	0.596	0.614	0.492	0.749	0.811	1.008	1.542	0.602	0.708	0.915	0.634	0.772	0.970	0.894	1.686	0.432	0.626	1.000	
0.690	0.732	0.636	0.766	0.597	0.955	1.117	0.968	1.421	0.678	0.783	0.883	0.668	0.821	1.108	0.869	1.694	0.448	0.639	1.101	
0.667	0.715	0.620	0.755	0.592	0.939	1.109	0.962	1.384	0.676	0.775	0.866	0.653	0.814	1.079	0.858	1.693	0.448	0.618	1.103	
0.684	0.733	0.624	0.773	0.614	0.958	1.115	0.977	1.351	0.690	0.788	0.869	0.657	0.831	1.100	0.866	1.705	0.455	0.630	1.107	
0.667	0.715	0.618	0.754	0.603	0.935	1.109	0.972	1.376	0.682	0.774	0.862	0.649	0.831	1.067	0.859	1.711	0.455	0.621	1.110	
0.662	0.706	0.621	0.752	0.590	0.918	1.064	0.958	1.346	0.664	0.766	0.863	0.655	0.812	1.068	0.829	1.667	0.437	0.624	1.099	
0.684	0.734	0.615	0.774	0.614	0.954	1.148	0.970	1.348	0.693	0.780	0.857	0.651	0.829	1.082	0.851	1.740	0.454	0.621	1.106	
0.666	0.712	0.625	0.753	0.590	0.928	1.095	0.964	1.386	0.674	0.771	0.872	0.657	0.815	1.075	0.857	1.702	0.445	0.627	1.089	
0.671	0.722	0.626	0.756	0.602	0.953	1.120	0.973	1.357	0.687	0.781	0.870	0.659	0.832	1.083	0.856	1.721	0.458	0.629	1.113	
0.678	0.723	0.621	0.758	0.611	0.950	1.112	0.967	1.371	0.685	0.780	0.867	0.648	0.831	1.084	0.842	1.696	0.455	0.621	1.119	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.003	0.001	0.003	0.001	0.008	0.000	0.000	0.000	0.000	0.001	0.000	0.000	0.002	0.000	0.000	0.000	0.000	0.978	0.003	0.000	
LSPI converged after 4 iterations in 2.850 seconds.
MEAN LOSS: 0.469
MEAN RF POLICY* IN MDP_m (loss): 0.456

Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.235	0.410	0.368	0.515	0.165	0.597	0.684	0.715	0.861	0.414	0.725	0.565	0.386	0.461	0.640	0.542	1.317	0.269	0.392	1.079	
0.236	0.412	0.370	0.517	0.164	0.599	0.687	0.709	0.859	0.417	0.725	0.563	0.387	0.462	0.644	0.543	1.315	0.272	0.391	1.080	
0.236	0.411	0.369	0.513	0.163	0.597	0.686	0.708	0.858	0.416	0.724	0.562	0.387	0.461	0.641	0.541	1.313	0.270	0.390	1.078	
0.236	0.411	0.369	0.516	0.163	0.599	0.688	0.707	0.857	0.416	0.724	0.562	0.386	0.461	0.641	0.542	1.313	0.272	0.390	1.079	
0.236	0.412	0.370	0.515	0.164	0.601	0.688	0.707	0.858	0.418	0.725	0.563	0.387	0.462	0.644	0.543	1.315	0.273	0.390	1.082	
0.236	0.412	0.371	0.514	0.164	0.600	0.688	0.707	0.858	0.419	0.725	0.563	0.388	0.462	0.645	0.543	1.313	0.273	0.391	1.081	
0.236	0.412	0.370	0.516	0.163	0.598	0.687	0.708	0.859	0.417	0.726	0.562	0.387	0.462	0.644	0.544	1.312	0.271	0.388	1.080	
0.236	0.412	0.369	0.514	0.164	0.599	0.687	0.709	0.859	0.417	0.725	0.563	0.387	0.461	0.643	0.543	1.314	0.272	0.391	1.079	
0.236	0.412	0.370	0.517	0.163	0.598	0.687	0.709	0.859	0.417	0.726	0.563	0.387	0.462	0.643	0.544	1.314	0.272	0.393	1.080	
0.236	0.411	0.369	0.515	0.164	0.599	0.688	0.707	0.857	0.417	0.724	0.562	0.387	0.461	0.641	0.541	1.314	0.272	0.391	1.079	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.129	0.006	0.016	0.002	0.750	0.001	0.000	0.000	0.000	0.006	0.000	0.001	0.009	0.004	0.000	0.001	0.000	0.065	0.009	0.000	
LSPI converged after 4 iterations in 3.260 seconds.
MEAN LOSS: 0.600
MEAN RF POLICY* IN MDP_m (loss): 0.495

Loss matrix
0.301	0.330	0.310	0.585	0.237	0.480	0.541	0.733	1.111	0.423	0.533	0.596	0.481	0.421	0.512	0.688	1.579	0.271	0.406	0.886	
0.337	0.381	0.354	0.633	0.254	0.527	0.582	0.768	1.147	0.466	0.572	0.632	0.526	0.458	0.591	0.725	1.615	0.303	0.453	0.942	
0.335	0.377	0.351	0.629	0.253	0.524	0.581	0.764	1.140	0.463	0.570	0.629	0.522	0.457	0.587	0.718	1.611	0.301	0.449	0.939	
0.338	0.385	0.355	0.637	0.256	0.527	0.583	0.774	1.148	0.471	0.575	0.633	0.528	0.463	0.591	0.731	1.626	0.306	0.456	0.941	
0.336	0.380	0.354	0.632	0.254	0.525	0.581	0.768	1.146	0.466	0.572	0.631	0.525	0.459	0.590	0.726	1.612	0.303	0.452	0.942	
0.337	0.380	0.354	0.633	0.255	0.526	0.581	0.768	1.149	0.466	0.572	0.633	0.525	0.460	0.591	0.726	1.608	0.302	0.453	0.940	
0.335	0.379	0.353	0.632	0.254	0.524	0.577	0.767	1.140	0.468	0.570	0.631	0.526	0.459	0.587	0.724	1.620	0.304	0.452	0.940	
0.336	0.378	0.353	0.627	0.254	0.524	0.579	0.768	1.142	0.465	0.570	0.632	0.524	0.459	0.586	0.722	1.610	0.303	0.452	0.937	
0.333	0.377	0.351	0.629	0.252	0.523	0.580	0.764	1.143	0.464	0.570	0.629	0.523	0.457	0.587	0.722	1.609	0.300	0.450	0.938	
0.336	0.379	0.353	0.631	0.254	0.525	0.579	0.766	1.142	0.466	0.570	0.631	0.524	0.459	0.588	0.723	1.612	0.303	0.452	0.939	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.042	0.023	0.032	0.001	0.781	0.003	0.001	0.000	0.000	0.006	0.001	0.001	0.003	0.007	0.001	0.000	0.000	0.090	0.008	0.000	
LSPI converged after 5 iterations in 3.920 seconds.
MEAN LOSS: 0.726
MEAN RF POLICY* IN MDP_m (loss): 0.649

Loss matrix
0.338	0.323	0.299	0.576	0.173	0.474	0.398	0.549	1.089	0.384	0.476	0.593	0.445	0.334	0.576	0.549	1.541	0.273	0.332	0.785	
0.343	0.333	0.316	0.572	0.178	0.488	0.412	0.553	1.076	0.404	0.485	0.601	0.455	0.355	0.584	0.556	1.560	0.293	0.345	0.801	
0.340	0.330	0.314	0.573	0.174	0.483	0.411	0.550	1.074	0.399	0.483	0.598	0.453	0.352	0.583	0.551	1.556	0.291	0.343	0.799	
0.343	0.331	0.312	0.572	0.177	0.488	0.412	0.551	1.079	0.398	0.485	0.601	0.452	0.350	0.586	0.553	1.551	0.288	0.341	0.798	
0.342	0.330	0.313	0.571	0.177	0.486	0.411	0.551	1.073	0.400	0.482	0.601	0.453	0.352	0.581	0.548	1.555	0.290	0.342	0.801	
0.343	0.332	0.314	0.571	0.178	0.490	0.414	0.553	1.075	0.402	0.485	0.602	0.453	0.354	0.583	0.554	1.567	0.293	0.341	0.801	
0.342	0.331	0.313	0.571	0.177	0.486	0.411	0.553	1.078	0.400	0.484	0.600	0.453	0.352	0.582	0.555	1.562	0.291	0.340	0.800	
0.342	0.331	0.314	0.570	0.178	0.488	0.412	0.552	1.076	0.399	0.484	0.601	0.453	0.352	0.584	0.552	1.555	0.291	0.342	0.799	
0.342	0.331	0.313	0.570	0.177	0.488	0.412	0.550	1.075	0.400	0.483	0.600	0.452	0.352	0.582	0.552	1.557	0.290	0.341	0.800	
0.342	0.332	0.314	0.572	0.176	0.486	0.412	0.553	1.076	0.401	0.484	0.600	0.453	0.354	0.583	0.553	1.560	0.291	0.343	0.801	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.015	0.019	0.030	0.001	0.809	0.002	0.006	0.001	0.000	0.007	0.002	0.001	0.004	0.013	0.001	0.001	0.000	0.073	0.015	0.000	
LSPI converged after 4 iterations in 3.260 seconds.
MEAN LOSS: 0.738
MEAN RF POLICY* IN MDP_m (loss): 0.637

Loss matrix
0.515	0.508	0.520	0.538	0.412	0.663	0.608	0.925	1.357	0.530	0.647	0.822	0.518	0.688	0.897	0.815	1.480	0.385	0.553	0.883	
0.522	0.525	0.520	0.573	0.421	0.692	0.674	0.908	1.307	0.552	0.648	0.809	0.530	0.690	0.885	0.778	1.499	0.394	0.544	0.911	
0.520	0.527	0.519	0.578	0.424	0.697	0.683	0.904	1.283	0.562	0.645	0.806	0.534	0.696	0.874	0.773	1.509	0.401	0.541	0.917	
0.518	0.526	0.519	0.579	0.421	0.694	0.678	0.905	1.293	0.558	0.646	0.807	0.534	0.692	0.878	0.773	1.509	0.396	0.542	0.909	
0.526	0.529	0.523	0.575	0.425	0.698	0.681	0.915	1.301	0.556	0.650	0.814	0.534	0.695	0.887	0.776	1.502	0.396	0.551	0.913	
0.522	0.527	0.520	0.576	0.423	0.696	0.680	0.907	1.297	0.553	0.648	0.809	0.532	0.691	0.884	0.774	1.500	0.394	0.545	0.913	
0.526	0.531	0.527	0.576	0.426	0.698	0.673	0.914	1.314	0.555	0.652	0.817	0.534	0.700	0.897	0.786	1.502	0.395	0.550	0.910	
0.521	0.527	0.521	0.578	0.421	0.696	0.677	0.899	1.286	0.555	0.646	0.807	0.533	0.692	0.880	0.772	1.502	0.395	0.540	0.912	
0.525	0.529	0.522	0.575	0.423	0.694	0.670	0.908	1.304	0.552	0.649	0.809	0.532	0.692	0.890	0.784	1.494	0.393	0.547	0.908	
0.519	0.526	0.520	0.575	0.420	0.692	0.676	0.904	1.293	0.557	0.647	0.805	0.533	0.693	0.881	0.776	1.501	0.396	0.543	0.912	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.006	0.007	0.006	0.003	0.046	0.001	0.001	0.000	0.000	0.003	0.001	0.000	0.005	0.001	0.000	0.000	0.000	0.917	0.004	0.000	
LSPI converged after 4 iterations in 3.230 seconds.
MEAN LOSS: 0.487
MEAN RF POLICY* IN MDP_m (loss): 0.432

