Generating LSTDQ demonstrations...
2156 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.144	0.247	0.919	-0.017	0.085	0.134	0.141	-0.223	-1.353	-0.175	0.101	-0.422	0.027	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.047	0.330	1.009	0.015	0.038	0.169	-0.158	-0.307	-1.111	-0.095	0.097	-0.386	-0.005	
Sampling 20 reward functions...
	Sampled reward functions:
	 (0)	-0.118	-0.114	0.875	0.074	0.212	-0.605	0.078	0.340	-0.784	0.110	-0.251	-0.634	0.159	
	 (1)	-0.043	0.385	1.696	-0.166	-0.145	0.860	0.110	-0.578	-0.797	0.190	0.033	0.125	-0.237	
	 (2)	-0.218	-0.129	1.495	0.068	-0.048	0.280	0.271	0.064	-0.699	-0.287	-0.004	1.340	-0.018	
	 (3)	-0.149	-0.082	1.830	-0.121	-0.301	0.332	0.276	1.272	0.131	-0.764	-0.257	-1.291	0.264	
	 (4)	0.130	0.245	1.508	0.111	-0.090	0.551	-0.139	-0.631	-1.049	0.237	0.079	-0.916	-0.163	
	 (5)	0.031	-0.342	1.409	0.058	0.058	1.893	0.003	-0.029	-0.687	0.102	-0.091	-1.678	0.153	
	 (6)	0.135	0.122	0.999	-0.143	-0.032	-0.450	-0.190	0.065	-1.083	0.217	0.027	-2.485	0.010	
	 (7)	-0.328	-0.539	0.326	0.237	0.128	-0.340	0.265	0.710	-0.509	-0.132	-0.068	0.462	0.549	
	 (8)	-0.052	0.243	1.442	0.115	-0.135	-0.663	0.071	-0.290	-0.554	-0.119	0.031	1.609	-0.004	
	 (9)	0.185	-0.659	0.802	-0.067	0.039	1.330	-0.294	0.342	-0.555	0.544	0.151	-1.752	-0.226	
	 (10)	0.011	0.111	0.990	0.313	-0.017	0.198	-0.009	0.019	-1.128	-0.102	0.061	0.429	-0.150	
	 (11)	-0.147	0.061	0.912	0.290	0.062	-0.537	0.233	-0.032	-1.059	-0.518	-0.065	-0.938	0.081	
	 (12)	0.103	0.276	1.025	-0.335	0.146	-2.140	-0.052	0.313	-0.993	-0.060	-0.204	0.065	0.021	
	 (13)	-0.009	0.116	1.230	0.001	-0.043	0.797	-0.017	0.147	-0.885	-0.066	-0.131	-3.369	-0.068	
	 (14)	-0.062	-0.127	1.187	0.445	0.136	0.468	0.071	-0.067	-0.745	-0.132	-0.171	-1.214	0.006	
	 (15)	0.223	0.267	1.479	0.142	-0.185	0.354	-0.209	-0.211	-0.421	-0.303	0.066	1.186	-0.192	
	 (16)	-0.092	0.134	0.894	0.138	0.094	-0.892	0.157	-0.337	-0.298	-0.189	-0.315	1.229	-0.150	
	 (17)	0.147	-0.054	1.346	-0.063	0.089	0.127	-0.145	-0.154	-0.567	0.409	-0.181	3.263	0.042	
	 (18)	-0.165	0.044	1.299	0.090	-0.040	-0.023	0.181	-0.298	-0.828	0.022	0.033	0.069	-0.009	
	 (19)	0.042	-0.238	0.976	-0.118	0.040	0.976	0.022	0.277	-0.793	-0.026	-0.090	-2.115	0.045	
LSPI converged after 5 iterations in 3.180 seconds.
Creating 20 corresponding optimal policies...
LSPI converged after 4 iterations in 2.890 seconds.
LSPI converged after 5 iterations in 3.450 seconds.
LSPI converged after 5 iterations in 3.390 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 4 iterations in 2.840 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.410 seconds.
LSPI converged after 4 iterations in 2.830 seconds.
LSPI converged after 5 iterations in 3.420 seconds.
LSPI converged after 5 iterations in 3.470 seconds.
LSPI converged after 5 iterations in 3.430 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.440 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 4 iterations in 2.840 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.410 seconds.
LSPI converged after 5 iterations in 3.410 seconds.
LSPI converged after 5 iterations in 3.430 seconds.
LSPI converged after 5 iterations in 3.440 seconds.
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.970 }	0.173	0.293	0.920	-0.024	0.080	0.288	0.352	-0.188	-1.506	-0.379	0.106	-0.720	0.024	
	{ -0.132 }	 (random)
	{ -0.135 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.602 }	0.125	0.076	0.480	-0.077	0.498	-0.209	0.515	0.087	-1.482	-0.434	-0.256	-1.782	0.209	
 (1)	{ 0.827 }	0.249	0.938	1.579	-0.218	-0.117	0.784	0.941	-0.769	-1.867	-0.848	0.171	-1.183	-0.331	
 (2)	{ 0.877 }	-0.093	0.273	1.288	-0.054	0.017	0.273	1.006	-0.082	-1.659	-0.962	0.147	0.121	-0.021	
 (3)	{ 0.647 }	-0.048	0.449	2.053	-0.160	-0.438	0.436	1.288	1.051	-0.484	-1.312	-0.305	-1.552	0.632	
 (4)	{ 0.930 }	0.541	0.742	1.252	0.033	-0.032	0.697	0.178	-0.923	-1.999	-0.398	0.296	-1.668	-0.268	
 (5)	{ 0.829 }	0.448	0.164	1.055	0.053	0.236	2.007	0.535	-0.422	-1.641	-0.679	-0.004	-2.511	0.276	
 (6)	{ 0.950 }	0.345	0.473	0.787	-0.184	0.025	-0.242	0.067	-0.227	-1.767	-0.184	0.190	-2.744	-0.002	
 (7)	{ 0.199 }	-0.204	-0.567	-0.020	0.057	0.230	0.014	0.831	0.600	-1.003	-0.522	0.019	-0.460	0.926	
 (8)	{ 0.854 }	0.139	0.599	1.307	-0.038	-0.148	-0.368	0.501	-0.355	-1.247	-0.559	0.179	0.673	0.033	
 (9)	{ 0.351 }	0.353	-0.496	0.526	-0.232	0.131	0.932	0.444	0.142	-1.648	-0.146	0.412	-2.164	-0.459	
 (10)	{ 0.830 }	0.373	0.310	0.723	0.099	0.090	0.301	0.618	-0.260	-2.105	-0.783	0.245	-0.881	-0.255	
 (11)	{ 0.852 }	0.150	0.248	0.613	0.050	0.211	-0.197	0.751	-0.382	-1.784	-1.310	0.041	-1.947	0.139	
 (12)	{ 0.679 }	0.455	0.450	0.655	-0.636	0.401	-1.435	0.445	0.023	-1.845	-0.603	-0.148	-1.265	-0.019	
 (13)	{ 0.851 }	0.212	0.613	1.129	0.129	0.062	0.863	0.282	-0.253	-1.505	-0.545	-0.114	-3.361	-0.099	
 (14)	{ 0.857 }	0.382	0.115	0.822	0.214	0.377	0.781	0.579	-0.435	-1.593	-0.961	-0.116	-2.136	-0.005	
 (15)	{ 0.953 }	0.595	0.703	1.367	-0.002	-0.181	0.485	0.073	-0.457	-1.237	-0.545	0.231	0.610	-0.278	
 (16)	{ 0.423 }	0.002	0.237	0.796	0.039	0.307	-0.517	0.376	-0.420	-0.531	-0.574	-0.460	0.402	-0.281	
 (17)	{ 0.919 }	0.441	0.228	1.048	-0.225	0.273	0.397	0.335	-0.251	-1.341	0.006	-0.154	1.703	0.087	
 (18)	{ 0.820 }	-0.001	0.426	1.066	-0.013	0.017	0.064	0.689	-0.433	-1.637	-0.649	0.195	-0.937	-0.007	
 (19)	{ 0.902 }	0.221	0.120	0.779	-0.101	0.166	0.951	0.472	-0.047	-1.474	-0.579	-0.036	-2.620	0.077	
Scores for the experts true policies:
	{ 0.947 }	 (expert 0)
	{ 0.943 }	 (expert 1)
	{ 0.946 }	 (expert 2)
	{ 0.648 }	 (expert 3)
Summed loss of reward-optimal policies w.r.t. true rho values (expert weights)
	0.551	1.049	0.660	0.927	0.700	0.993	0.398	0.587	0.398	0.833	0.835	0.588	0.695	0.616	0.796	0.546	0.702	0.757	0.473	0.543	
	0.591	1.101	0.646	0.938	0.753	1.046	0.447	0.565	0.369	0.864	0.847	0.628	0.698	0.669	0.847	0.523	0.699	0.733	0.524	0.597	
	0.827	1.323	0.967	1.146	0.997	1.285	0.696	0.843	0.702	1.096	1.172	0.799	0.969	0.832	1.053	0.851	0.612	1.090	0.739	0.840	
	0.611	1.159	0.720	0.964	0.806	1.082	0.491	0.563	0.469	0.839	0.885	0.668	0.701	0.736	0.872	0.580	0.680	0.796	0.590	0.621	
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.022	0.000	0.002	0.000	0.003	0.000	0.878	0.027	0.020	0.002	0.001	0.002	0.001	0.008	0.000	0.004	0.002	0.002	0.015	0.009	
LSPI ended after MAX_ITERATIONS = 100 in 1.080 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.487

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.016	0.000	0.002	0.000	0.009	0.000	0.833	0.000	0.005	0.001	0.005	0.008	0.005	0.061	0.001	0.014	0.002	0.003	0.007	0.027	
LSPI converged after 4 iterations in 0.060 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.367

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.011	0.001	0.003	0.000	0.011	0.001	0.647	0.000	0.003	0.000	0.005	0.004	0.002	0.200	0.002	0.018	0.008	0.001	0.005	0.079	
LSPI converged after 4 iterations in 0.050 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.771

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.012	0.000	0.001	0.000	0.001	0.000	0.901	0.018	0.028	0.000	0.000	0.003	0.000	0.001	0.000	0.018	0.001	0.002	0.004	0.010	
LSPI converged after 5 iterations in 0.070 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.668

Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.018	0.001	0.007	0.000	0.005	0.000	0.786	0.010	0.039	0.003	0.002	0.003	0.004	0.011	0.001	0.004	0.042	0.005	0.050	0.010	
LSPI ended after MAX_ITERATIONS = 100 in 11.430 seconds.
MEAN RF POLICY* IN MDP_m (loss): 3.270

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.021	0.001	0.005	0.000	0.015	0.001	0.680	0.001	0.007	0.004	0.004	0.007	0.002	0.111	0.003	0.011	0.039	0.005	0.021	0.061	
LSPI converged after 5 iterations in 0.680 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.302

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.027	0.000	0.005	0.000	0.007	0.001	0.771	0.002	0.014	0.002	0.005	0.010	0.005	0.044	0.004	0.013	0.014	0.003	0.014	0.058	
LSPI converged after 5 iterations in 0.690 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.874

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.008	0.000	0.002	0.000	0.001	0.000	0.894	0.013	0.017	0.002	0.001	0.004	0.002	0.004	0.001	0.025	0.005	0.003	0.004	0.015	
LSPI converged after 5 iterations in 0.700 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.635

Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.036	0.001	0.007	0.000	0.004	0.000	0.748	0.010	0.025	0.006	0.002	0.003	0.009	0.016	0.001	0.004	0.080	0.004	0.034	0.011	
LSPI ended after MAX_ITERATIONS = 100 in 129.640 seconds.
MEAN RF POLICY* IN MDP_m (loss): 2.494

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.053	0.000	0.009	0.000	0.005	0.001	0.623	0.003	0.012	0.004	0.003	0.009	0.006	0.055	0.004	0.008	0.126	0.005	0.025	0.048	
LSPI converged after 5 iterations in 8.010 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.325

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.036	0.000	0.006	0.000	0.008	0.002	0.697	0.005	0.022	0.001	0.004	0.014	0.005	0.053	0.005	0.012	0.020	0.004	0.017	0.090	
LSPI converged after 5 iterations in 8.110 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.817

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.016	0.000	0.004	0.000	0.002	0.001	0.854	0.012	0.010	0.003	0.003	0.010	0.005	0.008	0.002	0.007	0.015	0.004	0.008	0.034	
LSPI converged after 5 iterations in 8.220 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.674

