Generating LSTDQ demonstrations...
2153 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.163	0.250	0.916	-0.026	0.086	0.172	0.115	-0.232	-1.373	-0.154	0.105	-0.509	-0.001	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.048	0.324	1.024	0.009	0.043	0.186	-0.161	-0.303	-1.125	-0.091	0.091	-0.421	-0.026	
Sampling 20 reward functions...
	Sampled reward functions:
	 (0)	-0.136	-0.121	0.615	-0.149	0.128	-0.154	0.145	-0.156	-0.057	-0.049	-0.091	-0.630	-0.034	
	 (1)	-0.133	0.205	0.870	0.512	-0.202	0.615	0.027	-0.247	-1.108	-0.661	0.289	3.017	-0.335	
	 (2)	-0.005	-0.264	0.693	0.062	-0.034	0.383	-0.009	-0.053	-0.545	0.105	0.015	-1.404	0.253	
	 (3)	0.031	0.083	0.492	-0.123	0.062	-0.637	0.010	-0.135	-1.231	0.128	0.027	-0.108	0.014	
	 (4)	0.162	-0.509	1.354	-0.280	0.096	0.560	0.035	-0.105	-0.625	0.164	-0.152	1.568	0.131	
	 (5)	0.321	-0.027	1.389	-0.394	-0.115	-0.204	-0.256	-0.409	-0.975	0.658	0.163	0.893	-0.147	
	 (6)	-0.120	-0.226	1.195	0.096	-0.032	0.748	0.055	-0.027	-0.803	0.104	0.075	0.945	0.014	
	 (7)	-0.061	0.031	0.715	-0.248	-0.086	0.357	0.087	0.168	-1.119	0.083	0.052	-0.412	-0.105	
	 (8)	0.185	-0.053	1.117	0.043	-0.031	-0.184	-0.146	0.010	-0.806	0.274	-0.026	0.540	0.039	
	 (9)	0.205	-0.180	1.305	-0.433	0.144	-0.213	-0.066	0.173	-0.815	0.081	-0.094	0.536	0.233	
	 (10)	-0.040	0.435	2.178	0.345	-0.005	-1.202	0.021	-1.259	-1.270	0.441	-0.056	4.893	0.128	
	 (11)	0.161	-0.185	1.188	-0.620	0.034	-1.651	-0.002	0.389	-0.753	0.079	-0.141	-0.557	0.045	
	 (12)	0.134	0.040	1.626	-0.162	-0.130	1.542	-0.141	-0.002	-1.122	0.094	0.145	-1.413	-0.106	
	 (13)	0.204	0.271	1.479	0.030	-0.099	-0.022	-0.143	-0.284	-0.916	-0.319	0.015	2.156	-0.150	
	 (14)	-0.026	0.247	0.580	-0.354	0.030	-0.648	-0.038	0.054	-1.070	0.329	-0.062	-0.371	-0.018	
	 (15)	0.274	0.133	1.098	-0.299	-0.035	0.805	-0.392	-0.489	-0.689	0.702	0.062	0.748	0.029	
	 (16)	-0.004	0.377	1.132	0.139	-0.176	0.157	0.043	0.116	-0.428	-0.475	-0.019	-0.187	-0.330	
	 (17)	0.152	0.510	2.416	-0.021	-0.183	-1.998	-0.097	-0.251	-1.237	-0.131	0.063	1.344	-0.290	
	 (18)	0.067	-0.410	1.462	-0.402	0.049	0.909	0.008	-0.050	-0.685	0.215	0.002	-2.067	0.083	
	 (19)	-0.287	0.070	2.045	0.583	-0.026	-1.230	0.213	-0.101	-1.295	-0.412	0.146	0.699	0.008	
LSPI converged after 5 iterations in 3.160 seconds.
Creating 20 corresponding optimal policies...
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.968 }	0.201	0.298	0.900	-0.042	0.088	0.291	0.315	-0.215	-1.536	-0.344	0.114	-0.805	-0.010	
	{ -0.140 }	 (random)
	{ -0.134 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.558 }	-0.087	0.026	0.352	-0.129	0.289	-0.046	0.397	-0.230	-0.533	-0.430	-0.046	-0.739	-0.123	
 (1)	{ 0.603 }	0.184	0.457	0.695	0.317	-0.251	0.490	0.367	-0.458	-2.117	-0.654	0.618	1.668	-0.592	
 (2)	{ 0.876 }	0.191	-0.077	0.496	-0.062	-0.012	0.478	0.154	-0.262	-0.906	-0.231	0.122	-1.511	0.421	
 (3)	{ 0.844 }	0.250	0.177	0.200	-0.334	0.164	-0.452	0.310	-0.342	-1.827	-0.302	0.174	-1.307	-0.007	
 (4)	{ 0.935 }	0.509	-0.201	1.012	-0.638	0.268	0.665	0.440	-0.348	-1.379	-0.390	-0.083	0.172	0.202	
 (5)	{ 0.931 }	0.680	0.262	1.137	-0.860	-0.131	-0.233	0.255	-0.520	-1.953	0.110	0.440	-0.464	-0.247	
 (6)	{ 0.758 }	0.043	0.204	0.923	0.081	0.006	0.666	0.463	-0.206	-1.692	-0.254	0.287	-0.181	-0.018	
 (7)	{ 0.464 }	-0.001	0.234	0.643	-0.325	-0.079	0.183	0.667	-0.063	-1.739	-0.477	0.168	-1.343	-0.165	
 (8)	{ 0.894 }	0.576	0.184	0.887	-0.260	0.030	-0.004	0.201	-0.204	-1.501	-0.113	0.100	-0.752	0.066	
 (9)	{ 0.936 }	0.762	0.080	0.736	-0.869	0.373	0.228	0.535	-0.156	-1.937	-0.634	0.084	-1.114	0.351	
 (10)	{ 0.933 }	0.578	0.939	1.625	-0.055	0.148	-0.446	0.463	-1.426	-2.442	-0.349	0.171	2.199	0.198	
 (11)	{ 0.914 }	0.348	0.070	0.988	-0.941	0.153	-1.223	0.480	0.145	-1.418	-0.428	-0.083	-1.459	0.049	
 (12)	{ 0.931 }	0.542	0.704	1.326	-0.086	-0.113	1.417	0.486	-0.431	-2.425	-0.607	0.442	-2.494	-0.187	
 (13)	{ 0.932 }	0.620	0.680	1.228	-0.170	-0.028	0.276	0.123	-0.600	-1.828	-0.588	0.205	0.897	-0.267	
 (14)	{ 0.449 }	0.013	0.391	0.415	-0.358	0.129	-0.529	0.402	-0.126	-1.559	-0.163	-0.013	-1.308	-0.043	
 (15)	{ 0.940 }	0.618	0.556	0.823	-0.281	0.004	0.905	-0.098	-0.688	-1.475	0.246	0.238	-0.392	0.045	
 (16)	{ 0.901 }	0.212	0.710	1.156	0.067	-0.166	0.192	0.443	-0.157	-1.091	-0.812	0.058	-0.561	-0.559	
 (17)	{ 0.819 }	0.604	1.161	2.076	-0.390	-0.126	-1.459	0.529	-0.579	-2.721	-0.779	0.398	-0.231	-0.549	
 (18)	{ 0.935 }	0.266	0.124	1.078	-0.380	0.167	0.804	0.443	-0.315	-1.663	-0.389	0.195	-2.453	0.089	
 (19)	{ 0.792 }	0.213	0.652	1.426	0.207	0.106	-0.768	0.896	-0.450	-2.944	-1.173	0.567	-1.046	-0.088	
Scores for the experts true policies:
	{ 0.817 }	 (expert 0)
	{ 0.865 }	 (expert 1)
	{ 0.864 }	 (expert 2)
	{ 0.685 }	 (expert 3)
Summed loss of reward-optimal policies w.r.t. true rho values rho_m == MDP_m == V_m == (expert weights)
	0.474	1.228	0.720	0.734	0.833	1.163	0.915	0.842	0.960	1.491	1.538	0.871	1.764	0.969	0.680	1.015	0.805	1.517	1.114	1.766	
	0.463	1.032	0.649	0.468	0.525	0.857	0.615	0.623	0.572	1.146	1.274	0.713	1.448	0.714	0.536	0.733	0.570	1.215	0.856	1.438	
	0.397	1.222	0.521	0.526	0.618	0.980	0.778	0.666	0.718	1.308	1.384	0.740	1.557	0.883	0.524	0.819	0.675	1.386	0.885	1.618	
	0.454	1.051	0.630	0.443	0.519	0.847	0.577	0.580	0.573	1.121	1.227	0.736	1.386	0.705	0.484	0.660	0.551	1.198	0.787	1.412	
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.497	0.026	0.019	0.047	0.008	0.010	0.066	0.016	0.031	0.001	0.001	0.005	0.004	0.030	0.038	0.035	0.149	0.004	0.012	0.001	
LSPI converged after 5 iterations in 3.950 seconds.
MEAN LOSS: 0.684
MEAN RF POLICY* LOSS IN MDP_m: 0.427
MEAN RF POLICY* SCORE IN MDP: 0.921

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.869	0.002	0.001	0.075	0.000	0.000	0.010	0.001	0.001	0.000	0.000	0.000	0.000	0.002	0.037	0.000	0.001	0.000	0.001	0.000	
LSPI converged after 5 iterations in 4.010 seconds.
MEAN LOSS: 0.471
MEAN RF POLICY* LOSS IN MDP_m: 0.433
MEAN RF POLICY* SCORE IN MDP: 0.838

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.929	0.000	0.023	0.005	0.001	0.000	0.002	0.001	0.007	0.000	0.000	0.000	0.000	0.007	0.003	0.000	0.019	0.000	0.002	0.000	
LSPI converged after 5 iterations in 3.980 seconds.
MEAN LOSS: 0.415
MEAN RF POLICY* LOSS IN MDP_m: 0.367
MEAN RF POLICY* SCORE IN MDP: 0.892

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.417	0.027	0.068	0.046	0.015	0.018	0.047	0.027	0.033	0.009	0.002	0.021	0.014	0.026	0.045	0.025	0.118	0.009	0.027	0.007	
LSPI converged after 5 iterations in 3.950 seconds.
MEAN LOSS: 0.576
MEAN RF POLICY* LOSS IN MDP_m: 0.208
MEAN RF POLICY* SCORE IN MDP: 0.931

LSPI converged after 4 iterations in 3.280 seconds.
~~~ MAGIC SCORE ~~~	0.917
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.543	0.012	0.038	0.043	0.008	0.011	0.082	0.014	0.033	0.004	0.002	0.006	0.008	0.024	0.037	0.061	0.046	0.002	0.027	0.001	
LSPI converged after 4 iterations in 3.380 seconds.
MEAN LOSS: 0.677
MEAN RF POLICY* LOSS IN MDP_m: 0.417
MEAN RF POLICY* SCORE IN MDP: 0.917

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.001	0.000	0.003	0.003	0.000	0.000	0.972	0.000	0.002	0.000	0.000	0.000	0.000	0.007	0.005	0.001	0.004	0.000	0.000	0.000	
LSPI converged after 5 iterations in 4.190 seconds.
MEAN LOSS: 0.615
MEAN RF POLICY* LOSS IN MDP_m: 0.600
MEAN RF POLICY* SCORE IN MDP: 0.769

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.860	0.002	0.009	0.008	0.001	0.000	0.012	0.001	0.005	0.000	0.000	0.001	0.001	0.010	0.006	0.012	0.068	0.000	0.004	0.000	
LSPI converged after 4 iterations in 3.400 seconds.
MEAN LOSS: 0.441
MEAN RF POLICY* LOSS IN MDP_m: 0.350
MEAN RF POLICY* SCORE IN MDP: 0.888

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.447	0.021	0.045	0.045	0.019	0.016	0.056	0.019	0.040	0.009	0.002	0.014	0.012	0.035	0.040	0.060	0.083	0.004	0.031	0.001	
LSPI converged after 5 iterations in 4.230 seconds.
MEAN LOSS: 0.563
MEAN RF POLICY* LOSS IN MDP_m: 0.194
MEAN RF POLICY* SCORE IN MDP: 0.939

LSPI converged after 5 iterations in 4.220 seconds.
~~~ MAGIC SCORE ~~~	0.920
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.504	0.012	0.039	0.042	0.009	0.010	0.094	0.013	0.034	0.003	0.001	0.005	0.010	0.032	0.036	0.059	0.067	0.003	0.026	0.001	
LSPI converged after 5 iterations in 4.470 seconds.
MEAN LOSS: 0.693
MEAN RF POLICY* LOSS IN MDP_m: 0.424
MEAN RF POLICY* SCORE IN MDP: 0.916

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.956	0.000	0.000	0.005	0.000	0.000	0.021	0.000	0.003	0.000	0.000	0.000	0.000	0.000	0.008	0.001	0.004	0.000	0.000	0.000	
LSPI converged after 5 iterations in 4.410 seconds.
MEAN LOSS: 0.469
MEAN RF POLICY* LOSS IN MDP_m: 0.445
MEAN RF POLICY* SCORE IN MDP: 0.884

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.855	0.002	0.008	0.011	0.001	0.001	0.019	0.002	0.009	0.001	0.000	0.001	0.001	0.010	0.010	0.010	0.053	0.000	0.006	0.000	
LSPI converged after 5 iterations in 4.390 seconds.
MEAN LOSS: 0.443
MEAN RF POLICY* LOSS IN MDP_m: 0.348
MEAN RF POLICY* SCORE IN MDP: 0.855

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.446	0.021	0.042	0.056	0.017	0.016	0.054	0.020	0.041	0.008	0.001	0.016	0.011	0.034	0.044	0.043	0.091	0.006	0.030	0.002	
LSPI converged after 5 iterations in 4.340 seconds.
MEAN LOSS: 0.560
MEAN RF POLICY* LOSS IN MDP_m: 0.207
MEAN RF POLICY* SCORE IN MDP: 0.916

LSPI converged after 5 iterations in 4.320 seconds.
~~~ MAGIC SCORE ~~~	0.921
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.485	0.013	0.033	0.052	0.011	0.010	0.100	0.017	0.038	0.004	0.000	0.008	0.011	0.031	0.040	0.041	0.065	0.005	0.034	0.002	
LSPI converged after 5 iterations in 4.300 seconds.
MEAN LOSS: 0.704
MEAN RF POLICY* LOSS IN MDP_m: 0.442
MEAN RF POLICY* SCORE IN MDP: 0.919

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.949	0.000	0.000	0.003	0.000	0.000	0.024	0.001	0.005	0.000	0.000	0.000	0.000	0.001	0.006	0.001	0.009	0.000	0.000	0.000	
LSPI converged after 5 iterations in 4.480 seconds.
MEAN LOSS: 0.470
MEAN RF POLICY* LOSS IN MDP_m: 0.441
MEAN RF POLICY* SCORE IN MDP: 0.894

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.818	0.003	0.014	0.016	0.002	0.001	0.047	0.002	0.009	0.001	0.000	0.001	0.002	0.014	0.012	0.024	0.023	0.001	0.010	0.000	
LSPI converged after 4 iterations in 3.710 seconds.
MEAN LOSS: 0.460
MEAN RF POLICY* LOSS IN MDP_m: 0.325
MEAN RF POLICY* SCORE IN MDP: 0.928

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.424	0.022	0.042	0.053	0.017	0.016	0.066	0.025	0.040	0.008	0.001	0.015	0.015	0.032	0.047	0.046	0.085	0.006	0.035	0.003	
LSPI converged after 5 iterations in 4.470 seconds.
MEAN LOSS: 0.568
MEAN RF POLICY* LOSS IN MDP_m: 0.198
MEAN RF POLICY* SCORE IN MDP: 0.932

LSPI converged after 5 iterations in 4.400 seconds.
~~~ MAGIC SCORE ~~~	0.914
