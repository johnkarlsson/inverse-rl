Generating LSTDQ demonstrations...
2157 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.154	0.267	0.951	-0.016	0.084	0.114	0.130	-0.233	-1.382	-0.166	0.105	-0.483	-0.007	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.041	0.323	1.040	0.005	0.043	0.147	-0.153	-0.293	-1.126	-0.092	0.094	-0.417	-0.030	
Sampling 10 reward functions...
	Sampled reward functions:
	 (0)	-0.764	-1.195	-0.512	0.141	0.546	-0.516	0.632	0.253	-0.697	0.025	0.091	2.509	1.096	
	 (1)	0.081	-0.154	0.964	-0.149	-0.017	0.267	-0.110	0.052	-0.920	0.060	0.020	0.637	0.036	
	 (2)	-0.080	-0.430	1.168	0.250	0.125	0.432	0.063	0.061	-0.282	0.154	-0.151	1.977	0.144	
	 (3)	0.006	0.481	1.775	0.290	-0.210	-0.232	0.092	-0.437	-0.830	-0.313	-0.126	1.273	-0.191	
	 (4)	0.076	0.259	0.736	-0.188	-0.049	-0.793	-0.009	-0.023	-1.150	-0.023	0.135	0.204	0.045	
	 (5)	-0.134	-0.191	0.632	0.054	0.104	-0.922	0.187	0.073	-1.433	-0.233	0.208	1.540	0.255	
	 (6)	0.346	-0.218	0.923	-0.339	-0.016	0.836	-0.274	0.250	-0.751	0.366	-0.007	-1.006	0.169	
	 (7)	-0.249	-0.652	1.244	-0.009	-0.017	0.829	0.349	0.872	-0.101	-0.205	-0.263	1.043	0.342	
	 (8)	-0.132	-0.555	1.292	-0.176	0.194	0.697	0.044	-0.420	-1.288	0.589	0.143	1.445	0.420	
	 (9)	-0.183	-0.282	1.303	0.376	-0.017	1.270	0.138	-0.307	-0.401	0.152	-0.142	-0.041	0.087	
LSPI converged after 5 iterations in 4.350 seconds.
Creating 10 corresponding optimal policies...
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.969 }	0.186	0.307	0.944	-0.034	0.086	0.256	0.344	-0.208	-1.532	-0.372	0.108	-0.760	-0.011	
	{ -0.124 }	 (random)
	{ -0.117 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.435 }	-0.466	-1.404	-1.598	-0.099	0.858	0.067	1.303	0.192	-1.822	-0.732	0.472	0.623	1.637	
 (1)	{ 0.954 }	0.196	0.246	0.772	-0.096	0.031	0.260	0.071	-0.195	-1.572	-0.106	0.172	-0.238	0.017	
 (2)	{ 0.686 }	0.174	-0.178	0.824	0.083	0.320	0.550	0.535	-0.072	-1.044	-0.334	-0.099	0.685	0.185	
 (3)	{ 0.847 }	0.257	0.966	1.832	0.172	-0.212	0.017	0.435	-0.698	-1.384	-0.830	-0.106	0.187	-0.284	
 (4)	{ 0.915 }	0.369	0.415	0.496	-0.430	-0.047	-0.601	0.600	-0.246	-2.006	-0.698	0.379	-1.071	0.060	
 (5)	{ 0.638 }	0.205	-0.104	0.053	-0.290	0.215	-0.650	0.941	-0.170	-2.667	-1.009	0.592	-0.332	0.341	
 (6)	{ 0.921 }	0.838	0.005	0.724	-0.676	0.023	0.970	0.176	-0.085	-1.497	-0.169	0.131	-1.874	0.287	
 (7)	{ 0.649 }	-0.240	-0.290	1.232	-0.076	0.019	0.708	1.122	0.731	-0.672	-0.687	-0.311	0.022	0.566	
 (8)	{ 0.920 }	0.198	-0.063	0.550	-0.199	0.371	0.798	0.626	-0.613	-2.717	-0.180	0.536	-0.578	0.562	
 (9)	{ 0.851 }	-0.014	0.208	1.208	0.405	0.058	1.216	0.344	-0.483	-0.867	-0.259	-0.153	-0.577	0.128	
Scores for the experts true policies:
	{ 0.826 }	 (expert 0)
	{ 0.853 }	 (expert 1)
	{ 0.861 }	 (expert 2)
	{ 0.702 }	 (expert 3)
Generating 4 expert demonstrations of size 10...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.940	0.600	1.112	1.239	1.491	3.916	3.680	2.450	5.065	4.029	
3.463	0.581	1.280	1.020	1.323	2.989	3.173	2.708	4.167	3.789	
3.622	0.619	1.689	4.269	1.106	5.771	8.999	3.831	9.297	10.481	
4.249	0.536	1.252	1.101	1.404	2.818	2.842	2.412	1.965	2.258	
3.224	0.342	0.991	1.091	1.392	3.186	4.339	2.095	4.923	4.772	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.006	0.607	0.133	0.106	0.100	0.007	0.005	0.021	0.009	0.006	
LSPI converged after 5 iterations in 4.600 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.706
MEAN RF POLICY* SCORE IN MDP: 0.948

Loss matrix
5.327	0.721	1.530	0.737	1.849	2.795	1.442	1.728	3.615	1.607	
5.743	0.831	1.398	0.813	2.036	3.032	1.798	1.938	3.793	1.625	
5.823	0.715	1.424	0.671	2.076	3.097	1.711	1.735	3.878	1.365	
5.950	0.781	1.540	0.788	1.988	3.117	1.701	1.947	3.815	1.496	
6.062	0.783	1.643	0.669	1.814	2.885	1.653	1.926	4.153	2.126	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.056	0.001	0.938	0.000	0.000	0.000	0.000	0.000	0.005	
LSPI converged after 5 iterations in 4.440 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.499
MEAN RF POLICY* SCORE IN MDP: 0.827

Loss matrix
2.841	0.551	1.280	1.994	0.873	1.329	1.003	1.814	1.141	1.756	
3.339	0.494	1.106	1.610	0.699	1.270	1.112	1.749	0.915	1.569	
2.945	0.547	1.065	1.561	0.761	1.121	1.045	1.707	1.015	1.441	
3.161	0.744	1.097	1.769	1.067	1.237	1.158	1.783	1.083	1.664	
3.437	0.622	1.133	1.852	0.840	1.224	1.268	1.748	0.934	1.499	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.815	0.017	0.002	0.094	0.012	0.022	0.002	0.032	0.004	
LSPI converged after 5 iterations in 4.590 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.366
MEAN RF POLICY* SCORE IN MDP: 0.939

Loss matrix
2.835	1.102	1.709	2.394	1.405	1.789	1.839	2.471	1.974	1.820	
3.051	1.135	1.739	2.237	1.470	1.770	1.959	2.188	2.039	1.797	
3.043	1.110	1.633	2.036	1.290	1.671	1.835	2.791	2.131	1.837	
4.173	1.235	1.691	2.142	1.438	1.876	1.887	2.981	2.137	2.104	
2.745	1.291	1.736	2.309	1.587	1.766	1.804	3.289	2.510	1.867	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.021	0.425	0.082	0.045	0.152	0.072	0.064	0.029	0.047	0.063	
LSPI converged after 4 iterations in 3.790 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.323
MEAN RF POLICY* SCORE IN MDP: 0.937

LSPI converged after 5 iterations in 4.490 seconds.
~~~ MAGIC SCORE ~~~	0.935
Generating 4 expert demonstrations of size 10...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
2.958	0.751	1.002	3.655	3.842	2.228	12.738	2.342	20.165	9.737	
2.701	0.585	0.958	0.843	1.223	1.923	5.027	2.234	5.472	3.962	
3.124	0.638	1.095	1.089	1.539	2.638	3.876	2.062	3.904	3.683	
5.089	0.555	1.273	1.203	1.469	4.384	4.138	2.160	3.190	4.458	
3.817	0.915	1.226	0.839	1.436	3.468	3.155	2.360	3.577	3.491	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.010	0.552	0.149	0.151	0.070	0.026	0.003	0.032	0.004	0.003	
LSPI converged after 5 iterations in 4.460 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.722
MEAN RF POLICY* SCORE IN MDP: 0.943

Loss matrix
5.634	0.729	1.387	0.728	2.096	3.123	1.615	1.839	3.869	1.599	
5.650	0.768	1.579	0.718	1.980	2.969	1.608	2.201	3.796	1.849	
5.573	0.805	1.164	0.763	1.822	2.797	1.543	1.903	3.878	2.022	
5.488	0.704	1.350	0.753	2.138	3.077	1.632	1.609	3.759	1.227	
5.252	0.689	1.605	0.691	1.972	2.929	1.355	1.549	3.504	1.561	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.929	0.006	0.063	0.000	0.000	0.001	0.000	0.000	0.001	
LSPI converged after 5 iterations in 4.580 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.108
MEAN RF POLICY* SCORE IN MDP: 0.947

Loss matrix
2.518	0.366	0.934	1.625	0.790	1.200	1.188	1.590	0.982	1.251	
2.982	0.425	1.217	1.913	0.780	1.423	1.042	1.927	0.899	1.714	
3.251	0.648	1.155	1.663	0.986	1.524	1.278	1.829	0.999	1.723	
2.222	0.425	1.063	1.312	1.000	1.782	0.903	1.808	0.949	1.633	
2.326	0.525	0.951	1.461	0.963	1.125	0.930	1.543	0.954	1.141	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.855	0.019	0.003	0.053	0.007	0.023	0.002	0.033	0.005	
LSPI converged after 5 iterations in 4.590 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.338
MEAN RF POLICY* SCORE IN MDP: 0.944

Loss matrix
4.603	1.046	1.643	2.170	1.420	1.795	1.883	2.020	1.943	1.570	
3.056	1.095	1.593	2.072	1.361	1.801	1.817	2.341	1.617	1.553	
2.932	1.193	1.920	2.569	1.458	1.769	1.906	2.579	2.231	2.257	
3.286	1.104	1.754	2.229	1.518	1.844	1.858	2.013	2.054	1.878	
3.445	1.121	1.548	2.130	1.513	1.903	1.813	2.472	1.990	1.755	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.021	0.423	0.082	0.045	0.133	0.066	0.062	0.043	0.057	0.069	
LSPI converged after 4 iterations in 3.770 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.309
MEAN RF POLICY* SCORE IN MDP: 0.936

LSPI converged after 4 iterations in 3.810 seconds.
~~~ MAGIC SCORE ~~~	0.948
Generating 4 expert demonstrations of size 20...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.697	1.213	3.007	2.361	2.248	3.001	1.741	4.199	2.818	1.893	
5.583	1.586	5.276	2.413	2.220	3.228	1.755	7.899	2.593	1.681	
4.856	1.543	3.673	3.049	2.172	3.212	1.782	4.963	2.233	1.267	
3.042	1.102	3.841	2.414	2.162	2.430	1.784	5.465	2.262	1.381	
3.025	1.506	2.181	2.692	2.377	2.881	1.733	2.310	2.698	2.628	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.007	0.597	0.015	0.026	0.045	0.017	0.099	0.009	0.029	0.157	
LSPI converged after 5 iterations in 4.610 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.664
MEAN RF POLICY* SCORE IN MDP: 0.931

Loss matrix
4.298	0.862	1.396	1.640	1.391	1.647	1.327	1.649	1.978	1.659	
4.591	0.911	1.546	1.743	1.549	1.954	1.394	1.750	2.181	1.543	
4.183	0.779	1.324	1.422	1.454	1.747	1.326	2.011	1.972	1.596	
4.389	0.917	1.435	1.768	1.788	2.027	1.547	1.774	1.912	1.634	
4.368	0.745	1.352	1.491	1.411	1.711	1.303	1.674	1.993	1.515	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.992	0.001	0.000	0.001	0.000	0.005	0.000	0.000	0.000	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.137
MEAN RF POLICY* SCORE IN MDP: 0.948

Loss matrix
2.542	0.872	1.324	1.575	1.841	2.290	2.145	2.343	2.428	1.300	
2.605	0.786	1.206	1.628	1.815	2.348	1.948	2.201	2.337	1.400	
2.384	0.772	1.191	1.597	1.772	2.245	1.862	2.377	2.116	1.217	
2.081	0.985	1.308	1.748	1.804	2.174	2.082	2.163	2.371	1.434	
2.992	0.914	1.387	1.798	2.197	2.634	2.130	2.584	3.095	1.518	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.931	0.037	0.006	0.002	0.000	0.001	0.000	0.000	0.021	
LSPI converged after 6 iterations in 5.320 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.219
MEAN RF POLICY* SCORE IN MDP: 0.950

Loss matrix
3.618	1.109	1.725	1.337	1.956	3.034	1.675	2.024	2.359	1.507	
3.136	1.002	1.691	1.402	1.217	2.188	1.555	2.340	2.163	1.466	
3.433	0.935	1.637	1.423	1.506	2.603	1.577	2.073	2.289	1.670	
3.846	1.093	1.649	1.482	1.454	2.626	1.692	2.353	2.279	1.620	
3.430	1.023	1.653	1.457	1.360	2.241	1.529	2.182	2.159	1.559	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.008	0.428	0.071	0.112	0.110	0.032	0.077	0.041	0.038	0.082	
LSPI converged after 5 iterations in 4.600 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.245
MEAN RF POLICY* SCORE IN MDP: 0.938

LSPI converged after 6 iterations in 5.320 seconds.
~~~ MAGIC SCORE ~~~	0.943
Generating 4 expert demonstrations of size 20...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.663	1.602	2.901	2.985	2.293	2.824	1.735	4.314	2.444	1.802	
4.623	1.152	4.139	2.297	2.211	2.728	1.822	5.915	2.462	1.298	
3.485	1.489	2.281	2.376	2.473	3.219	1.855	3.002	3.044	1.927	
2.985	1.005	3.428	2.430	2.074	2.793	1.547	4.790	2.212	1.212	
4.004	2.249	4.691	3.721	3.026	3.119	3.909	5.486	6.656	5.375	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.008	0.630	0.015	0.026	0.043	0.021	0.090	0.004	0.023	0.138	
LSPI converged after 5 iterations in 4.540 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.656
MEAN RF POLICY* SCORE IN MDP: 0.939

Loss matrix
4.501	0.819	1.337	1.515	1.668	1.914	1.463	2.066	1.669	1.701	
4.330	0.854	1.414	1.649	1.553	1.780	1.397	1.997	1.938	1.573	
4.060	0.831	1.285	1.522	1.389	1.618	1.311	1.846	1.791	1.733	
4.378	0.825	1.411	1.478	1.483	1.698	1.394	1.737	1.993	1.718	
4.457	0.849	1.345	1.565	1.376	1.842	1.333	1.793	1.974	1.582	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.990	0.007	0.000	0.001	0.000	0.001	0.000	0.000	0.000	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.136
MEAN RF POLICY* SCORE IN MDP: 0.947

Loss matrix
2.414	0.728	1.307	1.512	1.745	2.131	1.882	2.411	2.302	1.231	
2.351	1.066	1.239	1.478	2.043	2.421	2.154	2.102	2.701	1.307	
1.999	0.846	1.253	1.752	1.632	1.954	1.972	2.207	2.080	1.320	
2.285	0.680	1.216	1.610	1.614	2.022	1.857	2.200	1.982	1.280	
2.350	0.838	1.232	1.617	1.774	2.175	1.956	2.088	2.192	1.266	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.933	0.031	0.008	0.004	0.001	0.001	0.000	0.001	0.022	
LSPI converged after 6 iterations in 5.380 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.219
MEAN RF POLICY* SCORE IN MDP: 0.948

Loss matrix
3.537	1.003	1.710	1.465	1.563	2.818	1.614	2.011	2.341	1.490	
3.274	1.099	1.795	1.501	1.528	2.493	1.707	2.236	2.552	1.526	
3.527	1.039	1.699	1.503	1.741	2.745	1.553	2.268	2.237	1.695	
3.390	1.074	1.626	1.589	1.231	2.090	1.626	1.957	2.155	1.349	
3.560	0.999	1.717	1.644	1.570	2.727	1.707	1.931	2.516	1.482	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.005	0.462	0.067	0.093	0.097	0.027	0.074	0.046	0.032	0.098	
LSPI converged after 4 iterations in 3.780 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.211
MEAN RF POLICY* SCORE IN MDP: 0.940

LSPI converged after 6 iterations in 5.270 seconds.
~~~ MAGIC SCORE ~~~	0.947
Generating 4 expert demonstrations of size 50...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.636	1.382	1.480	1.629	2.157	2.910	1.797	1.751	2.767	1.154	
3.846	1.438	1.406	1.639	2.171	2.920	1.808	1.768	3.001	1.123	
3.495	1.357	1.470	1.697	2.184	2.860	1.850	1.904	2.760	1.301	
3.639	1.327	1.413	1.589	2.113	2.822	1.868	1.874	3.065	1.236	
3.417	1.580	1.510	1.808	2.231	2.855	2.130	1.893	3.328	1.372	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.002	0.119	0.103	0.066	0.031	0.010	0.045	0.048	0.012	0.564	
LSPI converged after 5 iterations in 4.650 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.687
MEAN RF POLICY* SCORE IN MDP: 0.860

Loss matrix
4.059	0.763	1.501	1.107	1.749	2.539	1.495	1.958	2.460	1.286	
4.016	0.797	1.393	1.219	1.655	2.387	1.432	2.116	2.426	1.271	
4.043	0.857	1.435	1.083	1.756	2.476	1.526	2.021	2.571	1.196	
4.079	0.855	1.422	1.235	1.899	2.626	1.539	2.091	2.631	1.231	
4.341	0.866	1.436	1.131	1.857	2.630	1.580	1.969	2.747	1.277	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.985	0.001	0.010	0.000	0.000	0.001	0.000	0.000	0.003	
LSPI converged after 4 iterations in 3.770 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.146
MEAN RF POLICY* SCORE IN MDP: 0.952

Loss matrix
4.817	2.515	3.803	2.712	2.893	3.651	3.156	2.539	4.715	2.835	
3.595	2.419	2.452	2.728	2.903	3.580	3.064	2.959	4.108	2.782	
3.510	2.484	2.269	2.737	2.943	3.589	3.455	2.992	4.378	2.667	
3.481	2.361	2.644	2.623	2.755	3.418	3.121	2.881	4.085	2.790	
3.409	2.734	2.465	2.872	2.818	3.533	3.556	2.944	4.728	2.767	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.002	0.997	0.000	0.000	0.000	0.000	0.000	0.000	0.000	
LSPI converged after 4 iterations in 3.740 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.865
MEAN RF POLICY* SCORE IN MDP: 0.687

Loss matrix
3.123	1.243	1.848	2.018	2.024	2.708	1.767	2.102	2.477	1.506	
3.227	1.347	1.881	2.036	2.221	2.952	1.997	2.193	2.486	1.577	
3.245	1.527	1.826	1.990	2.187	2.945	1.943	2.188	2.465	1.522	
3.252	1.295	1.829	1.970	2.250	2.999	1.966	2.121	2.538	1.512	
3.230	1.317	1.782	1.901	2.194	2.938	1.937	2.202	2.517	1.472	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.002	0.484	0.082	0.060	0.044	0.013	0.074	0.045	0.028	0.169	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.158
MEAN RF POLICY* SCORE IN MDP: 0.931

LSPI converged after 5 iterations in 4.500 seconds.
~~~ MAGIC SCORE ~~~	0.847
Generating 4 expert demonstrations of size 50...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.399	1.399	1.464	1.733	2.106	2.790	1.840	1.898	2.828	1.304	
3.646	1.242	1.489	1.685	2.147	2.909	1.789	1.887	2.768	1.281	
3.553	1.421	1.479	1.755	2.142	2.856	1.789	1.846	2.692	1.306	
3.568	1.346	1.440	1.717	2.189	2.928	1.818	1.743	2.734	1.251	
3.767	1.526	1.488	1.750	2.261	3.009	1.843	1.939	2.902	1.327	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.003	0.491	0.102	0.063	0.032	0.009	0.051	0.047	0.014	0.188	
LSPI converged after 5 iterations in 4.490 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.625
MEAN RF POLICY* SCORE IN MDP: 0.935

Loss matrix
4.323	0.886	1.439	1.225	1.846	2.617	1.503	2.087	2.473	1.296	
4.225	0.854	1.401	1.182	2.023	2.761	1.575	1.784	2.792	1.136	
4.414	0.900	1.505	1.141	1.904	2.702	1.600	2.176	2.838	1.248	
4.150	0.850	1.385	1.090	1.840	2.636	1.489	2.036	2.458	1.263	
3.941	0.774	1.394	1.155	1.831	2.571	1.457	1.944	2.460	1.183	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.986	0.001	0.009	0.000	0.000	0.001	0.000	0.000	0.004	
LSPI converged after 4 iterations in 3.780 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.146
MEAN RF POLICY* SCORE IN MDP: 0.954

Loss matrix
3.640	1.969	2.497	2.454	2.591	3.186	2.626	2.773	3.891	2.354	
3.750	2.657	2.496	2.906	3.078	3.775	3.501	2.966	4.706	2.817	
3.992	2.343	3.178	2.657	2.655	3.370	2.899	2.819	4.294	2.766	
3.607	2.466	3.824	2.659	2.884	3.656	3.053	2.989	4.273	2.626	
3.304	2.733	3.449	2.867	2.736	3.374	3.574	2.829	4.586	2.730	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.998	0.000	0.000	0.000	0.000	0.000	0.000	0.000	0.001	
LSPI converged after 4 iterations in 3.800 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.235
MEAN RF POLICY* SCORE IN MDP: 0.947

Loss matrix
3.191	1.139	1.787	1.927	1.936	2.578	1.865	2.022	2.288	1.451	
3.280	1.161	1.832	1.986	2.085	2.806	1.894	1.961	2.605	1.565	
3.186	1.384	1.859	2.045	2.128	2.863	1.945	2.126	2.574	1.558	
3.366	1.305	1.851	1.980	1.990	2.693	1.838	1.988	2.472	1.598	
3.263	1.372	1.760	1.970	2.213	2.926	1.978	2.184	2.586	1.513	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.004	0.501	0.082	0.055	0.048	0.018	0.063	0.050	0.029	0.150	
LSPI converged after 5 iterations in 4.520 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.157
MEAN RF POLICY* SCORE IN MDP: 0.924

LSPI converged after 6 iterations in 5.430 seconds.
~~~ MAGIC SCORE ~~~	0.944
Generating 4 expert demonstrations of size 100...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.905	1.567	1.525	1.742	2.242	3.171	1.921	2.179	2.846	1.246	
3.949	1.724	1.511	1.683	2.203	3.171	1.874	2.178	2.767	1.242	
3.772	1.528	1.450	1.679	2.250	3.149	1.823	2.050	2.689	1.109	
3.897	1.654	1.471	1.735	2.258	3.152	1.869	2.255	2.775	1.324	
3.797	1.618	1.496	1.761	2.208	3.092	1.875	2.107	2.675	1.219	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.001	0.082	0.117	0.063	0.027	0.007	0.050	0.033	0.015	0.605	
LSPI converged after 5 iterations in 4.620 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.703
MEAN RF POLICY* SCORE IN MDP: 0.957

Loss matrix
4.466	0.867	1.267	1.378	1.429	2.241	1.425	2.171	2.604	1.119	
4.151	0.780	1.278	1.297	1.359	2.125	1.404	2.141	2.344	1.001	
4.463	0.845	1.250	1.297	1.523	2.347	1.492	2.068	2.493	1.072	
4.206	0.879	1.306	1.293	1.488	2.249	1.505	2.176	2.465	1.132	
4.183	0.772	1.219	1.380	1.517	2.329	1.414	2.063	2.306	0.963	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.981	0.003	0.001	0.001	0.000	0.001	0.000	0.000	0.014	
LSPI converged after 4 iterations in 3.920 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.149
MEAN RF POLICY* SCORE IN MDP: 0.951

Loss matrix
3.396	2.001	1.949	2.975	2.897	3.302	3.309	2.404	5.410	3.662	
3.375	1.520	1.968	2.345	2.631	3.244	3.085	2.510	4.704	3.090	
3.523	2.205	2.085	2.379	3.031	3.306	3.573	2.413	6.436	3.606	
3.578	2.174	1.929	2.359	2.915	3.345	3.537	2.411	6.224	3.706	
3.454	1.536	2.005	2.314	2.594	3.271	3.280	2.531	5.299	3.248	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.984	0.014	0.001	0.000	0.000	0.000	0.001	0.000	0.000	
LSPI converged after 5 iterations in 4.640 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.235
MEAN RF POLICY* SCORE IN MDP: 0.957

Loss matrix
2.840	1.450	1.647	2.188	2.539	3.187	2.178	2.196	2.582	1.591	
2.912	1.369	1.669	2.068	2.509	3.168	2.164	2.128	2.567	1.584	
2.851	1.378	1.652	2.149	2.502	3.122	2.177	2.163	2.589	1.604	
2.845	1.440	1.543	2.033	2.446	3.061	2.122	2.077	2.516	1.478	
2.897	1.406	1.595	2.081	2.495	3.119	2.121	2.102	2.551	1.535	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.012	0.489	0.132	0.063	0.029	0.003	0.048	0.050	0.022	0.154	
LSPI converged after 5 iterations in 4.580 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.150
MEAN RF POLICY* SCORE IN MDP: 0.938

LSPI converged after 5 iterations in 4.570 seconds.
~~~ MAGIC SCORE ~~~	0.939
Generating 4 expert demonstrations of size 100...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.912	1.659	1.484	1.732	2.231	3.173	1.894	2.125	2.815	1.231	
3.703	1.645	1.490	1.750	2.147	3.021	1.902	2.211	2.657	1.288	
3.865	1.712	1.461	1.666	2.227	3.150	1.862	2.169	2.712	1.149	
3.792	1.551	1.439	1.640	2.127	3.009	1.841	2.182	2.614	1.165	
3.787	1.563	1.504	1.774	2.288	3.204	1.915	2.179	2.696	1.227	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.001	0.078	0.122	0.065	0.028	0.007	0.048	0.031	0.015	0.603	
LSPI converged after 5 iterations in 4.580 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.706
MEAN RF POLICY* SCORE IN MDP: 0.959

Loss matrix
4.608	0.789	1.243	1.295	1.501	2.354	1.455	2.036	2.531	1.003	
4.251	0.823	1.247	1.325	1.421	2.188	1.437	2.042	2.417	1.053	
4.748	0.869	1.277	1.264	1.437	2.262	1.487	2.135	2.587	1.068	
4.197	0.820	1.250	1.364	1.453	2.255	1.421	1.993	2.372	1.004	
4.188	0.759	1.240	1.172	1.517	2.301	1.441	2.022	2.351	0.987	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.979	0.002	0.002	0.001	0.000	0.001	0.000	0.000	0.015	
LSPI converged after 4 iterations in 3.800 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.149
MEAN RF POLICY* SCORE IN MDP: 0.946

Loss matrix
3.404	1.443	1.930	2.229	2.555	3.199	3.235	2.422	4.857	3.089	
3.558	1.933	2.046	2.784	2.750	3.413	3.153	2.531	4.915	3.493	
3.489	1.528	1.979	2.255	2.604	3.287	3.241	2.422	5.384	3.027	
3.432	2.444	1.918	3.498	3.322	3.392	3.628	2.374	6.444	4.225	
3.415	1.613	2.042	2.368	2.583	3.266	2.962	2.504	4.219	3.083	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.986	0.011	0.001	0.000	0.000	0.000	0.001	0.000	0.000	
LSPI converged after 5 iterations in 4.590 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.234
MEAN RF POLICY* SCORE IN MDP: 0.954

Loss matrix
2.706	1.269	1.620	2.053	2.441	3.047	2.114	2.145	2.456	1.523	
2.880	1.397	1.637	2.127	2.572	3.231	2.187	2.213	2.651	1.553	
2.935	1.320	1.594	2.047	2.481	3.130	2.133	2.096	2.566	1.515	
2.953	1.375	1.571	1.998	2.407	3.044	2.064	2.044	2.506	1.524	
2.817	1.337	1.619	2.034	2.406	3.026	2.055	2.043	2.419	1.563	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.012	0.490	0.125	0.064	0.029	0.005	0.050	0.050	0.025	0.150	
LSPI converged after 5 iterations in 4.650 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.152
MEAN RF POLICY* SCORE IN MDP: 0.936

LSPI converged after 5 iterations in 4.690 seconds.
~~~ MAGIC SCORE ~~~	0.937
Process 19816 detected
