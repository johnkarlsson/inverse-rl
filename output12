Generating LSTDQ demonstrations...
2177 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.160	0.242	0.904	-0.021	0.076	0.159	0.128	-0.226	-1.371	-0.176	0.119	-0.489	0.018	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.052	0.330	1.011	0.005	0.030	0.179	-0.164	-0.297	-1.127	-0.093	0.105	-0.486	-0.012	
Sampling 10 reward functions...
	Sampled reward functions:
	 (0)	-0.235	0.219	1.550	0.240	-0.175	0.041	0.146	0.351	-0.525	-0.167	-0.173	-3.607	0.136	
	 (1)	0.006	0.696	0.945	0.253	-0.225	-1.017	-0.001	-0.342	-1.425	0.039	0.062	-3.494	-0.188	
	 (2)	0.067	0.037	0.705	-0.185	0.070	0.390	-0.167	-0.145	-0.910	0.568	-0.043	-0.415	0.013	
	 (3)	-0.119	-0.679	0.487	-0.026	0.392	2.155	0.098	-0.609	-0.446	0.592	-0.133	0.744	0.332	
	 (4)	-0.220	-0.334	0.774	-0.367	0.224	1.995	0.379	0.976	-0.200	0.148	-0.614	-1.326	0.467	
	 (5)	-0.016	-0.277	1.561	0.446	0.010	2.634	0.039	-0.976	-0.696	0.514	-0.012	-1.798	0.022	
	 (6)	0.121	0.202	1.381	-0.047	-0.165	-0.696	-0.128	-0.116	-1.358	0.126	0.365	-1.007	-0.343	
	 (7)	-0.088	-0.315	1.124	-0.015	0.178	-0.423	0.131	-0.097	-0.956	0.006	0.011	1.294	0.218	
	 (8)	0.570	0.909	2.377	-0.258	-0.371	-0.814	-0.538	-0.004	-1.643	-0.244	0.289	-1.073	-0.839	
	 (9)	-0.062	0.285	1.139	0.146	-0.027	0.629	0.081	-0.155	-1.029	-0.019	-0.097	0.711	0.052	
LSPI converged after 5 iterations in 4.310 seconds.
Creating 10 corresponding optimal policies...
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.969 }	0.191	0.292	0.900	-0.024	0.070	0.298	0.331	-0.199	-1.528	-0.367	0.129	-0.787	0.010	
	{ -0.124 }	 (random)
	{ -0.135 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.919 }	-0.063	0.718	1.587	0.252	-0.220	0.266	0.611	0.065	-0.997	-0.732	-0.161	-3.347	0.299	
 (1)	{ 0.722 }	0.275	0.973	1.021	0.094	-0.335	-0.884	0.314	-0.645	-1.853	-0.620	0.197	-3.585	-0.262	
 (2)	{ 0.868 }	0.241	0.310	0.462	-0.136	0.173	0.447	0.162	-0.329	-1.535	0.119	0.052	-1.454	0.001	
 (3)	{ 0.569 }	0.332	-0.460	-0.202	-0.063	0.714	2.286	0.601	-0.843	-1.413	-0.373	-0.015	-0.905	0.490	
 (4)	{ 0.872 }	-0.047	-0.108	0.661	-0.151	0.506	2.208	1.533	0.714	-0.648	-0.987	-0.975	-2.864	0.879	
 (5)	{ 0.844 }	0.552	0.203	1.199	0.378	0.114	2.730	0.600	-1.305	-1.698	-0.650	0.152	-2.594	0.067	
 (6)	{ 0.417 }	0.515	0.546	0.981	-0.440	-0.180	-0.734	0.629	-0.383	-2.896	-0.608	0.877	-2.189	-0.656	
 (7)	{ 0.831 }	0.249	-0.041	0.481	-0.253	0.395	-0.100	0.639	-0.317	-2.086	-0.635	0.276	-0.260	0.302	
 (8)	{ 0.731 }	1.227	1.605	2.132	-0.541	-0.396	-0.638	0.213	-0.625	-3.648	-0.874	0.848	-2.077	-1.509	
 (9)	{ 0.877 }	0.322	0.655	0.942	0.135	0.055	0.920	0.670	-0.488	-1.800	-0.815	-0.031	-0.821	0.139	
Scores for the experts true policies:
	{ 0.821 }	 (expert 0)
	{ 0.862 }	 (expert 1)
	{ 0.861 }	 (expert 2)
	{ 0.702 }	 (expert 3)
Generating 4 expert demonstrations of size 10...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.276	4.502	3.451	4.115	8.132	7.993	8.234	3.146	4.855	3.187	
2.226	2.075	1.695	3.048	10.359	3.850	5.984	4.232	4.467	3.250	
1.949	2.340	2.251	3.477	8.412	5.076	5.361	3.029	4.092	3.461	
9.500	10.573	7.060	4.987	14.848	15.784	11.131	7.101	5.067	5.205	
12.365	4.597	6.319	8.292	19.850	8.171	24.088	9.816	4.622	7.842	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.269	0.179	0.343	0.048	0.000	0.010	0.002	0.060	0.029	0.060	
LSPI converged after 5 iterations in 4.610 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.874
MEAN RF POLICY* SCORE IN MDP: 0.930

Loss matrix
4.787	2.946	6.860	25.454	28.606	29.127	7.874	2.297	9.484	14.434	
3.592	1.041	2.078	3.735	5.484	6.137	2.001	1.961	2.826	3.536	
5.361	1.068	5.221	10.207	14.710	13.267	2.495	1.795	2.206	8.155	
4.188	1.003	2.896	6.106	9.565	9.059	1.986	1.748	2.332	5.156	
4.076	1.736	2.847	13.316	12.659	15.767	4.225	2.835	5.801	6.696	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.988	0.000	0.000	0.000	0.000	0.002	0.009	0.000	0.000	
LSPI converged after 5 iterations in 4.740 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.483
MEAN RF POLICY* SCORE IN MDP: 0.743

Loss matrix
1.567	1.195	0.671	2.454	4.306	3.170	1.740	1.591	2.878	1.648	
1.359	1.240	0.565	2.136	4.696	2.884	1.823	1.503	1.945	1.528	
1.386	1.044	0.564	2.211	3.966	2.675	1.548	1.410	2.776	1.461	
1.360	0.945	0.421	2.088	3.907	2.537	1.770	1.407	3.146	1.390	
1.402	0.895	0.555	2.146	4.077	2.486	1.564	1.323	2.659	1.418	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.015	0.091	0.862	0.001	0.000	0.000	0.005	0.013	0.000	0.011	
LSPI converged after 5 iterations in 4.600 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.404
MEAN RF POLICY* SCORE IN MDP: 0.883

Loss matrix
2.652	1.865	1.102	2.037	3.457	3.042	2.011	1.677	4.099	2.598	
2.613	1.697	1.028	2.040	3.705	2.950	1.961	1.570	3.815	2.491	
3.195	2.167	1.291	2.185	3.752	3.409	2.006	1.892	2.912	2.958	
2.976	1.609	1.098	2.182	4.161	3.238	1.793	1.561	4.052	2.579	
3.093	2.008	1.208	2.092	3.886	3.312	1.890	1.745	2.927	2.817	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.046	0.108	0.374	0.084	0.027	0.038	0.100	0.137	0.032	0.053	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.502
MEAN RF POLICY* SCORE IN MDP: 0.925

LSPI converged after 5 iterations in 4.630 seconds.
~~~ MAGIC SCORE ~~~	0.939
Generating 4 expert demonstrations of size 10...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
4.394	2.434	2.136	4.210	13.453	5.365	5.078	3.635	5.404	3.754	
4.313	2.189	2.053	3.868	14.929	5.423	8.753	4.306	3.835	5.824	
3.101	1.980	1.865	3.473	5.581	4.065	6.780	2.968	4.160	2.764	
4.139	2.494	2.043	3.306	12.041	7.778	7.508	4.560	5.298	6.128	
2.792	2.019	1.801	3.759	6.916	5.304	5.456	2.753	4.167	3.586	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.039	0.311	0.517	0.035	0.001	0.005	0.002	0.045	0.013	0.032	
LSPI converged after 5 iterations in 4.570 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.866
MEAN RF POLICY* SCORE IN MDP: 0.946

Loss matrix
4.622	2.143	4.819	20.128	19.773	23.613	5.350	4.892	8.897	10.519	
3.377	1.019	1.875	3.086	5.026	5.572	1.844	1.872	2.860	3.146	
5.269	1.361	4.645	8.290	13.719	12.066	2.839	1.973	3.834	7.064	
3.687	1.316	2.708	5.748	8.504	8.189	2.417	1.884	3.342	4.651	
3.923	1.529	2.134	6.244	5.382	7.644	1.926	2.420	3.888	3.268	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.981	0.003	0.000	0.000	0.000	0.006	0.010	0.000	0.000	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.481
MEAN RF POLICY* SCORE IN MDP: 0.721

Loss matrix
1.375	0.887	0.660	1.970	4.362	2.122	1.303	1.500	2.198	1.627	
1.418	1.077	0.536	2.501	4.192	3.094	1.813	2.060	3.248	1.490	
1.412	1.388	0.532	2.154	3.988	3.020	1.561	1.451	1.759	1.621	
1.356	0.909	0.577	2.210	4.469	2.221	1.593	1.563	2.966	1.570	
1.377	1.274	0.596	2.270	4.805	2.916	1.950	1.910	2.839	1.454	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.020	0.095	0.855	0.001	0.000	0.000	0.010	0.007	0.001	0.010	
LSPI converged after 4 iterations in 3.890 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.402
MEAN RF POLICY* SCORE IN MDP: 0.822

Loss matrix
2.690	1.719	1.033	2.168	3.559	2.996	1.807	1.630	2.747	2.484	
3.124	1.720	1.136	2.126	4.162	3.183	1.907	1.527	4.207	2.630	
2.641	1.652	1.044	2.019	3.731	3.200	1.833	1.543	3.469	2.447	
2.462	1.496	0.926	2.079	3.619	2.844	1.876	1.404	4.577	2.347	
2.720	1.791	1.127	2.112	3.657	3.044	1.921	1.688	2.920	2.562	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.048	0.119	0.364	0.078	0.026	0.039	0.096	0.141	0.031	0.057	
LSPI converged after 5 iterations in 4.570 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.494
MEAN RF POLICY* SCORE IN MDP: 0.922

LSPI converged after 5 iterations in 4.560 seconds.
~~~ MAGIC SCORE ~~~	0.932
Generating 4 expert demonstrations of size 20...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
2.854	3.102	1.690	2.875	3.600	4.041	2.624	2.046	3.260	2.564	
4.313	4.175	1.561	2.706	3.906	3.784	2.602	2.164	3.496	2.815	
3.461	3.937	1.588	2.929	3.225	3.829	2.437	2.100	3.012	2.357	
2.259	2.410	1.546	2.854	3.080	4.148	2.459	1.905	2.890	2.340	
4.021	4.042	1.717	3.121	3.318	4.125	2.711	2.179	3.502	2.583	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.030	0.023	0.544	0.038	0.019	0.010	0.064	0.178	0.024	0.069	
LSPI converged after 5 iterations in 4.510 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 1.080
MEAN RF POLICY* SCORE IN MDP: 0.910

Loss matrix
2.184	1.875	0.903	1.701	2.815	4.002	2.373	1.249	3.462	2.702	
2.355	2.104	1.396	1.971	3.733	4.562	2.619	1.502	3.712	3.089	
2.601	2.222	1.103	1.671	3.256	3.698	2.487	1.369	3.802	2.748	
2.334	1.897	1.040	1.666	4.435	3.934	2.467	1.410	4.006	3.166	
2.391	2.360	1.282	1.672	4.259	3.945	2.467	1.531	3.977	3.023	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.002	0.886	0.013	0.000	0.000	0.000	0.098	0.000	0.000	
LSPI converged after 4 iterations in 3.830 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.455
MEAN RF POLICY* SCORE IN MDP: 0.863

Loss matrix
2.033	1.448	1.079	4.203	4.472	3.974	1.508	2.189	1.692	2.208	
1.835	1.752	1.128	4.175	4.400	3.693	1.374	1.916	1.633	2.144	
1.816	1.090	1.108	3.830	4.255	3.839	1.626	2.057	1.642	2.210	
1.722	1.351	1.124	4.399	4.125	3.797	1.431	1.953	1.541	2.143	
1.912	1.045	1.151	4.176	4.177	4.067	1.528	1.986	1.577	2.294	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.018	0.280	0.556	0.000	0.000	0.000	0.086	0.010	0.045	0.005	
LSPI converged after 5 iterations in 4.600 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.501
MEAN RF POLICY* SCORE IN MDP: 0.825

Loss matrix
2.068	2.130	1.433	2.584	3.076	3.274	2.118	2.081	1.843	2.654	
2.080	2.294	1.509	2.646	3.266	3.367	2.249	2.151	2.068	2.715	
2.074	2.123	1.492	2.625	3.212	3.314	2.215	2.098	2.005	2.695	
2.124	2.230	1.537	2.604	2.802	3.363	2.334	2.161	2.017	2.726	
1.828	2.126	1.446	2.689	2.806	3.274	2.254	2.133	2.020	2.654	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.103	0.084	0.327	0.060	0.047	0.040	0.081	0.089	0.111	0.058	
LSPI converged after 5 iterations in 4.580 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.577
MEAN RF POLICY* SCORE IN MDP: 0.935

LSPI converged after 5 iterations in 4.780 seconds.
~~~ MAGIC SCORE ~~~	0.887
Generating 4 expert demonstrations of size 20...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.891	3.710	1.636	2.767	3.118	3.476	2.681	1.957	3.115	2.260	
3.068	3.321	1.628	2.677	3.740	3.506	2.592	2.120	3.337	2.528	
2.920	3.052	1.679	2.757	3.185	4.994	2.873	2.162	3.693	2.579	
2.669	2.737	1.514	2.751	3.153	3.948	2.535	1.855	2.728	2.259	
2.796	2.913	1.547	2.765	3.305	4.237	2.474	2.001	3.065	2.516	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.030	0.025	0.528	0.042	0.021	0.010	0.053	0.189	0.024	0.079	
LSPI converged after 5 iterations in 4.550 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 1.083
MEAN RF POLICY* SCORE IN MDP: 0.912

Loss matrix
2.462	2.352	1.145	1.722	3.173	3.686	2.325	1.321	3.721	2.668	
2.751	2.475	0.999	1.532	3.617	4.136	2.427	1.271	3.691	3.011	
1.572	1.652	0.838	1.533	2.437	2.810	2.759	1.538	3.517	2.014	
3.026	2.318	1.356	2.295	5.523	4.834	2.465	1.418	4.138	3.762	
2.010	2.248	1.414	1.838	4.018	3.624	2.469	1.545	3.964	3.021	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.003	0.002	0.901	0.013	0.000	0.000	0.000	0.079	0.000	0.000	
LSPI converged after 4 iterations in 3.800 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.450
MEAN RF POLICY* SCORE IN MDP: 0.870

Loss matrix
1.845	1.269	1.070	3.854	4.255	3.816	1.609	1.926	1.667	2.215	
1.724	1.640	0.856	3.462	4.298	3.634	1.447	1.878	1.756	2.070	
1.788	1.029	1.057	3.773	4.103	3.693	1.660	1.948	1.632	2.140	
1.579	1.416	0.896	3.328	3.747	3.647	1.598	2.075	1.898	2.011	
1.799	1.280	1.041	3.871	4.223	3.741	1.517	2.077	1.710	2.091	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.019	0.167	0.739	0.000	0.000	0.000	0.044	0.007	0.020	0.005	
LSPI converged after 4 iterations in 3.910 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.460
MEAN RF POLICY* SCORE IN MDP: 0.832

Loss matrix
2.073	2.237	1.543	2.473	3.062	3.122	2.190	2.000	2.143	2.647	
2.022	2.095	1.438	2.535	3.094	3.217	2.022	1.952	1.965	2.612	
2.114	2.249	1.524	2.520	3.139	3.249	2.230	2.043	2.139	2.744	
2.180	2.260	1.535	2.611	3.147	3.382	2.190	2.099	2.033	2.808	
2.293	2.536	1.685	2.528	3.152	3.388	2.586	2.270	2.508	2.919	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.094	0.081	0.333	0.066	0.046	0.042	0.083	0.104	0.093	0.058	
LSPI converged after 4 iterations in 3.850 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.569
MEAN RF POLICY* SCORE IN MDP: 0.935

LSPI converged after 5 iterations in 4.650 seconds.
~~~ MAGIC SCORE ~~~	0.887
Generating 4 expert demonstrations of size 50...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
2.634	2.258	1.421	2.764	3.057	3.399	2.398	1.953	2.925	2.197	
2.379	2.265	1.516	2.830	3.264	3.628	2.352	1.991	2.912	2.382	
2.602	2.217	1.532	2.679	3.439	3.378	2.347	2.007	2.964	2.429	
2.430	2.326	1.508	2.751	3.196	3.418	2.467	1.937	3.039	2.334	
2.304	2.159	1.550	2.693	3.342	3.286	2.487	1.949	2.958	2.383	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.050	0.075	0.520	0.033	0.018	0.015	0.054	0.150	0.025	0.061	
LSPI converged after 5 iterations in 4.590 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.987
MEAN RF POLICY* SCORE IN MDP: 0.896

Loss matrix
1.071	1.017	0.648	1.662	2.819	1.939	2.081	1.216	2.312	1.432	
1.271	0.980	0.663	1.646	2.701	2.169	2.108	1.262	2.320	1.523	
1.028	0.863	0.581	1.324	2.585	1.736	2.414	1.228	2.243	1.442	
1.005	0.874	0.666	1.996	2.649	2.446	1.819	1.160	2.261	1.480	
1.089	1.126	0.646	1.430	2.639	1.892	1.997	1.168	2.232	1.536	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.032	0.081	0.868	0.002	0.000	0.000	0.000	0.014	0.000	0.003	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.388
MEAN RF POLICY* SCORE IN MDP: 0.826

Loss matrix
2.738	2.429	2.426	3.383	3.635	4.746	3.150	2.439	3.827	2.765	
2.452	2.611	2.389	3.360	3.627	4.721	3.419	2.693	4.298	2.976	
2.900	2.724	2.887	3.477	3.568	4.553	3.436	2.737	4.762	3.041	
3.422	2.909	2.469	3.382	4.784	4.999	3.410	2.745	4.265	2.981	
3.173	2.491	2.331	3.165	4.274	4.174	3.190	2.477	3.979	2.873	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.097	0.181	0.466	0.010	0.002	0.000	0.011	0.185	0.001	0.047	
LSPI converged after 5 iterations in 4.510 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.521
MEAN RF POLICY* SCORE IN MDP: 0.949

Loss matrix
2.565	2.708	1.771	2.563	3.235	3.557	3.272	2.423	3.749	2.818	
2.586	2.810	1.746	2.513	3.156	3.461	3.132	2.311	3.600	2.698	
2.518	2.591	1.691	2.500	3.472	3.363	2.892	2.190	3.505	2.755	
2.646	2.608	1.745	2.436	3.333	3.411	3.071	2.242	3.605	2.736	
2.535	2.482	1.656	2.516	3.416	3.426	2.924	2.181	3.342	2.690	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.081	0.077	0.370	0.088	0.046	0.043	0.055	0.131	0.040	0.070	
LSPI converged after 5 iterations in 4.550 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.549
MEAN RF POLICY* SCORE IN MDP: 0.905

LSPI converged after 4 iterations in 3.800 seconds.
~~~ MAGIC SCORE ~~~	0.833
Generating 4 expert demonstrations of size 50...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
2.417	2.316	1.523	2.801	3.390	3.462	2.469	1.978	3.019	2.389	
2.205	2.136	1.511	2.657	3.197	3.240	2.508	1.930	3.029	2.305	
2.366	2.289	1.600	2.765	3.431	3.396	2.446	1.972	3.171	2.477	
2.491	2.277	1.574	2.742	3.225	3.318	2.469	2.013	3.101	2.415	
2.281	2.116	1.429	2.781	3.121	3.251	2.480	1.892	2.872	2.189	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.061	0.079	0.505	0.034	0.018	0.016	0.049	0.155	0.023	0.060	
LSPI converged after 5 iterations in 4.550 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.979
MEAN RF POLICY* SCORE IN MDP: 0.899

Loss matrix
1.025	1.068	0.602	1.404	2.578	1.753	2.053	1.173	2.358	1.481	
1.159	0.857	0.764	2.176	2.674	2.675	1.767	1.179	2.231	1.514	
1.155	1.002	0.715	2.097	2.743	2.278	1.893	1.231	2.213	1.564	
1.032	0.959	0.606	2.077	2.631	2.425	1.721	1.225	2.190	1.384	
1.106	0.855	0.665	1.471	2.760	1.929	2.246	1.192	2.114	1.561	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.034	0.103	0.842	0.001	0.000	0.000	0.000	0.016	0.000	0.003	
LSPI converged after 4 iterations in 3.870 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.378
MEAN RF POLICY* SCORE IN MDP: 0.834

Loss matrix
2.334	2.431	2.246	3.218	3.595	4.397	3.223	2.522	4.049	2.813	
2.603	2.685	2.390	3.306	3.807	4.705	3.406	2.585	4.272	3.013	
3.300	2.662	2.449	3.671	4.400	4.987	3.383	2.676	4.258	3.044	
2.514	3.130	3.037	3.383	3.513	4.681	3.614	2.773	5.656	2.966	
3.451	2.639	2.452	3.287	4.920	4.636	3.471	2.831	4.250	2.965	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.203	0.130	0.490	0.009	0.002	0.000	0.007	0.121	0.000	0.038	
LSPI converged after 5 iterations in 4.720 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.473
MEAN RF POLICY* SCORE IN MDP: 0.938

Loss matrix
2.539	2.569	1.693	2.441	3.213	3.398	3.120	2.259	3.576	2.636	
2.847	3.466	1.693	2.505	3.414	3.466	3.118	2.334	3.696	2.765	
2.585	2.640	1.718	2.507	3.300	3.444	3.029	2.222	3.641	2.737	
2.634	2.791	1.824	2.526	3.547	3.493	3.135	2.296	3.874	2.889	
2.581	2.546	1.650	2.441	3.282	3.454	2.980	2.213	3.523	2.648	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.078	0.069	0.374	0.094	0.045	0.042	0.055	0.133	0.038	0.071	
LSPI converged after 5 iterations in 4.770 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.556
MEAN RF POLICY* SCORE IN MDP: 0.909

LSPI converged after 5 iterations in 4.720 seconds.
~~~ MAGIC SCORE ~~~	0.836
Generating 4 expert demonstrations of size 100...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.299	3.430	1.854	2.401	3.268	3.056	2.668	1.994	3.210	2.301	
3.176	3.031	1.708	2.445	3.253	3.151	2.577	2.000	3.098	2.351	
3.716	3.354	1.670	2.382	3.322	3.095	2.459	1.999	2.883	2.327	
3.331	3.061	1.686	2.360	3.202	3.025	2.609	2.007	3.028	2.313	
3.221	3.080	1.679	2.447	3.262	3.172	2.622	2.084	3.147	2.422	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.020	0.024	0.453	0.080	0.022	0.027	0.060	0.195	0.028	0.091	
LSPI converged after 5 iterations in 4.650 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 1.087
MEAN RF POLICY* SCORE IN MDP: 0.904

Loss matrix
1.115	0.859	0.572	1.783	2.720	2.436	1.830	1.321	1.982	1.429	
1.111	0.783	0.587	1.758	2.564	2.213	1.835	1.142	2.057	1.420	
1.142	0.742	0.604	1.775	2.829	2.256	1.643	1.227	1.964	1.457	
1.103	0.856	0.540	1.685	2.705	2.202	1.893	1.245	1.962	1.414	
1.286	0.773	0.563	2.111	2.528	2.645	1.615	1.367	1.936	1.582	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.016	0.149	0.825	0.000	0.000	0.000	0.000	0.008	0.000	0.002	
LSPI converged after 5 iterations in 4.650 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.361
MEAN RF POLICY* SCORE IN MDP: 0.815

Loss matrix
3.283	2.294	2.218	2.801	4.614	3.441	2.915	2.579	3.874	2.719	
2.795	2.254	1.927	2.691	3.726	3.377	2.793	2.339	3.737	2.663	
2.305	2.256	2.131	2.796	3.313	3.503	3.013	2.508	3.879	2.731	
2.432	2.350	2.344	3.027	3.466	3.603	3.097	2.563	4.101	2.809	
2.299	2.333	2.311	3.062	3.376	3.510	3.095	2.717	4.095	2.744	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.100	0.206	0.553	0.019	0.002	0.002	0.012	0.076	0.001	0.030	
LSPI converged after 4 iterations in 3.910 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.470
MEAN RF POLICY* SCORE IN MDP: 0.950

Loss matrix
2.761	2.673	1.845	2.813	3.551	3.736	3.114	2.493	3.379	2.942	
2.849	2.709	1.848	2.774	3.615	3.760	3.119	2.526	3.538	3.019	
2.798	2.654	1.883	2.815	3.633	3.776	3.020	2.475	3.375	2.975	
2.718	2.549	1.733	2.709	3.491	3.642	2.925	2.350	3.243	2.831	
2.620	2.455	1.699	2.704	3.559	3.525	2.799	2.275	3.131	2.804	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.074	0.089	0.389	0.073	0.042	0.039	0.060	0.121	0.048	0.064	
LSPI converged after 5 iterations in 4.640 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.520
MEAN RF POLICY* SCORE IN MDP: 0.905

LSPI converged after 4 iterations in 4.040 seconds.
~~~ MAGIC SCORE ~~~	0.833
Generating 4 expert demonstrations of size 100...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.409	3.151	1.797	2.394	3.324	3.073	2.635	2.065	3.154	2.398	
3.266	2.993	1.683	2.410	3.202	3.199	2.621	2.039	3.217	2.396	
2.987	2.808	1.688	2.377	3.307	2.956	2.659	2.052	3.110	2.316	
3.264	3.166	1.721	2.435	3.236	3.103	2.558	2.027	3.084	2.341	
3.306	3.239	1.688	2.426	3.376	3.171	2.581	2.060	3.216	2.412	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.022	0.028	0.469	0.080	0.021	0.026	0.057	0.185	0.024	0.086	
LSPI converged after 5 iterations in 4.680 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 1.072
MEAN RF POLICY* SCORE IN MDP: 0.911

Loss matrix
1.094	0.744	0.557	2.049	2.694	2.448	1.508	1.034	1.913	1.451	
1.147	0.779	0.573	1.687	2.725	2.206	1.807	1.216	1.937	1.407	
1.314	0.869	0.589	2.127	2.764	2.675	1.670	1.397	1.855	1.643	
1.048	0.825	0.545	1.822	2.657	2.320	1.731	1.256	1.994	1.387	
1.152	0.762	0.550	1.744	2.629	2.254	1.646	1.206	1.891	1.383	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.015	0.145	0.826	0.000	0.000	0.000	0.001	0.010	0.000	0.002	
LSPI converged after 5 iterations in 4.480 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.363
MEAN RF POLICY* SCORE IN MDP: 0.808

Loss matrix
2.213	2.253	2.126	2.878	3.204	3.351	2.988	2.408	3.895	2.659	
2.403	2.294	2.034	2.747	3.436	3.490	2.910	2.483	3.756	2.702	
2.381	2.321	2.397	2.985	3.506	3.454	2.989	2.700	3.930	2.756	
2.318	2.351	2.080	2.965	3.411	3.571	3.068	2.479	4.412	2.779	
2.282	2.349	2.235	3.028	3.341	3.466	3.046	2.587	3.932	2.721	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.169	0.179	0.526	0.014	0.003	0.002	0.010	0.067	0.000	0.029	
LSPI converged after 6 iterations in 5.330 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.456
MEAN RF POLICY* SCORE IN MDP: 0.945

Loss matrix
2.737	2.655	1.839	2.815	3.516	3.757	3.101	2.490	3.415	2.956	
2.707	2.520	1.806	2.732	3.637	3.568	2.937	2.357	3.325	2.917	
2.708	2.677	1.797	2.735	3.501	3.677	3.115	2.443	3.440	2.916	
2.725	2.557	1.834	2.781	3.638	3.668	2.922	2.395	3.316	2.893	
2.596	2.487	1.748	2.703	3.443	3.538	2.889	2.289	3.225	2.790	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.078	0.091	0.379	0.073	0.042	0.040	0.060	0.124	0.048	0.065	
LSPI converged after 5 iterations in 4.750 seconds.
MEAN RF POLICY* LOSS IN MDP_m: 0.521
MEAN RF POLICY* SCORE IN MDP: 0.907

LSPI converged after 4 iterations in 3.820 seconds.
~~~ MAGIC SCORE ~~~	0.832
Process 4954 detected
