Generating LSTDQ demonstrations...
2161 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.167	0.266	0.912	-0.036	0.089	0.092	0.120	-0.225	-1.373	-0.168	0.108	-0.530	-0.001	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.058	0.337	1.011	0.007	0.045	0.153	-0.160	-0.307	-1.126	-0.099	0.096	-0.433	-0.026	
Sampling 10 reward functions...
	Sampled reward functions:
	 (0)	-0.322	-0.565	1.233	0.425	0.223	0.491	0.256	0.180	-0.306	-0.081	-0.235	-0.570	0.300	
	 (1)	-0.135	-0.289	1.009	0.262	-0.036	0.236	0.196	-0.060	-0.675	-0.271	-0.015	1.437	0.136	
	 (2)	0.423	0.048	0.968	-0.423	0.078	-0.084	-0.453	-0.396	-1.015	0.449	-0.003	2.529	-0.089	
	 (3)	-0.057	0.391	1.438	-0.048	0.107	-0.956	0.051	-0.055	-1.047	0.239	-0.167	-0.574	-0.073	
	 (4)	-0.131	-0.365	0.717	-0.066	0.187	0.163	0.201	0.060	-0.898	-0.245	-0.054	-0.026	0.313	
	 (5)	-0.074	0.217	0.970	-0.124	0.040	-0.497	0.081	-0.197	-1.145	-0.085	-0.081	-0.412	0.038	
	 (6)	-0.185	-0.279	1.196	0.536	0.126	0.389	0.114	-0.048	-0.561	0.041	-0.116	-0.324	0.122	
	 (7)	-0.104	-0.413	1.053	-0.033	0.169	0.721	0.171	0.450	-0.172	-0.179	-0.249	-0.432	0.276	
	 (8)	-0.181	0.162	0.967	0.482	-0.083	0.091	0.178	-0.550	-1.326	-0.433	0.241	0.135	-0.272	
	 (9)	0.007	-0.057	1.430	0.002	0.108	0.108	0.021	0.052	-0.827	0.101	-0.113	0.346	0.120	
LSPI converged after 5 iterations in 4.210 seconds.
Creating 10 corresponding optimal policies...
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.970 }	0.203	0.309	0.910	-0.061	0.084	0.216	0.329	-0.190	-1.524	-0.371	0.115	-0.807	-0.007	
	{ -0.148 }	 (random)
	{ -0.134 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.665 }	-0.017	-0.170	0.749	0.238	0.502	0.624	0.608	-0.072	-1.103	-0.641	-0.200	-1.315	0.426	
 (1)	{ 0.798 }	-0.014	0.001	0.840	0.119	-0.001	0.232	0.514	-0.235	-1.197	-0.640	0.091	0.396	0.227	
 (2)	{ 0.936 }	0.920	0.310	0.594	-0.740	0.213	0.176	-0.328	-0.669	-1.867	0.199	0.202	0.996	-0.195	
 (3)	{ 0.836 }	0.319	0.722	1.021	-0.352	0.356	-0.739	0.839	-0.275	-2.184	-0.724	-0.068	-2.056	-0.188	
 (4)	{ 0.905 }	0.138	-0.152	0.210	-0.193	0.392	0.360	0.635	-0.268	-1.701	-0.907	0.110	-1.173	0.460	
 (5)	{ 0.916 }	0.139	0.534	0.707	-0.138	0.162	-0.243	0.308	-0.487	-1.739	-0.538	0.016	-1.291	0.041	
 (6)	{ 0.576 }	0.224	0.023	0.789	0.272	0.324	0.528	0.537	-0.300	-1.409	-0.636	-0.010	-1.306	0.152	
 (7)	{ 0.841 }	0.119	-0.079	0.695	-0.110	0.405	0.781	0.696	0.210	-0.884	-0.767	-0.265	-1.227	0.422	
 (8)	{ 0.664 }	0.145	0.405	0.655	0.214	-0.045	-0.003	0.654	-0.850	-2.391	-1.139	0.591	-0.860	-0.525	
 (9)	{ 0.855 }	0.415	0.354	0.968	-0.218	0.322	0.290	0.602	-0.211	-1.911	-0.619	0.029	-1.174	0.144	
Scores for the experts true policies:
	{ 0.825 }	 (expert 0)
	{ 0.860 }	 (expert 1)
	{ 0.863 }	 (expert 2)
	{ 0.690 }	 (expert 3)
Summed loss of reward-optimal policies w.r.t. true rho values rho_m == MDP_m == V_m == (expert weights)
	2.341	1.500	2.239	3.610	2.183	2.040	2.749	2.320	2.736	2.790	
	1.748	0.859	1.907	2.916	1.488	1.951	1.929	2.064	2.105	2.062	
	1.732	1.275	2.273	3.050	1.596	1.619	2.140	2.178	2.290	2.121	
	1.553	1.040	2.005	2.916	1.488	1.726	1.962	1.910	2.105	2.030	
Generating 4 expert demonstrations of size 100...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
2.291	1.942	1.832	2.849	2.501	2.256	2.426	2.130	2.889	2.379	
2.168	1.862	1.750	2.782	2.373	2.070	2.309	1.996	2.862	2.236	
2.221	1.878	1.783	2.727	2.423	2.039	2.333	2.061	2.741	2.258	
2.091	1.875	1.730	2.720	2.337	1.987	2.229	1.968	2.825	2.217	
2.180	1.893	1.757	2.706	2.374	1.975	2.305	2.026	2.760	2.232	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.029	0.083	0.734	0.004	0.016	0.042	0.019	0.048	0.002	0.022	
LSPI converged after 5 iterations in 4.600 seconds.
MEAN LOSS: 2.204
MEAN RF POLICY* LOSS IN MDP_m: 1.828
MEAN RF POLICY* SCORE IN MDP: 0.928

Loss matrix
1.461	1.758	1.774	2.314	1.719	1.504	1.770	1.345	2.528	1.712	
1.676	1.835	1.721	2.311	1.742	1.515	1.904	1.510	2.498	1.755	
1.485	1.740	1.780	2.338	1.728	1.505	1.788	1.407	2.486	1.750	
1.583	1.660	1.754	2.513	1.712	1.502	1.868	1.491	2.413	1.697	
1.596	1.745	1.719	2.340	1.700	1.500	1.858	1.495	2.470	1.722	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.001	0.000	0.000	0.000	0.000	0.001	0.000	0.998	0.000	0.000	
LSPI converged after 6 iterations in 5.300 seconds.
MEAN LOSS: 2.063
MEAN RF POLICY* LOSS IN MDP_m: 2.061
MEAN RF POLICY* SCORE IN MDP: 0.838

Loss matrix
1.345	1.369	1.142	1.582	1.444	0.788	1.284	1.547	1.379	1.209	
1.345	1.466	1.163	1.642	1.571	0.935	1.296	1.650	1.448	1.371	
1.321	1.313	1.217	1.648	1.429	0.838	1.249	1.601	1.311	1.248	
1.374	1.439	1.167	1.673	1.468	0.887	1.320	1.617	1.406	1.303	
1.221	1.314	1.127	1.575	1.381	0.789	1.176	1.488	1.342	1.165	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.007	0.005	0.035	0.001	0.003	0.920	0.011	0.001	0.005	0.012	
LSPI converged after 5 iterations in 4.590 seconds.
MEAN LOSS: 1.658
MEAN RF POLICY* LOSS IN MDP_m: 1.454
MEAN RF POLICY* SCORE IN MDP: 0.918

Loss matrix
1.947	1.946	2.144	3.071	2.524	2.261	2.082	2.035	2.802	2.423	
1.961	1.904	2.208	3.128	2.519	2.253	2.105	2.045	2.802	2.452	
1.969	1.916	2.149	3.119	2.559	2.289	2.099	2.068	2.812	2.458	
1.956	1.928	2.113	3.094	2.548	2.286	2.079	2.059	2.790	2.426	
2.103	2.000	2.196	3.183	2.647	2.380	2.205	2.195	2.865	2.546	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.102	0.546	0.063	0.005	0.030	0.052	0.070	0.079	0.017	0.036	
LSPI converged after 5 iterations in 4.570 seconds.
MEAN LOSS: 1.399
MEAN RF POLICY* LOSS IN MDP_m: 0.772
MEAN RF POLICY* SCORE IN MDP: 0.917

LSPI converged after 5 iterations in 4.550 seconds.
~~~ MAGIC SCORE ~~~	0.936
Generating 4 expert demonstrations of size 100...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.701	1.746	1.703	2.282	2.204	1.582	1.810	1.665	2.371	1.947	
1.663	1.773	1.964	2.354	2.201	1.674	1.790	1.622	2.449	1.927	
1.692	1.795	1.846	2.353	2.211	1.652	1.826	1.657	2.418	1.955	
1.742	1.778	1.914	2.382	2.253	1.669	1.844	1.699	2.416	1.996	
1.648	1.766	1.890	2.366	2.174	1.651	1.772	1.613	2.419	1.940	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.068	0.047	0.037	0.005	0.013	0.673	0.043	0.085	0.002	0.028	
LSPI converged after 5 iterations in 4.640 seconds.
MEAN LOSS: 2.128
MEAN RF POLICY* LOSS IN MDP_m: 1.768
MEAN RF POLICY* SCORE IN MDP: 0.929

Loss matrix
1.343	1.276	1.130	1.240	1.300	0.662	1.564	1.278	1.508	1.182	
1.361	1.291	1.137	1.335	1.257	0.613	1.628	1.255	1.400	1.224	
1.419	1.277	1.069	1.221	1.256	0.607	1.682	1.291	1.274	1.258	
1.342	1.282	1.151	1.302	1.288	0.649	1.584	1.242	1.412	1.218	
1.431	1.290	1.130	1.292	1.265	0.604	1.669	1.294	1.421	1.218	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.001	0.018	0.001	0.001	0.976	0.000	0.001	0.000	0.002	
LSPI converged after 5 iterations in 4.520 seconds.
MEAN LOSS: 1.950
MEAN RF POLICY* LOSS IN MDP_m: 1.897
MEAN RF POLICY* SCORE IN MDP: 0.916

Loss matrix
1.826	2.615	1.991	2.295	2.727	2.016	2.150	1.670	3.173	2.412	
1.792	2.454	1.907	2.212	2.629	1.869	2.110	1.619	3.024	2.320	
1.769	2.492	1.865	2.195	2.606	1.837	2.121	1.659	2.990	2.331	
1.882	2.313	1.735	2.187	2.447	1.688	2.114	1.760	2.573	2.239	
1.692	2.429	1.760	2.190	2.509	1.810	2.005	1.510	3.028	2.237	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.006	0.000	0.005	0.001	0.000	0.005	0.001	0.980	0.000	0.001	
LSPI converged after 5 iterations in 4.620 seconds.
MEAN LOSS: 2.173
MEAN RF POLICY* LOSS IN MDP_m: 2.154
MEAN RF POLICY* SCORE IN MDP: 0.843

Loss matrix
1.696	1.783	1.652	2.565	2.190	1.820	1.939	1.534	2.799	1.928	
1.790	1.759	1.611	2.663	2.180	1.872	1.987	1.616	2.713	1.974	
1.795	1.742	1.575	2.668	2.199	1.912	1.979	1.641	2.625	1.983	
1.696	1.784	1.748	2.537	2.118	1.801	1.879	1.527	2.717	1.902	
1.777	1.883	1.673	2.636	2.260	1.885	1.998	1.607	2.822	2.018	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.084	0.076	0.123	0.012	0.036	0.064	0.052	0.495	0.005	0.052	
LSPI converged after 5 iterations in 4.630 seconds.
MEAN LOSS: 1.821
MEAN RF POLICY* LOSS IN MDP_m: 1.244
MEAN RF POLICY* SCORE IN MDP: 0.834

LSPI converged after 5 iterations in 4.620 seconds.
~~~ MAGIC SCORE ~~~	0.859
Generating 4 expert demonstrations of size 500...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.775	1.712	1.797	2.296	2.110	1.612	1.942	1.724	2.359	1.921	
1.701	1.653	1.790	2.271	2.079	1.589	1.875	1.670	2.339	1.873	
1.695	1.668	1.787	2.267	2.061	1.554	1.873	1.657	2.331	1.860	
1.674	1.689	1.766	2.230	2.069	1.545	1.843	1.646	2.313	1.845	
1.761	1.690	1.836	2.322	2.100	1.619	1.919	1.728	2.316	1.925	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.051	0.060	0.039	0.005	0.015	0.713	0.028	0.060	0.001	0.028	
LSPI converged after 5 iterations in 4.660 seconds.
MEAN LOSS: 2.099
MEAN RF POLICY* LOSS IN MDP_m: 1.756
MEAN RF POLICY* SCORE IN MDP: 0.928

Loss matrix
1.327	1.584	1.873	1.975	1.746	1.137	1.589	1.224	2.287	1.678	
1.319	1.585	1.857	1.959	1.739	1.124	1.589	1.245	2.300	1.679	
1.367	1.598	1.906	2.073	1.790	1.200	1.630	1.281	2.348	1.746	
1.271	1.596	1.798	1.989	1.764	1.156	1.538	1.216	2.312	1.678	
1.297	1.589	1.849	1.960	1.757	1.135	1.554	1.211	2.284	1.659	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.002	0.000	0.000	0.000	0.000	0.993	0.000	0.004	0.000	0.000	
LSPI converged after 5 iterations in 4.520 seconds.
MEAN LOSS: 1.951
MEAN RF POLICY* LOSS IN MDP_m: 1.939
MEAN RF POLICY* SCORE IN MDP: 0.908

Loss matrix
1.381	1.552	1.668	2.152	1.812	1.340	1.621	1.380	2.362	1.647	
1.709	1.531	1.649	2.414	1.789	1.743	2.060	1.394	2.344	2.088	
1.349	1.527	1.679	2.126	1.800	1.327	1.642	1.346	2.377	1.601	
1.385	1.558	1.715	2.166	1.826	1.356	1.620	1.382	2.366	1.659	
1.368	1.518	1.685	2.179	1.802	1.332	1.613	1.363	2.362	1.625	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.014	0.009	0.004	0.000	0.002	0.940	0.004	0.022	0.000	0.004	
LSPI converged after 5 iterations in 4.630 seconds.
MEAN LOSS: 1.637
MEAN RF POLICY* LOSS IN MDP_m: 1.506
MEAN RF POLICY* SCORE IN MDP: 0.907

Loss matrix
1.915	1.831	1.883	2.822	2.311	1.964	2.107	1.851	2.575	2.254	
1.904	1.801	1.875	2.804	2.286	1.938	2.093	1.832	2.554	2.230	
1.891	1.854	1.851	2.772	2.289	1.934	2.082	1.827	2.570	2.220	
1.916	1.880	1.894	2.827	2.345	1.983	2.110	1.853	2.608	2.275	
1.931	1.857	1.894	2.839	2.337	1.983	2.133	1.865	2.625	2.279	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.077	0.525	0.084	0.003	0.031	0.069	0.053	0.104	0.016	0.038	
LSPI converged after 5 iterations in 4.520 seconds.
MEAN LOSS: 1.421
MEAN RF POLICY* LOSS IN MDP_m: 0.714
MEAN RF POLICY* SCORE IN MDP: 0.915

LSPI converged after 5 iterations in 4.650 seconds.
~~~ MAGIC SCORE ~~~	0.930
Generating 4 expert demonstrations of size 500...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.775	1.704	1.862	2.251	2.169	1.596	1.870	1.719	2.397	1.889	
1.757	1.730	1.823	2.237	2.176	1.583	1.853	1.707	2.431	1.896	
1.744	1.713	1.821	2.218	2.165	1.575	1.833	1.708	2.375	1.883	
1.749	1.673	1.879	2.266	2.100	1.571	1.856	1.705	2.361	1.870	
1.792	1.712	1.803	2.242	2.169	1.595	1.885	1.735	2.397	1.888	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.047	0.066	0.035	0.008	0.015	0.709	0.032	0.056	0.002	0.030	
LSPI converged after 5 iterations in 4.650 seconds.
MEAN LOSS: 2.103
MEAN RF POLICY* LOSS IN MDP_m: 1.753
MEAN RF POLICY* SCORE IN MDP: 0.931

Loss matrix
1.484	1.575	1.810	2.239	1.789	1.280	1.752	1.405	2.326	1.795	
1.382	1.554	1.924	2.166	1.759	1.235	1.653	1.301	2.291	1.748	
1.447	1.548	1.903	2.158	1.769	1.223	1.744	1.377	2.286	1.743	
1.492	1.581	1.778	2.261	1.805	1.291	1.771	1.396	2.352	1.812	
1.462	1.626	1.865	2.308	1.834	1.325	1.743	1.371	2.381	1.861	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.001	0.000	0.000	0.000	0.000	0.996	0.000	0.002	0.000	0.000	
LSPI converged after 5 iterations in 4.680 seconds.
MEAN LOSS: 1.951
MEAN RF POLICY* LOSS IN MDP_m: 1.944
MEAN RF POLICY* SCORE IN MDP: 0.910

Loss matrix
1.742	1.914	1.659	2.410	2.266	1.695	1.962	1.688	2.680	2.028	
1.697	1.862	1.583	2.325	2.183	1.631	1.903	1.678	2.560	1.979	
1.721	1.931	1.611	2.390	2.272	1.713	1.946	1.664	2.676	2.015	
1.709	1.825	1.612	2.356	2.176	1.646	1.918	1.660	2.602	1.970	
1.728	1.889	1.649	2.380	2.216	1.676	1.941	1.670	2.628	1.993	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.005	0.003	0.974	0.000	0.001	0.007	0.002	0.007	0.000	0.001	
LSPI converged after 5 iterations in 4.550 seconds.
MEAN LOSS: 2.262
MEAN RF POLICY* LOSS IN MDP_m: 2.220
MEAN RF POLICY* SCORE IN MDP: 0.939

Loss matrix
1.944	1.763	1.887	2.750	2.258	1.886	2.158	1.892	2.533	2.258	
2.009	1.829	1.902	2.830	2.335	1.940	2.228	1.940	2.642	2.330	
1.974	1.822	1.889	2.778	2.297	1.916	2.191	1.917	2.570	2.299	
1.974	1.776	1.908	2.794	2.291	1.910	2.204	1.913	2.579	2.307	
1.930	1.791	1.887	2.743	2.249	1.865	2.153	1.887	2.536	2.264	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.069	0.564	0.081	0.004	0.032	0.082	0.045	0.076	0.016	0.031	
LSPI converged after 4 iterations in 3.880 seconds.
MEAN LOSS: 1.387
MEAN RF POLICY* LOSS IN MDP_m: 0.676
MEAN RF POLICY* SCORE IN MDP: 0.917

LSPI converged after 5 iterations in 4.550 seconds.
~~~ MAGIC SCORE ~~~	0.936
Generating 4 expert demonstrations of size 1000...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.637	1.595	1.667	2.325	2.026	1.584	1.823	1.575	2.362	1.849	
1.658	1.609	1.691	2.338	2.033	1.589	1.841	1.593	2.375	1.862	
1.634	1.560	1.669	2.313	2.013	1.574	1.814	1.574	2.359	1.831	
1.677	1.623	1.641	2.315	2.061	1.604	1.839	1.618	2.366	1.863	
1.646	1.596	1.634	2.315	2.013	1.569	1.819	1.591	2.352	1.839	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.056	0.649	0.054	0.005	0.020	0.078	0.035	0.071	0.001	0.031	
LSPI converged after 4 iterations in 3.750 seconds.
MEAN LOSS: 1.795
MEAN RF POLICY* LOSS IN MDP_m: 1.421
MEAN RF POLICY* SCORE IN MDP: 0.919

Loss matrix
1.053	1.373	1.836	1.645	1.469	1.131	1.338	1.040	1.962	1.339	
1.073	1.367	1.888	1.638	1.473	1.044	1.356	1.077	1.945	1.351	
1.099	1.343	1.826	1.686	1.458	1.135	1.378	1.099	1.958	1.353	
1.064	1.358	1.869	1.629	1.459	1.111	1.343	1.064	1.951	1.322	
1.036	1.333	1.858	1.676	1.453	1.112	1.317	1.042	1.969	1.339	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.983	0.001	0.000	0.000	0.000	0.006	0.001	0.008	0.000	0.001	
LSPI converged after 5 iterations in 4.510 seconds.
MEAN LOSS: 1.751
MEAN RF POLICY* LOSS IN MDP_m: 1.740
MEAN RF POLICY* SCORE IN MDP: 0.649

Loss matrix
1.444	1.569	1.599	2.312	1.949	1.436	1.715	1.446	2.354	1.795	
1.414	1.543	1.570	2.283	1.902	1.390	1.683	1.417	2.322	1.752	
1.399	1.551	1.552	2.236	1.917	1.384	1.670	1.392	2.339	1.745	
1.418	1.572	1.592	2.272	1.949	1.411	1.691	1.411	2.373	1.774	
1.415	1.601	1.626	2.240	1.953	1.398	1.683	1.414	2.359	1.777	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.014	0.007	0.006	0.000	0.001	0.949	0.004	0.015	0.000	0.003	
LSPI converged after 5 iterations in 4.520 seconds.
MEAN LOSS: 1.634
MEAN RF POLICY* LOSS IN MDP_m: 1.520
MEAN RF POLICY* SCORE IN MDP: 0.913

Loss matrix
1.798	1.788	1.712	2.550	2.295	1.725	1.990	1.802	2.510	2.173	
1.817	1.802	1.742	2.524	2.305	1.726	1.996	1.813	2.523	2.195	
1.817	1.818	1.733	2.526	2.331	1.729	2.005	1.813	2.518	2.185	
1.787	1.783	1.737	2.504	2.309	1.709	1.983	1.781	2.508	2.164	
1.785	1.778	1.693	2.509	2.273	1.696	1.970	1.777	2.483	2.143	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.076	0.077	0.544	0.002	0.019	0.113	0.053	0.079	0.004	0.032	
LSPI converged after 5 iterations in 4.600 seconds.
MEAN LOSS: 1.848
MEAN RF POLICY* LOSS IN MDP_m: 1.051
MEAN RF POLICY* SCORE IN MDP: 0.924

LSPI converged after 5 iterations in 4.570 seconds.
~~~ MAGIC SCORE ~~~	0.914
Generating 4 expert demonstrations of size 1000...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.709	1.704	1.681	2.280	2.073	1.585	1.852	1.650	2.406	1.855	
1.683	1.648	1.692	2.252	2.040	1.564	1.830	1.613	2.375	1.821	
1.690	1.665	1.685	2.276	2.064	1.561	1.836	1.622	2.411	1.830	
1.733	1.708	1.727	2.326	2.119	1.635	1.887	1.661	2.467	1.895	
1.705	1.678	1.692	2.293	2.064	1.591	1.853	1.640	2.401	1.855	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.050	0.056	0.052	0.008	0.020	0.672	0.034	0.071	0.003	0.035	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN LOSS: 2.123
MEAN RF POLICY* LOSS IN MDP_m: 1.719
MEAN RF POLICY* SCORE IN MDP: 0.928

Loss matrix
1.186	1.388	1.932	1.618	1.540	1.008	1.434	1.097	2.011	1.414	
1.191	1.379	1.947	1.631	1.533	1.000	1.446	1.086	2.021	1.418	
1.193	1.421	1.908	1.656	1.546	0.998	1.437	1.084	2.022	1.427	
1.272	1.402	1.903	1.699	1.552	1.023	1.512	1.160	2.034	1.435	
1.235	1.406	1.921	1.644	1.544	1.039	1.471	1.131	2.019	1.419	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.003	0.001	0.000	0.000	0.000	0.987	0.000	0.008	0.000	0.001	
LSPI converged after 5 iterations in 4.500 seconds.
MEAN LOSS: 1.950
MEAN RF POLICY* LOSS IN MDP_m: 1.928
MEAN RF POLICY* SCORE IN MDP: 0.917

Loss matrix
1.412	1.633	1.559	2.118	1.923	1.354	1.642	1.400	2.268	1.667	
1.435	1.611	1.562	2.156	1.928	1.388	1.670	1.416	2.306	1.696	
1.421	1.592	1.575	2.150	1.890	1.357	1.652	1.405	2.277	1.679	
1.412	1.598	1.562	2.165	1.924	1.389	1.660	1.395	2.313	1.693	
1.405	1.614	1.554	2.125	1.924	1.355	1.637	1.401	2.271	1.673	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.014	0.005	0.007	0.000	0.002	0.949	0.004	0.015	0.000	0.003	
LSPI converged after 5 iterations in 4.480 seconds.
MEAN LOSS: 1.637
MEAN RF POLICY* LOSS IN MDP_m: 1.522
MEAN RF POLICY* SCORE IN MDP: 0.920

Loss matrix
1.841	1.768	1.805	2.594	2.247	1.735	2.008	1.804	2.506	2.116	
1.812	1.759	1.843	2.561	2.222	1.708	1.981	1.780	2.492	2.090	
1.821	1.793	1.831	2.578	2.258	1.726	2.001	1.780	2.545	2.132	
1.815	1.758	1.851	2.536	2.217	1.701	1.990	1.773	2.479	2.087	
1.836	1.769	1.817	2.564	2.246	1.737	2.004	1.794	2.501	2.107	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.073	0.090	0.072	0.002	0.026	0.558	0.051	0.082	0.008	0.038	
LSPI converged after 5 iterations in 4.610 seconds.
MEAN LOSS: 1.710
MEAN RF POLICY* LOSS IN MDP_m: 1.039
MEAN RF POLICY* SCORE IN MDP: 0.941

LSPI converged after 5 iterations in 4.700 seconds.
~~~ MAGIC SCORE ~~~	0.919
Generating 4 expert demonstrations of size 5000...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.334	1.325	1.407	1.876	1.684	1.159	1.519	1.315	2.018	1.458	
1.341	1.337	1.395	1.897	1.701	1.170	1.525	1.318	2.037	1.466	
1.351	1.327	1.430	1.903	1.687	1.179	1.535	1.326	2.015	1.477	
1.365	1.338	1.430	1.938	1.707	1.202	1.547	1.342	2.045	1.499	
1.359	1.356	1.437	1.905	1.725	1.196	1.541	1.338	2.047	1.488	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.063	0.066	0.051	0.009	0.022	0.630	0.035	0.079	0.002	0.042	
LSPI converged after 5 iterations in 4.620 seconds.
MEAN LOSS: 2.131
MEAN RF POLICY* LOSS IN MDP_m: 1.690
MEAN RF POLICY* SCORE IN MDP: 0.929

Loss matrix
1.040	1.184	1.760	1.473	1.261	1.180	1.315	0.924	1.779	1.189	
1.033	1.169	1.771	1.455	1.255	1.170	1.310	0.901	1.784	1.188	
1.039	1.169	1.781	1.416	1.251	1.141	1.317	0.897	1.779	1.190	
1.034	1.171	1.785	1.447	1.260	1.165	1.314	0.895	1.784	1.186	
1.047	1.162	1.774	1.463	1.240	1.170	1.321	0.901	1.756	1.166	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.010	0.002	0.000	0.000	0.001	0.003	0.001	0.980	0.000	0.002	
LSPI converged after 5 iterations in 4.610 seconds.
MEAN LOSS: 2.057
MEAN RF POLICY* LOSS IN MDP_m: 2.043
MEAN RF POLICY* SCORE IN MDP: 0.845

Loss matrix
1.188	1.314	1.438	1.869	1.629	1.047	1.439	1.198	1.996	1.421	
1.178	1.308	1.408	1.870	1.614	1.047	1.430	1.191	1.980	1.418	
1.162	1.307	1.379	1.849	1.612	1.030	1.418	1.184	1.971	1.406	
1.172	1.303	1.365	1.846	1.604	1.022	1.425	1.186	1.967	1.402	
1.181	1.303	1.411	1.853	1.612	1.030	1.431	1.193	1.968	1.408	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.032	0.014	0.008	0.001	0.003	0.905	0.006	0.024	0.000	0.007	
LSPI converged after 5 iterations in 4.460 seconds.
MEAN LOSS: 1.645
MEAN RF POLICY* LOSS IN MDP_m: 1.436
MEAN RF POLICY* SCORE IN MDP: 0.920

Loss matrix
1.563	1.524	1.529	2.225	1.915	1.412	1.733	1.561	2.193	1.768	
1.561	1.543	1.522	2.218	1.928	1.415	1.733	1.562	2.204	1.773	
1.556	1.526	1.506	2.232	1.913	1.405	1.725	1.556	2.205	1.778	
1.553	1.536	1.499	2.187	1.924	1.390	1.720	1.551	2.184	1.770	
1.537	1.531	1.503	2.195	1.920	1.390	1.714	1.538	2.214	1.767	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.073	0.080	0.097	0.002	0.027	0.551	0.050	0.073	0.006	0.041	
LSPI converged after 5 iterations in 4.460 seconds.
MEAN LOSS: 1.721
MEAN RF POLICY* LOSS IN MDP_m: 1.005
MEAN RF POLICY* SCORE IN MDP: 0.936

LSPI converged after 5 iterations in 4.570 seconds.
~~~ MAGIC SCORE ~~~	0.934
Generating 4 expert demonstrations of size 5000...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.361	1.328	1.392	1.872	1.691	1.171	1.544	1.316	1.993	1.453	
1.352	1.311	1.401	1.869	1.675	1.166	1.530	1.310	1.972	1.442	
1.363	1.319	1.415	1.884	1.681	1.175	1.545	1.326	1.987	1.465	
1.345	1.321	1.386	1.885	1.673	1.175	1.526	1.317	1.976	1.458	
1.358	1.323	1.398	1.879	1.687	1.172	1.545	1.315	1.994	1.459	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.059	0.068	0.051	0.008	0.021	0.632	0.033	0.082	0.001	0.043	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN LOSS: 2.127
MEAN RF POLICY* LOSS IN MDP_m: 1.685
MEAN RF POLICY* SCORE IN MDP: 0.927

Loss matrix
1.049	1.147	1.847	1.451	1.241	1.163	1.326	0.931	1.754	1.174	
1.059	1.139	1.871	1.440	1.233	1.151	1.335	0.926	1.733	1.179	
1.054	1.147	1.856	1.473	1.239	1.173	1.330	0.941	1.744	1.169	
1.053	1.139	1.868	1.423	1.242	1.138	1.332	0.927	1.756	1.180	
1.055	1.141	1.861	1.470	1.228	1.173	1.331	0.934	1.737	1.165	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.009	0.003	0.000	0.000	0.001	0.003	0.001	0.981	0.000	0.002	
LSPI converged after 5 iterations in 4.520 seconds.
MEAN LOSS: 2.057
MEAN RF POLICY* LOSS IN MDP_m: 2.043
MEAN RF POLICY* SCORE IN MDP: 0.838

Loss matrix
1.231	1.320	1.404	1.920	1.608	1.079	1.467	1.219	2.015	1.430	
1.234	1.331	1.392	1.918	1.623	1.081	1.469	1.218	2.031	1.432	
1.231	1.334	1.417	1.914	1.625	1.084	1.471	1.215	2.033	1.435	
1.229	1.308	1.413	1.910	1.596	1.063	1.463	1.215	2.008	1.420	
1.222	1.307	1.374	1.898	1.590	1.054	1.454	1.208	1.994	1.412	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.020	0.013	0.008	0.001	0.004	0.913	0.006	0.028	0.000	0.007	
LSPI converged after 4 iterations in 3.980 seconds.
MEAN LOSS: 1.646
MEAN RF POLICY* LOSS IN MDP_m: 1.453
MEAN RF POLICY* SCORE IN MDP: 0.923

Loss matrix
1.529	1.518	1.517	2.176	1.953	1.403	1.704	1.542	2.171	1.759	
1.536	1.534	1.532	2.167	1.964	1.405	1.711	1.546	2.195	1.768	
1.536	1.521	1.512	2.165	1.961	1.400	1.710	1.549	2.185	1.764	
1.527	1.522	1.519	2.138	1.954	1.381	1.701	1.538	2.176	1.757	
1.535	1.528	1.510	2.152	1.956	1.381	1.709	1.540	2.186	1.764	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.075	0.078	0.090	0.005	0.023	0.563	0.051	0.073	0.001	0.040	
LSPI converged after 5 iterations in 4.560 seconds.
MEAN LOSS: 1.724
MEAN RF POLICY* LOSS IN MDP_m: 1.035
MEAN RF POLICY* SCORE IN MDP: 0.936

LSPI converged after 5 iterations in 4.480 seconds.
~~~ MAGIC SCORE ~~~	0.935
Process 6117 detected
