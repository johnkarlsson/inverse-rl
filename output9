Generating LSTDQ demonstrations...
2138 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.164	0.270	0.956	-0.009	0.079	0.126	0.132	-0.239	-1.381	-0.177	0.109	-0.475	-0.006	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.050	0.340	1.050	0.019	0.037	0.163	-0.158	-0.309	-1.126	-0.101	0.096	-0.428	-0.024	
Sampling 10 reward functions...
	Sampled reward functions:
	 (0)	0.384	0.200	0.658	-0.328	0.026	-0.073	-0.451	-0.497	-1.534	0.626	0.114	1.530	-0.270	
	 (1)	-0.038	-0.395	1.600	0.188	-0.113	1.345	0.044	-0.290	-0.798	0.391	0.070	4.379	0.031	
	 (2)	-0.219	-0.418	1.416	0.199	0.005	1.155	0.224	-0.100	-0.847	0.503	-0.023	-0.496	0.239	
	 (3)	-0.099	0.140	1.496	0.030	-0.087	-0.631	0.129	0.184	-0.814	-0.217	-0.062	0.260	0.046	
	 (4)	0.137	0.157	1.455	-0.149	-0.165	-0.208	-0.154	-0.499	-0.795	0.111	0.121	1.617	-0.070	
	 (5)	-0.126	0.042	0.740	-0.031	0.051	-0.748	0.161	0.213	-1.019	-0.236	-0.042	1.665	0.132	
	 (6)	0.011	-0.017	1.265	-0.043	0.099	-0.268	0.038	-0.078	-0.814	0.080	-0.139	2.481	0.030	
	 (7)	0.062	-0.353	1.037	-0.146	0.161	0.362	-0.107	0.256	-0.635	0.259	-0.053	-1.056	0.258	
	 (8)	-0.059	-0.159	1.097	-0.015	0.052	0.833	0.098	0.027	-0.821	-0.071	-0.014	-0.192	0.047	
	 (9)	0.002	0.266	1.543	0.167	0.024	-0.211	-0.053	-0.779	-1.426	0.211	0.161	-0.345	-0.074	
LSPI converged after 5 iterations in 4.240 seconds.
Creating 10 corresponding optimal policies...
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.969 }	0.193	0.310	0.951	-0.013	0.076	0.263	0.340	-0.209	-1.528	-0.372	0.114	-0.737	-0.010	
	{ -0.131 }	 (random)
	{ -0.118 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.939 }	0.757	0.363	0.389	-0.579	0.098	-0.051	-0.207	-0.778	-2.402	0.308	0.375	0.237	-0.514	
 (1)	{ 0.667 }	0.158	-0.051	1.432	0.011	-0.147	1.100	0.839	-0.316	-1.769	-0.154	0.269	2.567	0.069	
 (2)	{ 0.554 }	-0.009	-0.044	1.150	0.085	0.054	1.062	0.987	-0.209	-1.723	-0.301	0.107	-1.865	0.399	
 (3)	{ 0.884 }	0.074	0.567	1.358	-0.069	-0.060	-0.388	0.671	-0.040	-1.601	-0.736	0.060	-0.718	0.065	
 (4)	{ 0.949 }	0.345	0.636	1.300	-0.175	-0.204	-0.063	0.017	-0.672	-1.534	-0.126	0.349	0.862	-0.136	
 (5)	{ 0.794 }	0.017	0.170	0.485	-0.167	0.147	-0.480	0.677	0.061	-1.679	-0.647	0.070	0.340	0.188	
 (6)	{ 0.833 }	0.285	0.253	0.921	-0.213	0.292	-0.011	0.547	-0.242	-1.680	-0.439	-0.051	0.956	-0.010	
 (7)	{ 0.922 }	0.442	-0.008	0.553	-0.276	0.349	0.570	0.301	-0.034	-1.611	-0.244	0.124	-1.993	0.344	
 (8)	{ 0.898 }	0.193	0.219	0.793	-0.020	0.173	0.839	0.643	-0.267	-1.738	-0.743	0.139	-1.276	0.030	
 (9)	{ 0.912 }	0.582	0.785	1.014	0.000	0.141	0.076	0.374	-1.142	-2.812	-0.628	0.541	-1.675	-0.225	
Scores for the experts true policies:
	{ 0.826 }	 (expert 0)
	{ 0.844 }	 (expert 1)
	{ 0.866 }	 (expert 2)
	{ 0.699 }	 (expert 3)
Summed loss of reward-optimal policies w.r.t. true rho values rho_m == MDP_m == V_m == (expert weights)
	2.657	3.681	3.312	2.111	1.507	2.026	2.428	2.292	2.374	3.076	
	1.933	3.803	2.744	1.491	1.633	1.599	2.068	1.668	1.735	2.555	
	2.126	4.094	2.777	1.694	1.948	2.000	2.413	1.879	1.735	2.668	
	1.933	3.834	2.560	1.476	1.621	1.703	2.172	1.588	1.650	2.402	
Generating 4 expert demonstrations of size 10...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.536	4.499	1.313	2.023	2.957	3.474	2.931	0.762	1.155	2.100	
2.044	3.769	2.207	1.869	2.051	2.195	2.477	0.658	1.198	2.699	
1.419	5.427	1.485	2.289	3.695	3.056	2.865	0.773	1.193	2.285	
1.703	4.263	1.760	1.937	3.181	2.222	2.389	0.971	1.281	2.800	
1.441	4.926	1.833	1.892	2.981	2.722	2.941	0.797	1.258	2.701	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.073	0.002	0.061	0.037	0.012	0.015	0.014	0.612	0.155	0.019	
LSPI converged after 5 iterations in 4.580 seconds.
MEAN LOSS: 2.394
MEAN RF POLICY* LOSS IN MDP_m: 1.905
MEAN RF POLICY* SCORE IN MDP: 0.922

Loss matrix
1.485	3.041	2.687	2.572	2.653	1.140	2.332	1.750	1.957	3.057	
1.451	3.152	2.752	2.427	2.516	1.091	2.325	1.860	1.862	2.814	
1.366	2.917	2.628	2.276	2.318	1.071	2.132	1.776	1.747	2.637	
1.376	3.181	2.977	2.403	2.313	0.999	2.210	1.834	1.945	2.321	
1.666	3.357	2.971	3.045	2.648	1.292	3.126	2.188	2.349	3.505	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.002	0.000	0.000	0.000	0.000	0.997	0.000	0.000	0.000	0.000	
LSPI converged after 5 iterations in 4.660 seconds.
MEAN LOSS: 1.600
MEAN RF POLICY* LOSS IN MDP_m: 1.596
MEAN RF POLICY* SCORE IN MDP: 0.798

Loss matrix
4.102	3.672	3.596	6.561	5.238	6.188	5.125	3.583	3.847	6.180	
2.430	3.916	2.998	4.533	3.293	4.095	3.537	2.283	3.047	3.987	
2.175	5.431	3.424	4.986	4.444	5.681	4.156	2.511	2.886	3.943	
2.108	3.549	3.341	3.860	2.885	3.629	3.023	2.509	2.516	3.436	
2.314	2.520	3.377	4.259	2.906	3.572	3.079	3.032	3.044	4.576	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.998	0.000	0.000	0.000	0.000	0.000	0.000	0.001	0.000	0.000	
LSPI converged after 5 iterations in 4.650 seconds.
MEAN LOSS: 2.127
MEAN RF POLICY* LOSS IN MDP_m: 2.123
MEAN RF POLICY* SCORE IN MDP: 0.943

Loss matrix
5.402	12.040	3.680	2.246	4.259	3.649	6.822	4.500	2.485	3.126	
3.319	11.793	3.030	2.179	3.974	4.142	6.826	3.904	2.247	2.875	
7.533	11.612	5.038	3.276	4.515	4.596	6.872	2.136	2.791	4.106	
3.872	12.031	2.971	2.422	3.894	5.108	7.260	2.761	2.222	2.644	
2.888	8.676	4.515	2.181	1.910	2.800	4.672	5.086	2.408	3.001	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.039	0.001	0.049	0.160	0.335	0.044	0.012	0.120	0.161	0.079	
LSPI converged after 5 iterations in 4.530 seconds.
MEAN LOSS: 1.732
MEAN RF POLICY* LOSS IN MDP_m: 0.938
MEAN RF POLICY* SCORE IN MDP: 0.947

LSPI converged after 4 iterations in 3.900 seconds.
~~~ MAGIC SCORE ~~~	0.950
Generating 4 expert demonstrations of size 10...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.275	6.847	5.373	2.967	3.784	2.954	2.777	3.238	3.107	4.477	
2.589	6.844	9.316	3.712	6.592	3.905	2.070	4.101	5.412	5.817	
4.586	11.830	4.765	2.459	6.431	3.257	2.637	2.882	2.664	3.669	
3.018	6.738	4.429	2.486	3.638	2.546	2.411	2.713	2.639	3.843	
2.741	8.519	4.802	2.727	4.419	2.984	2.563	3.076	2.792	3.748	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.042	0.000	0.003	0.055	0.008	0.033	0.789	0.030	0.033	0.009	
LSPI converged after 5 iterations in 4.490 seconds.
MEAN LOSS: 2.403
MEAN RF POLICY* LOSS IN MDP_m: 2.197
MEAN RF POLICY* SCORE IN MDP: 0.835

Loss matrix
1.335	2.459	1.437	1.450	1.405	1.303	1.564	1.311	1.486	1.808	
1.343	2.797	1.766	1.446	1.357	1.367	1.664	1.416	1.665	1.968	
1.328	2.318	1.695	1.525	1.279	1.370	1.783	1.489	1.767	1.977	
1.403	2.191	1.570	1.388	1.302	1.353	1.718	1.404	1.690	1.892	
1.489	2.639	1.782	1.601	1.381	1.483	1.898	1.583	1.917	1.854	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.001	0.000	0.000	0.001	0.995	0.001	0.000	0.001	0.000	0.000	
LSPI converged after 5 iterations in 4.700 seconds.
MEAN LOSS: 1.634
MEAN RF POLICY* LOSS IN MDP_m: 1.627
MEAN RF POLICY* SCORE IN MDP: 0.954

Loss matrix
0.722	2.900	2.421	1.506	1.364	1.070	1.473	0.598	1.433	1.497	
0.995	2.999	2.439	1.748	2.828	1.499	1.287	0.664	1.189	2.350	
0.781	3.072	2.383	1.704	1.669	1.150	1.697	0.607	1.350	2.028	
0.759	2.960	2.382	1.665	1.634	1.130	1.517	0.693	1.436	1.716	
0.752	2.744	2.341	1.494	1.595	1.197	1.322	0.775	1.468	1.514	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.146	0.000	0.000	0.003	0.004	0.026	0.009	0.797	0.011	0.003	
LSPI converged after 5 iterations in 4.680 seconds.
MEAN LOSS: 1.924
MEAN RF POLICY* LOSS IN MDP_m: 1.568
MEAN RF POLICY* SCORE IN MDP: 0.919

Loss matrix
2.269	2.890	2.232	1.922	1.630	1.794	1.941	1.257	1.529	1.958	
1.787	2.369	2.119	1.469	1.282	1.224	1.361	0.965	1.365	1.753	
2.378	2.831	2.298	2.018	1.644	1.946	2.022	1.259	1.492	2.061	
1.879	2.951	2.319	1.996	1.622	1.712	1.763	1.161	1.436	1.745	
1.889	2.669	2.036	1.331	1.291	1.203	1.665	1.021	1.399	1.821	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.036	0.010	0.028	0.060	0.095	0.086	0.057	0.470	0.111	0.046	
LSPI converged after 5 iterations in 4.720 seconds.
MEAN LOSS: 1.734
MEAN RF POLICY* LOSS IN MDP_m: 0.984
MEAN RF POLICY* SCORE IN MDP: 0.923

LSPI converged after 4 iterations in 3.890 seconds.
~~~ MAGIC SCORE ~~~	0.950
Generating 4 expert demonstrations of size 20...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.596	2.788	2.282	1.543	2.035	1.139	1.252	1.854	1.297	2.234	
1.369	2.894	2.328	1.653	2.181	1.239	1.333	1.846	1.425	1.924	
1.592	3.098	2.265	1.558	2.099	1.217	1.303	1.851	1.314	2.427	
1.783	3.474	2.347	1.730	2.955	1.236	1.444	1.902	1.479	2.651	
1.127	2.293	2.250	1.438	1.756	1.132	1.262	1.807	1.316	1.930	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.410	0.009	0.019	0.067	0.027	0.196	0.109	0.039	0.102	0.024	
LSPI converged after 5 iterations in 4.680 seconds.
MEAN LOSS: 2.429
MEAN RF POLICY* LOSS IN MDP_m: 1.654
MEAN RF POLICY* SCORE IN MDP: 0.934

Loss matrix
0.906	4.330	3.927	1.652	2.840	1.471	1.147	1.731	1.638	2.572	
0.997	3.554	3.256	1.497	2.021	1.392	1.197	1.511	1.532	2.377	
0.939	3.458	3.288	1.509	1.863	1.422	1.243	1.601	1.618	2.304	
0.943	4.455	4.213	1.740	2.897	1.547	1.081	1.850	1.811	3.060	
1.193	4.073	3.566	1.735	2.655	1.475	1.319	1.666	1.648	2.952	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.990	0.000	0.000	0.000	0.000	0.001	0.008	0.000	0.000	0.000	
LSPI converged after 4 iterations in 3.880 seconds.
MEAN LOSS: 1.933
MEAN RF POLICY* LOSS IN MDP_m: 1.919
MEAN RF POLICY* SCORE IN MDP: 0.941

Loss matrix
2.252	3.444	3.416	2.167	1.960	2.147	1.763	2.063	2.154	2.668	
3.675	6.637	3.500	2.683	4.020	3.113	2.478	2.657	2.463	3.889	
3.322	5.873	3.820	2.860	3.699	3.319	2.665	2.655	2.798	3.569	
3.010	6.943	3.716	2.849	4.419	3.453	2.751	2.532	2.846	4.120	
2.696	4.662	3.459	2.394	2.817	2.605	1.892	2.171	2.208	3.127	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.000	0.000	0.001	0.001	0.000	0.995	0.002	0.001	0.000	
LSPI converged after 5 iterations in 4.750 seconds.
MEAN LOSS: 2.411
MEAN RF POLICY* LOSS IN MDP_m: 2.403
MEAN RF POLICY* SCORE IN MDP: 0.840

Loss matrix
2.364	2.926	5.174	2.523	1.878	2.447	2.431	2.772	3.124	4.124	
2.307	2.805	5.434	2.495	1.774	2.366	2.328	2.762	3.380	4.197	
2.204	2.732	5.483	2.598	1.852	2.424	2.357	2.612	3.536	3.743	
2.293	2.714	4.834	2.532	1.707	2.340	2.359	2.627	2.698	3.994	
2.366	2.825	4.714	2.473	1.805	2.427	2.461	2.610	2.729	4.005	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.100	0.048	0.005	0.063	0.525	0.072	0.073	0.053	0.042	0.019	
LSPI converged after 4 iterations in 3.770 seconds.
MEAN LOSS: 1.815
MEAN RF POLICY* LOSS IN MDP_m: 1.281
MEAN RF POLICY* SCORE IN MDP: 0.944

LSPI converged after 6 iterations in 5.340 seconds.
~~~ MAGIC SCORE ~~~	0.935
Generating 4 expert demonstrations of size 20...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.191	2.130	2.151	1.624	0.907	1.398	1.748	1.015	1.393	1.313	
1.100	2.244	1.591	1.807	0.985	1.489	1.661	0.956	1.557	1.269	
1.041	2.091	1.341	1.673	0.915	1.409	1.606	0.889	1.445	1.106	
1.053	1.999	2.500	1.649	0.943	1.405	1.647	0.944	1.318	1.899	
1.034	2.159	2.298	1.917	1.085	1.561	1.796	1.010	1.568	1.213	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.121	0.011	0.021	0.026	0.183	0.047	0.029	0.442	0.050	0.071	
LSPI converged after 5 iterations in 4.660 seconds.
MEAN LOSS: 2.276
MEAN RF POLICY* LOSS IN MDP_m: 1.646
MEAN RF POLICY* SCORE IN MDP: 0.952

Loss matrix
1.369	2.579	2.016	1.567	1.263	1.210	1.199	1.860	1.259	1.447	
1.502	2.478	2.007	1.168	1.092	0.987	1.077	1.616	1.245	1.506	
1.659	2.749	2.212	1.356	1.196	1.147	1.132	1.642	1.343	1.517	
1.325	2.566	2.049	1.491	1.105	1.208	1.188	1.612	1.291	1.396	
1.373	2.672	2.118	1.560	1.209	1.168	1.112	1.574	1.266	1.307	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.001	0.000	0.000	0.001	0.005	0.984	0.007	0.000	0.002	0.001	
LSPI converged after 5 iterations in 4.630 seconds.
MEAN LOSS: 1.604
MEAN RF POLICY* LOSS IN MDP_m: 1.590
MEAN RF POLICY* SCORE IN MDP: 0.807

Loss matrix
1.174	2.545	1.987	1.641	1.171	1.205	1.462	0.798	1.304	1.427	
1.110	2.606	2.309	1.807	1.186	1.348	1.527	0.951	1.371	1.385	
1.176	2.906	2.580	2.073	1.365	1.372	1.796	1.092	1.466	1.458	
1.360	2.347	1.628	1.823	1.203	1.038	1.441	0.868	1.177	1.597	
1.190	2.648	2.135	1.732	1.253	1.251	1.503	0.862	1.372	1.474	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.027	0.000	0.001	0.002	0.021	0.028	0.006	0.895	0.012	0.007	
LSPI converged after 5 iterations in 4.600 seconds.
MEAN LOSS: 1.899
MEAN RF POLICY* LOSS IN MDP_m: 1.721
MEAN RF POLICY* SCORE IN MDP: 0.924

Loss matrix
2.431	4.179	3.901	2.558	2.278	2.398	2.519	2.413	2.726	3.680	
2.429	3.986	3.742	2.505	2.166	2.363	2.529	2.394	2.693	3.607	
2.564	4.404	4.104	2.808	2.350	2.634	2.801	2.658	2.970	3.944	
2.660	4.053	3.748	2.657	2.296	2.495	2.764	2.468	2.865	3.944	
2.532	4.002	3.701	2.724	2.353	2.568	2.743	2.549	2.875	3.953	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.074	0.006	0.013	0.062	0.555	0.081	0.060	0.080	0.052	0.017	
LSPI converged after 4 iterations in 3.790 seconds.
MEAN LOSS: 1.712
MEAN RF POLICY* LOSS IN MDP_m: 1.117
MEAN RF POLICY* SCORE IN MDP: 0.946

LSPI converged after 4 iterations in 3.830 seconds.
~~~ MAGIC SCORE ~~~	0.946
Generating 4 expert demonstrations of size 50...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
3.020	8.114	5.606	3.579	2.232	2.205	2.134	3.385	3.044	10.033	
2.267	14.654	3.739	2.451	3.376	3.096	5.180	2.517	2.277	5.488	
2.487	12.377	3.785	2.384	2.160	2.323	4.060	3.024	2.292	7.486	
2.286	12.126	3.662	2.343	2.174	2.454	4.574	2.710	2.175	7.631	
1.929	13.535	3.531	2.355	2.209	2.529	4.573	2.985	2.155	6.745	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.658	0.000	0.010	0.049	0.077	0.056	0.047	0.034	0.068	0.001	
LSPI converged after 5 iterations in 4.670 seconds.
MEAN LOSS: 2.470
MEAN RF POLICY* LOSS IN MDP_m: 2.003
MEAN RF POLICY* SCORE IN MDP: 0.940

Loss matrix
1.019	2.517	2.052	1.306	1.209	1.111	1.195	1.076	1.395	1.472	
0.941	2.519	2.015	1.318	1.270	1.125	1.172	1.095	1.337	1.329	
0.930	2.485	2.112	1.356	1.185	1.154	1.202	1.132	1.379	1.501	
0.825	2.532	2.208	1.315	1.218	1.123	1.216	1.180	1.406	1.422	
0.883	2.440	2.197	1.295	1.141	1.106	1.350	1.190	1.467	1.571	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.985	0.000	0.000	0.001	0.002	0.004	0.002	0.005	0.001	0.000	
LSPI converged after 4 iterations in 3.790 seconds.
MEAN LOSS: 1.929
MEAN RF POLICY* LOSS IN MDP_m: 1.904
MEAN RF POLICY* SCORE IN MDP: 0.938

Loss matrix
1.035	2.605	2.186	1.368	1.066	1.130	1.374	1.034	1.345	1.397	
1.024	2.593	2.200	1.365	1.019	1.123	1.384	1.053	1.369	1.286	
0.992	2.673	2.270	1.410	1.069	1.133	1.358	1.016	1.361	1.305	
1.029	2.657	2.254	1.435	1.104	1.156	1.385	1.062	1.378	1.395	
1.040	2.567	2.171	1.351	1.021	1.127	1.362	1.026	1.361	1.338	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.849	0.000	0.002	0.008	0.040	0.026	0.008	0.045	0.009	0.012	
LSPI converged after 4 iterations in 3.820 seconds.
MEAN LOSS: 2.108
MEAN RF POLICY* LOSS IN MDP_m: 1.851
MEAN RF POLICY* SCORE IN MDP: 0.942

Loss matrix
1.922	2.891	2.886	2.369	1.559	1.939	2.116	1.894	2.274	2.544	
1.970	2.794	2.647	2.123	1.379	1.825	2.014	1.688	2.134	2.389	
1.929	2.886	2.810	2.301	1.491	1.940	2.114	1.845	2.228	2.472	
1.840	2.828	2.687	2.156	1.363	1.780	2.013	1.692	2.121	2.288	
1.932	2.831	2.743	2.274	1.460	1.895	2.098	1.784	2.221	2.490	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.068	0.002	0.007	0.033	0.594	0.079	0.050	0.108	0.037	0.022	
LSPI converged after 4 iterations in 3.990 seconds.
MEAN LOSS: 1.697
MEAN RF POLICY* LOSS IN MDP_m: 1.101
MEAN RF POLICY* SCORE IN MDP: 0.940

LSPI converged after 5 iterations in 4.700 seconds.
~~~ MAGIC SCORE ~~~	0.940
Generating 4 expert demonstrations of size 50...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
2.027	3.144	2.928	1.922	1.878	2.151	1.597	1.847	1.815	2.366	
1.933	3.326	3.252	2.039	2.185	2.216	1.583	1.881	1.831	2.500	
1.785	2.950	2.719	1.892	1.780	2.010	1.540	1.798	1.655	2.128	
1.691	2.931	2.581	1.786	1.529	1.902	1.524	1.637	1.628	2.046	
1.814	3.252	3.047	1.936	1.879	2.111	1.576	1.859	1.850	2.293	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.054	0.003	0.009	0.045	0.060	0.035	0.641	0.057	0.069	0.027	
LSPI converged after 5 iterations in 4.640 seconds.
MEAN LOSS: 2.374
MEAN RF POLICY* LOSS IN MDP_m: 1.982
MEAN RF POLICY* SCORE IN MDP: 0.861

Loss matrix
1.041	2.536	1.975	1.297	1.070	1.179	1.072	1.151	1.212	0.968	
0.969	2.729	2.022	1.308	1.358	1.175	1.047	1.188	1.135	0.907	
0.942	2.551	1.942	1.199	1.031	1.087	1.080	1.075	1.111	0.866	
0.986	2.526	1.935	1.270	0.976	1.142	1.086	1.133	1.212	1.043	
0.989	2.478	1.854	1.251	1.061	1.155	1.013	1.103	1.094	0.965	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.010	0.000	0.000	0.001	0.005	0.002	0.005	0.003	0.002	0.971	
LSPI converged after 4 iterations in 3.830 seconds.
MEAN LOSS: 2.534
MEAN RF POLICY* LOSS IN MDP_m: 2.505
MEAN RF POLICY* SCORE IN MDP: 0.917

Loss matrix
1.393	2.727	2.405	1.120	0.902	1.070	1.303	1.096	1.222	1.170	
1.371	2.805	2.475	1.169	0.943	1.079	1.308	1.151	1.234	1.139	
1.168	2.646	2.368	1.128	0.836	1.089	1.275	1.091	1.232	1.100	
1.407	2.806	2.515	1.234	0.930	1.128	1.349	1.152	1.300	1.207	
1.402	2.742	2.452	1.145	0.886	1.081	1.250	1.098	1.192	1.101	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.009	0.000	0.001	0.018	0.871	0.035	0.011	0.022	0.013	0.019	
LSPI converged after 5 iterations in 4.580 seconds.
MEAN LOSS: 1.963
MEAN RF POLICY* LOSS IN MDP_m: 1.793
MEAN RF POLICY* SCORE IN MDP: 0.942

Loss matrix
1.600	2.718	2.482	1.925	1.377	1.935	1.855	1.539	1.692	1.706	
1.603	2.668	2.403	1.840	1.368	1.945	1.899	1.564	1.747	1.848	
1.814	2.775	2.438	2.061	1.543	2.207	2.021	1.687	1.868	2.066	
1.738	2.774	2.572	2.033	1.476	2.154	2.073	1.717	1.872	2.055	
1.703	2.733	2.558	2.007	1.442	2.093	1.939	1.644	1.751	1.814	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.091	0.003	0.016	0.046	0.525	0.039	0.046	0.114	0.068	0.052	
LSPI converged after 5 iterations in 4.620 seconds.
MEAN LOSS: 1.732
MEAN RF POLICY* LOSS IN MDP_m: 0.895
MEAN RF POLICY* SCORE IN MDP: 0.939

LSPI converged after 5 iterations in 4.520 seconds.
~~~ MAGIC SCORE ~~~	0.948
Generating 4 expert demonstrations of size 100...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
1.858	2.437	2.360	1.816	1.360	1.934	1.720	1.724	1.811	2.493	
1.723	2.574	2.381	1.735	1.319	1.791	1.662	1.721	1.819	2.637	
1.747	2.435	2.555	1.669	1.298	1.784	1.635	1.595	1.738	2.528	
1.719	2.530	2.354	1.773	1.347	1.814	1.662	1.654	1.794	2.148	
1.730	2.364	2.627	1.598	1.222	1.709	1.581	1.566	1.691	2.431	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.041	0.004	0.007	0.045	0.689	0.037	0.058	0.071	0.039	0.008	
LSPI converged after 4 iterations in 3.860 seconds.
MEAN LOSS: 1.778
MEAN RF POLICY* LOSS IN MDP_m: 1.482
MEAN RF POLICY* SCORE IN MDP: 0.943

Loss matrix
2.321	3.786	3.503	2.354	2.064	2.130	2.490	1.849	2.363	3.163	
2.337	3.754	3.489	2.315	2.043	2.136	2.487	2.047	2.357	3.247	
2.213	3.654	3.388	2.240	1.980	2.066	2.418	1.888	2.269	3.093	
2.251	3.760	3.534	2.383	2.059	2.180	2.457	2.010	2.440	3.305	
2.366	3.768	3.520	2.366	2.086	2.189	2.477	2.062	2.384	3.305	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.000	0.000	0.000	0.000	0.000	0.000	1.000	0.000	0.000	
LSPI converged after 5 iterations in 4.720 seconds.
MEAN LOSS: 1.668
MEAN RF POLICY* LOSS IN MDP_m: 1.667
MEAN RF POLICY* SCORE IN MDP: 0.917

Loss matrix
3.496	3.737	2.901	2.174	2.061	2.181	2.541	1.936	2.217	2.677	
3.149	3.428	2.792	2.095	1.808	1.975	2.130	1.765	2.152	2.583	
3.194	3.439	2.775	2.119	1.798	1.991	2.136	1.767	2.146	2.545	
3.199	3.785	3.000	2.276	2.145	2.124	2.385	1.870	2.278	2.664	
3.290	3.582	2.814	2.156	1.975	2.053	2.346	1.804	2.204	2.604	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.000	0.000	0.001	0.005	0.002	0.001	0.989	0.001	0.000	
LSPI converged after 5 iterations in 4.640 seconds.
MEAN LOSS: 1.880
MEAN RF POLICY* LOSS IN MDP_m: 1.861
MEAN RF POLICY* SCORE IN MDP: 0.924

Loss matrix
2.103	2.361	2.390	2.128	1.552	1.851	1.980	2.047	2.138	2.550	
2.046	2.412	2.314	1.999	1.506	1.852	1.902	2.009	2.092	2.385	
1.999	2.720	2.370	2.047	1.533	1.837	1.850	2.027	2.093	2.398	
2.098	2.553	2.370	2.116	1.631	1.979	2.001	2.070	2.202	2.523	
2.055	2.463	2.349	2.141	1.559	1.935	2.007	2.039	2.144	2.454	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.042	0.012	0.019	0.038	0.638	0.095	0.063	0.045	0.035	0.013	
LSPI converged after 5 iterations in 4.670 seconds.
MEAN LOSS: 1.725
MEAN RF POLICY* LOSS IN MDP_m: 1.293
MEAN RF POLICY* SCORE IN MDP: 0.945

LSPI converged after 5 iterations in 4.710 seconds.
~~~ MAGIC SCORE ~~~	0.954
Generating 4 expert demonstrations of size 100...
Initializing 4 policy posteriors...
Sampling 5 policies from 4 experts...
Initializing 4 BMT objects...
Loss matrix
2.620	2.899	3.278	2.465	1.999	2.271	1.970	2.094	2.265	2.930	
2.525	2.602	2.777	2.202	1.734	2.033	1.795	1.724	2.043	2.624	
2.509	2.782	2.739	2.303	1.805	2.162	1.878	1.795	2.117	2.655	
2.484	2.664	2.790	2.249	1.723	2.123	1.820	1.802	2.084	2.617	
2.392	2.607	2.776	2.162	1.722	2.030	1.793	1.788	2.042	2.642	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.018	0.011	0.008	0.031	0.655	0.041	0.093	0.089	0.043	0.011	
LSPI converged after 5 iterations in 4.530 seconds.
MEAN LOSS: 1.817
MEAN RF POLICY* LOSS IN MDP_m: 1.503
MEAN RF POLICY* SCORE IN MDP: 0.941

Loss matrix
1.548	3.126	2.637	1.501	1.221	1.479	1.729	1.555	1.591	1.884	
1.570	3.189	2.705	1.570	1.290	1.502	1.749	1.595	1.632	1.933	
1.505	3.201	2.712	1.489	1.306	1.441	1.666	1.531	1.570	1.923	
1.571	3.179	2.709	1.488	1.266	1.464	1.707	1.510	1.610	1.938	
1.539	3.242	2.743	1.606	1.296	1.508	1.793	1.640	1.656	1.914	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.000	0.000	0.000	0.000	0.997	0.001	0.000	0.000	0.000	0.000	
LSPI converged after 5 iterations in 4.750 seconds.
MEAN LOSS: 1.633
MEAN RF POLICY* LOSS IN MDP_m: 1.629
MEAN RF POLICY* SCORE IN MDP: 0.950

Loss matrix
2.167	5.898	4.486	1.781	1.641	2.782	3.420	2.542	2.191	3.926	
2.051	6.171	3.981	1.762	2.110	2.829	3.551	2.189	2.054	3.537	
2.126	6.738	3.455	1.808	2.664	3.204	4.209	1.780	1.971	2.955	
2.171	6.008	3.961	1.785	2.047	2.939	3.820	2.038	1.999	3.411	
2.057	6.537	4.294	1.724	2.158	2.978	4.032	2.388	1.994	3.801	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.002	0.000	0.000	0.015	0.976	0.000	0.000	0.003	0.003	0.000	
LSPI converged after 6 iterations in 5.470 seconds.
MEAN LOSS: 1.944
MEAN RF POLICY* LOSS IN MDP_m: 1.919
MEAN RF POLICY* SCORE IN MDP: 0.946

Loss matrix
1.993	2.688	3.175	2.375	1.697	2.172	2.197	2.172	2.323	2.958	
1.995	2.618	2.791	2.234	1.633	2.082	2.103	2.060	2.234	2.953	
1.982	2.570	3.137	2.327	1.640	2.138	2.142	2.155	2.297	3.019	
1.984	2.637	2.925	2.289	1.637	2.103	2.109	2.104	2.267	2.912	
1.914	2.573	3.246	2.321	1.669	2.096	2.095	2.183	2.259	3.067	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	
0.109	0.027	0.008	0.042	0.580	0.061	0.058	0.059	0.045	0.010	
LSPI converged after 4 iterations in 3.850 seconds.
MEAN LOSS: 1.759
MEAN RF POLICY* LOSS IN MDP_m: 1.240
MEAN RF POLICY* SCORE IN MDP: 0.941

LSPI converged after 5 iterations in 4.580 seconds.
~~~ MAGIC SCORE ~~~	0.944
Process 19894 detected
