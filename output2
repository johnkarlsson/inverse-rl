Generating LSTDQ demonstrations...
2155 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.156	0.238	0.882	-0.016	0.101	0.138	0.124	-0.230	-1.379	-0.171	0.106	-0.502	0.007	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.051	0.322	0.997	0.019	0.053	0.197	-0.161	-0.309	-1.127	-0.097	0.093	-0.450	-0.022	
Sampling 20 reward functions...
	Sampled reward functions:
	 (0)	0.081	0.227	2.198	0.000	0.085	0.264	-0.047	-1.210	-1.311	0.437	-0.016	3.251	-0.082	
	 (1)	-0.047	-0.424	1.336	-0.024	0.006	1.015	0.079	0.483	-0.552	-0.490	-0.046	0.515	0.001	
	 (2)	-0.016	0.026	0.547	0.139	-0.069	0.943	0.041	-0.330	-0.521	0.125	-0.017	1.139	0.243	
	 (3)	0.198	-0.072	2.189	-0.198	0.078	1.336	-0.207	-0.206	-1.014	0.811	-0.082	-0.689	-0.009	
	 (4)	0.200	0.562	2.063	0.046	-0.279	-0.113	-0.131	-0.837	-1.130	0.064	0.181	2.829	-0.261	
	 (5)	0.223	-0.055	1.536	-0.348	0.049	0.737	-0.085	-0.643	-1.041	0.154	0.050	0.838	0.123	
	 (6)	-0.030	-0.050	1.423	0.508	-0.109	0.132	0.017	-0.329	-0.724	-0.092	0.025	1.454	0.051	
	 (7)	-0.058	0.058	0.665	-0.105	-0.051	0.055	0.055	-0.331	-0.775	0.165	0.122	-0.395	0.035	
	 (8)	-0.230	0.276	0.941	0.557	0.213	-0.192	0.107	-0.797	-1.746	-0.132	0.189	2.424	-0.010	
	 (9)	0.167	0.023	1.290	0.106	0.001	0.900	-0.132	-0.000	-0.786	-0.161	-0.079	-2.297	-0.048	
	 (10)	0.570	0.553	0.604	-0.714	-0.213	-0.146	-0.616	0.000	-2.005	0.079	0.444	-0.596	-0.315	
	 (11)	-0.512	-0.458	0.538	0.543	0.264	-0.270	0.517	-0.289	-0.669	-0.149	-0.127	0.329	0.551	
	 (12)	0.165	-0.077	1.017	-0.271	0.052	0.405	-0.138	-0.046	-0.922	0.237	-0.048	0.826	-0.037	
	 (13)	-0.016	0.129	0.751	0.118	-0.045	0.065	0.040	-0.149	-1.188	-0.309	0.097	0.450	-0.080	
	 (14)	-0.018	0.061	1.175	-0.088	-0.101	0.335	0.189	1.344	0.113	-0.344	-0.668	-1.273	0.178	
	 (15)	-0.070	-0.268	2.134	-0.213	0.109	0.508	-0.156	-0.334	-1.589	0.553	0.371	-0.844	0.242	
	 (16)	0.175	-0.082	1.749	-0.457	-0.083	0.482	-0.120	0.500	-0.603	-0.320	0.034	-0.043	-0.073	
	 (17)	0.001	0.121	0.959	-0.133	0.050	-0.005	0.001	-0.152	-1.145	-0.037	-0.029	0.182	-0.026	
	 (18)	-0.008	-0.195	1.079	-0.072	0.106	-0.423	0.053	0.041	-0.568	-0.009	-0.242	-1.506	-0.050	
	 (19)	0.024	-0.038	1.218	-0.121	-0.075	-0.657	0.056	0.452	-0.019	-0.033	-0.221	-1.068	0.190	
LSPI converged after 5 iterations in 3.140 seconds.
Creating 20 corresponding optimal policies...
LSPI converged after 5 iterations in 3.490 seconds.
LSPI converged after 5 iterations in 3.460 seconds.
LSPI converged after 5 iterations in 3.440 seconds.
LSPI converged after 4 iterations in 2.850 seconds.
LSPI converged after 5 iterations in 3.430 seconds.
LSPI converged after 5 iterations in 3.450 seconds.
LSPI converged after 4 iterations in 2.840 seconds.
LSPI converged after 5 iterations in 3.450 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.410 seconds.
LSPI converged after 5 iterations in 3.430 seconds.
LSPI converged after 5 iterations in 3.420 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.390 seconds.
LSPI converged after 5 iterations in 3.400 seconds.
LSPI converged after 5 iterations in 3.480 seconds.
LSPI converged after 5 iterations in 3.430 seconds.
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.969 }	0.181	0.275	0.864	-0.027	0.103	0.255	0.341	-0.193	-1.538	-0.376	0.111	-0.793	0.007	
	{ -0.136 }	 (random)
	{ -0.116 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.914 }	0.720	0.822	1.459	-0.260	0.359	0.549	0.503	-1.463	-2.886	-0.516	0.280	0.897	-0.198	
 (1)	{ 0.778 }	0.087	0.075	1.061	0.035	0.124	0.818	0.584	0.169	-1.539	-0.819	0.101	-0.256	-0.043	
 (2)	{ 0.913 }	0.241	0.103	0.415	0.046	-0.063	1.034	0.458	-0.476	-0.810	-0.501	0.009	0.186	0.495	
 (3)	{ 0.920 }	0.713	0.555	1.554	-0.333	0.341	1.247	0.775	-0.480	-2.669	-0.350	0.135	-2.688	-0.046	
 (4)	{ 0.906 }	0.623	1.081	1.827	-0.184	-0.325	0.004	0.450	-1.017	-2.325	-0.606	0.491	1.343	-0.392	
 (5)	{ 0.937 }	0.831	0.357	0.938	-0.617	0.215	1.003	0.406	-1.030	-2.271	-0.764	0.338	-0.706	0.185	
 (6)	{ 0.628 }	0.368	0.270	1.155	0.265	-0.055	0.349	0.311	-0.573	-1.449	-0.571	0.180	0.491	0.109	
 (7)	{ 0.844 }	0.062	0.283	0.455	-0.135	-0.052	0.015	0.351	-0.445	-1.338	-0.274	0.303	-1.022	0.054	
 (8)	{ 0.841 }	0.562	0.507	0.012	0.192	0.501	0.162	0.826	-1.214	-3.534	-1.234	0.661	-0.005	-0.153	
 (9)	{ 0.936 }	0.646	0.414	0.987	-0.013	0.146	1.036	0.206	-0.456	-1.661	-0.772	0.047	-2.739	-0.097	
 (10)	{ 0.810 }	1.154	0.844	0.272	-0.880	-0.287	-0.067	-0.220	-0.601	-3.374	-0.255	0.978	-1.304	-0.564	
 (11)	{ 0.508 }	-0.228	-0.434	-0.069	0.289	0.504	0.194	0.976	-0.442	-1.267	-1.046	-0.028	-1.080	0.868	
 (12)	{ 0.936 }	0.413	0.223	0.705	-0.359	0.192	0.435	0.185	-0.289	-1.696	-0.144	0.078	-0.250	-0.090	
 (13)	{ 0.764 }	0.212	0.333	0.510	0.025	0.003	0.141	0.359	-0.451	-1.907	-0.718	0.303	-0.513	-0.154	
 (14)	{ 0.683 }	-0.018	0.276	1.479	-0.032	0.010	0.486	0.965	1.169	0.146	-0.816	-1.109	-1.729	0.444	
 (15)	{ 0.943 }	0.471	0.549	1.024	-0.197	0.308	0.564	0.729	-0.709	-3.948	-0.532	1.069	-2.810	0.238	
 (16)	{ 0.845 }	0.419	0.544	1.431	-0.383	-0.015	0.434	0.569	0.133	-1.940	-0.810	0.300	-0.843	-0.166	
 (17)	{ 0.900 }	0.255	0.435	0.615	-0.149	0.196	0.149	0.273	-0.462	-1.927	-0.463	0.123	-0.954	-0.086	
 (18)	{ 0.850 }	0.078	0.125	0.867	-0.128	0.316	-0.230	0.236	-0.205	-1.053	-0.377	-0.269	-1.853	-0.148	
 (19)	{ 0.817 }	0.096	0.271	1.187	-0.231	-0.048	-0.401	0.539	0.367	-0.406	-0.437	-0.266	-1.457	0.366	
Scores for the experts true policies:
	{ 0.952 }	 (expert 0)
	{ 0.944 }	 (expert 1)
	{ 0.945 }	 (expert 2)
	{ 0.617 }	 (expert 3)
Summed loss of reward-optimal policies w.r.t. true rho values (expert weights)
	1.240	0.547	0.491	1.839	0.974	0.928	0.423	0.316	1.718	0.752	1.101	0.695	0.358	0.255	0.822	2.225	1.048	0.325	0.566	0.371	
	1.230	0.523	0.526	1.896	0.949	0.984	0.405	0.360	1.708	0.807	1.099	0.734	0.356	0.286	0.852	2.274	1.028	0.381	0.591	0.404	
	1.568	0.865	0.672	2.140	1.295	1.215	0.680	0.488	2.049	1.005	1.417	0.840	0.687	0.549	0.927	2.533	1.379	0.616	0.489	0.533	
	1.291	0.559	0.578	1.915	1.022	1.021	0.455	0.421	1.753	0.823	1.137	0.762	0.406	0.319	0.850	2.294	1.081	0.445	0.565	0.408	
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.004	0.000	0.000	0.000	0.000	0.000	0.051	0.000	0.007	0.000	0.001	0.048	0.048	0.000	0.000	0.000	0.170	0.666	0.002	
LSPI converged after 4 iterations in 0.050 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.399

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.007	0.000	0.000	0.001	0.000	0.001	0.022	0.000	0.012	0.001	0.000	0.051	0.155	0.001	0.000	0.001	0.157	0.586	0.005	
LSPI converged after 4 iterations in 0.060 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.927

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.008	0.001	0.000	0.000	0.000	0.001	0.040	0.000	0.007	0.000	0.002	0.071	0.032	0.000	0.000	0.001	0.058	0.776	0.002	
LSPI converged after 6 iterations in 0.080 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.912

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.003	0.000	0.000	0.001	0.000	0.001	0.112	0.000	0.001	0.000	0.000	0.008	0.017	0.000	0.000	0.001	0.006	0.847	0.003	
LSPI converged after 4 iterations in 0.060 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.627

Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.019	0.001	0.000	0.001	0.000	0.003	0.024	0.000	0.024	0.000	0.001	0.076	0.061	0.005	0.000	0.005	0.102	0.667	0.011	
LSPI converged after 4 iterations in 0.550 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.531

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.030	0.001	0.000	0.002	0.000	0.003	0.043	0.000	0.016	0.001	0.000	0.119	0.094	0.002	0.000	0.007	0.093	0.577	0.011	
LSPI converged after 4 iterations in 0.550 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.549

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.026	0.001	0.000	0.001	0.001	0.004	0.056	0.000	0.018	0.001	0.002	0.089	0.122	0.001	0.000	0.005	0.075	0.586	0.011	
LSPI converged after 5 iterations in 0.660 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.774

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.011	0.002	0.000	0.001	0.000	0.002	0.043	0.000	0.003	0.000	0.002	0.019	0.026	0.000	0.000	0.004	0.014	0.823	0.051	
LSPI converged after 4 iterations in 0.560 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.537

Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.031	0.002	0.001	0.001	0.000	0.005	0.022	0.000	0.020	0.000	0.001	0.094	0.047	0.007	0.000	0.007	0.077	0.661	0.022	
LSPI converged after 4 iterations in 6.270 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.494

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.022	0.001	0.001	0.001	0.000	0.004	0.022	0.000	0.012	0.000	0.001	0.087	0.030	0.002	0.000	0.006	0.060	0.722	0.029	
LSPI converged after 4 iterations in 6.190 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.663

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.032	0.004	0.001	0.002	0.001	0.011	0.047	0.000	0.019	0.000	0.003	0.059	0.081	0.002	0.000	0.008	0.053	0.642	0.035	
LSPI converged after 6 iterations in 8.540 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.803

Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.022	0.002	0.000	0.000	0.001	0.003	0.017	0.000	0.006	0.000	0.003	0.043	0.043	0.000	0.000	0.005	0.041	0.799	0.013	
LSPI converged after 5 iterations in 7.580 seconds.
MEAN RF POLICY* IN MDP_m (loss): 0.551

