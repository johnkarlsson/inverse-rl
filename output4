Generating LSTDQ demonstrations...
2151 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.157	0.259	0.923	-0.029	0.075	0.114	0.126	-0.231	-1.392	-0.158	0.120	-0.500	-0.001	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.046	0.324	1.019	0.006	0.036	0.176	-0.155	-0.298	-1.131	-0.091	0.102	-0.436	-0.020	
Sampling 20 reward functions...
	Sampled reward functions:
	 (0)	-0.043	0.327	2.241	0.039	0.063	-2.761	0.041	-1.529	-1.568	0.818	0.105	2.467	0.047	
	 (1)	-0.107	-0.091	0.847	0.445	-0.026	-0.026	0.126	0.271	-0.978	-0.174	0.002	-0.224	-0.029	
	 (2)	0.282	0.334	1.744	-0.111	-0.070	0.897	-0.218	-0.634	-0.933	0.200	-0.034	2.097	-0.089	
	 (3)	-0.042	-0.260	1.011	-0.127	0.079	1.658	0.166	0.367	-0.396	-0.192	-0.483	-3.224	0.614	
	 (4)	-0.016	-0.357	1.892	0.176	-0.002	1.998	0.157	0.040	0.347	0.250	-0.629	1.650	0.239	
	 (5)	0.348	0.187	2.114	-0.044	-0.369	1.678	-0.385	-0.561	-1.022	0.403	0.182	1.873	-0.597	
	 (6)	-0.085	0.192	0.927	0.534	-0.025	-0.866	0.038	-0.431	-1.105	-0.301	0.047	4.033	-0.091	
	 (7)	-0.122	-0.449	0.268	0.059	0.167	0.425	0.020	0.314	-0.949	-0.100	0.109	-0.230	0.106	
	 (8)	0.251	0.309	1.944	-0.170	-0.256	-0.261	-0.164	-0.503	-1.017	0.368	-0.049	1.608	-0.244	
	 (9)	-0.125	-0.025	1.617	-0.026	0.160	0.446	0.142	0.428	-0.839	-0.320	-0.185	-0.979	-0.140	
	 (10)	0.045	-0.468	1.153	-0.287	0.029	1.097	-0.009	0.437	-0.390	-0.007	-0.053	-0.026	0.156	
	 (11)	-0.088	0.057	0.980	0.293	0.031	0.457	0.158	0.029	-1.069	-0.233	-0.072	-0.847	0.104	
	 (12)	0.292	0.314	1.723	-0.181	-0.195	-0.157	-0.292	-0.203	-1.385	0.170	0.281	-0.980	-0.360	
	 (13)	-0.041	-0.294	1.698	0.196	-0.136	-0.285	0.066	0.523	-0.385	-0.287	-0.161	0.520	-0.013	
	 (14)	0.214	0.673	2.774	0.080	-0.055	0.153	-0.181	-2.186	-1.859	0.853	0.136	2.679	-0.211	
	 (15)	-0.054	-0.303	1.104	0.495	0.054	1.539	0.151	0.160	-0.601	-0.106	-0.108	1.005	0.171	
	 (16)	0.106	-0.034	1.299	-0.469	-0.067	-0.681	-0.088	0.060	-0.779	0.397	0.011	0.894	0.076	
	 (17)	0.147	0.136	1.230	-0.155	-0.109	0.634	-0.035	0.056	-1.239	0.002	0.094	-0.859	-0.152	
	 (18)	0.221	-0.130	1.151	-0.427	0.003	-1.029	-0.187	-0.028	-0.879	0.396	-0.039	2.980	-0.038	
	 (19)	0.156	0.627	2.564	0.078	-0.215	-0.693	-0.158	-0.477	-1.344	0.070	0.104	-1.582	-0.267	
LSPI converged after 5 iterations in 3.150 seconds.
Creating 20 corresponding optimal policies...
 in 3.400 seconds.
 in 3.420 seconds.
 in 3.400 seconds.
 in 3.430 seconds.
 in 3.480 seconds.
 in 3.560 seconds.
 in 3.590 seconds.
 in 3.620 seconds.
 in 3.640 seconds.
 in 3.060 seconds.
 in 3.730 seconds.
 in 3.890 seconds.
 in 3.800 seconds.
 in 3.030 seconds.
 in 3.610 seconds.
 in 3.780 seconds.
 in 3.900 seconds.
 in 3.920 seconds.
 in 3.930 seconds.
 in 3.950 seconds.
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.970 }	0.187	0.308	0.920	-0.045	0.073	0.226	0.337	-0.197	-1.546	-0.358	0.124	-0.784	-0.005	
	{ -0.138 }	 (random)
	{ -0.123 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.924 }	0.542	0.884	1.522	-0.493	0.223	-2.019	0.544	-1.641	-3.117	-0.149	0.530	0.134	-0.029	
 (1)	{ 0.716 }	0.168	0.053	0.670	0.184	0.038	-0.055	0.699	0.012	-1.704	-0.753	0.126	-1.371	-0.060	
 (2)	{ 0.952 }	0.881	0.869	1.455	-0.299	0.015	1.143	0.199	-0.964	-1.994	-0.439	0.141	0.366	-0.138	
 (3)	{ 0.954 }	0.315	0.014	0.849	-0.116	0.285	2.018	0.663	-0.095	-0.599	-0.975	-0.723	-3.372	1.088	
 (4)	{ 0.655 }	0.178	0.216	1.960	0.236	0.177	2.108	0.670	-0.038	0.085	-0.283	-0.989	0.321	0.451	
 (5)	{ 0.712 }	0.589	0.923	2.202	-0.057	-0.515	1.141	0.112	-0.840	-2.199	-0.023	0.489	0.696	-1.020	
 (6)	{ 0.702 }	0.244	0.310	0.662	0.250	0.051	-0.560	0.292	-0.575	-1.807	-0.446	0.219	2.281	-0.157	
 (7)	{ 0.562 }	0.051	-0.338	-0.204	-0.024	0.318	0.408	0.351	0.068	-1.798	-0.387	0.362	-1.055	0.087	
 (8)	{ 0.960 }	0.450	0.875	2.027	-0.359	-0.328	-0.263	0.163	-0.698	-1.673	-0.048	0.056	0.414	-0.384	
 (9)	{ 0.838 }	0.252	0.415	1.068	-0.164	0.511	0.548	1.080	-0.019	-2.345	-1.309	-0.039	-2.274	-0.357	
 (10)	{ 0.939 }	0.180	0.006	0.912	-0.215	0.098	0.907	0.450	0.215	-1.220	-0.339	0.080	-0.856	0.231	
 (11)	{ 0.910 }	0.291	0.302	0.708	0.105	0.146	0.643	0.771	-0.340	-1.860	-1.089	0.032	-2.082	0.172	
 (12)	{ 0.948 }	0.736	0.885	1.422	-0.425	-0.228	-0.246	0.268	-0.559	-2.889	-0.420	0.744	-2.008	-0.684	
 (13)	{ 0.751 }	0.083	0.315	1.713	0.083	-0.135	-0.237	0.397	0.280	-1.069	-0.420	-0.111	-0.193	-0.041	
 (14)	{ 0.933 }	1.161	1.541	2.104	-0.323	0.074	0.427	0.371	-2.527	-3.774	-0.387	0.612	-0.107	-0.440	
 (15)	{ 0.906 }	0.490	-0.185	0.765	0.118	0.211	1.696	0.980	-0.125	-1.498	-1.050	-0.038	-0.745	0.317	
 (16)	{ 0.940 }	0.230	0.308	1.128	-0.643	-0.072	-0.523	0.482	-0.057	-1.565	-0.138	0.188	-0.252	0.107	
 (17)	{ 0.853 }	0.488	0.512	1.065	-0.368	-0.108	0.490	0.689	-0.272	-2.294	-0.777	0.324	-2.116	-0.257	
 (18)	{ 0.938 }	0.418	0.084	0.940	-0.746	0.068	-0.779	0.215	-0.143	-1.610	0.143	0.105	1.468	-0.084	
 (19)	{ 0.928 }	0.705	1.452	2.263	-0.146	-0.214	-0.481	0.505	-0.907	-2.935	-0.864	0.491	-2.570	-0.503	
Scores for the experts true policies:
	{ 0.948 }	 (expert 0)
	{ 0.946 }	 (expert 1)
	{ 0.947 }	 (expert 2)
	{ 0.627 }	 (expert 3)
Summed loss of reward-optimal policies w.r.t. true rho values rho_m == MDP_m == V_m == (expert weights)
	1.125	0.559	0.921	1.216	0.968	0.815	0.550	0.438	0.423	1.378	0.466	0.802	1.128	0.345	1.685	1.161	0.479	0.965	0.584	1.350	
	1.161	0.590	0.910	1.272	0.994	0.791	0.492	0.447	0.424	1.420	0.469	0.858	1.172	0.293	1.742	1.166	0.463	1.021	0.527	1.407	
	1.387	0.863	1.247	1.289	1.113	1.088	0.826	0.738	0.567	1.690	0.801	1.088	1.439	0.533	1.969	1.489	0.716	1.264	0.786	1.647	
	1.204	0.586	0.988	1.307	1.050	0.856	0.524	0.449	0.504	1.456	0.513	0.894	1.209	0.286	1.803	1.204	0.518	1.058	0.527	1.446	
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.651	0.533	0.794	0.921	0.977	0.557	0.391	0.232	0.414	0.720	0.353	0.673	0.457	0.354	1.168	1.018	0.281	0.642	0.275	0.617	
0.595	0.580	0.808	0.995	1.365	0.793	0.397	0.242	0.530	0.975	0.477	0.651	0.517	0.703	0.993	1.028	0.359	0.650	0.422	0.677	
0.653	0.570	0.830	0.944	1.256	0.774	0.426	0.241	0.492	0.832	0.421	0.671	0.505	0.632	1.116	1.041	0.345	0.648	0.435	0.665	
0.701	0.566	0.862	0.972	1.284	0.779	0.419	0.265	0.528	0.880	0.441	0.708	0.509	0.702	1.108	1.077	0.380	0.686	0.533	0.684	
0.603	0.572	0.819	0.983	1.333	0.790	0.399	0.261	0.530	0.930	0.469	0.662	0.509	0.720	1.002	1.038	0.377	0.651	0.476	0.653	
0.617	0.564	0.821	0.952	1.276	0.788	0.409	0.259	0.502	0.903	0.440	0.668	0.506	0.662	1.069	1.032	0.375	0.653	0.445	0.657	
0.622	0.568	0.832	0.963	1.350	0.828	0.410	0.248	0.536	0.907	0.459	0.682	0.500	0.704	1.090	1.047	0.363	0.655	0.473	0.662	
0.606	0.580	0.849	0.995	1.382	0.840	0.414	0.243	0.570	0.947	0.473	0.675	0.527	0.719	1.065	1.049	0.362	0.671	0.460	0.709	
0.632	0.588	0.876	1.011	1.362	0.871	0.415	0.257	0.570	0.942	0.484	0.697	0.554	0.719	1.083	1.071	0.359	0.698	0.457	0.737	
0.611	0.588	0.876	1.014	1.365	0.850	0.409	0.251	0.559	0.942	0.475	0.707	0.546	0.699	1.079	1.077	0.352	0.701	0.455	0.727	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.001	0.002	0.000	0.000	0.000	0.000	0.021	0.868	0.004	0.000	0.011	0.000	0.004	0.003	0.000	0.000	0.063	0.001	0.020	0.001	
LSPI converged after 3 iterations in 0.050 seconds.
MEAN LOSS: 0.451
MEAN RF POLICY* IN MDP_m (loss): 0.768

Loss matrix
0.839	0.457	0.724	0.948	0.820	0.581	0.459	0.290	0.394	0.648	0.317	0.560	0.468	0.309	1.270	0.918	0.430	0.526	0.438	0.535	
0.862	0.596	0.909	1.489	1.560	0.776	0.552	0.280	0.618	0.888	0.579	0.722	0.499	0.744	1.274	1.175	0.588	0.607	0.568	0.750	
0.868	0.591	0.907	1.481	1.595	0.814	0.544	0.276	0.621	0.893	0.577	0.715	0.474	0.736	1.286	1.181	0.577	0.605	0.562	0.722	
0.826	0.592	0.844	1.403	1.507	0.746	0.534	0.280	0.575	0.886	0.559	0.688	0.480	0.728	1.167	1.135	0.569	0.592	0.576	0.694	
0.880	0.590	0.893	1.427	1.513	0.792	0.554	0.281	0.611	0.883	0.571	0.702	0.519	0.742	1.280	1.147	0.593	0.605	0.585	0.758	
0.905	0.614	0.916	1.478	1.623	0.855	0.579	0.279	0.673	0.901	0.598	0.716	0.536	0.815	1.289	1.179	0.614	0.614	0.607	0.791	
0.883	0.621	0.904	1.435	1.572	0.831	0.561	0.279	0.651	0.928	0.590	0.716	0.535	0.794	1.260	1.177	0.607	0.630	0.601	0.776	
0.875	0.606	0.925	1.547	1.674	0.827	0.546	0.280	0.647	0.946	0.616	0.738	0.494	0.777	1.281	1.205	0.611	0.623	0.587	0.760	
0.903	0.587	0.954	1.516	1.602	0.837	0.561	0.280	0.655	0.903	0.593	0.730	0.531	0.737	1.353	1.188	0.605	0.622	0.579	0.790	
0.763	0.591	0.855	1.414	1.511	0.778	0.522	0.280	0.582	0.913	0.560	0.700	0.485	0.703	1.163	1.153	0.550	0.604	0.543	0.708	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.002	0.000	0.000	0.000	0.000	0.005	0.945	0.007	0.000	0.008	0.001	0.013	0.009	0.000	0.000	0.004	0.002	0.003	0.001	
LSPI converged after 4 iterations in 0.070 seconds.
MEAN LOSS: 0.458
MEAN RF POLICY* IN MDP_m (loss): 0.448

Loss matrix
0.989	0.388	0.488	0.494	0.507	0.354	0.428	0.416	0.296	0.566	0.224	0.373	0.485	0.332	0.995	0.569	0.476	0.445	0.508	0.435	
1.263	0.494	0.624	0.976	0.957	0.544	0.580	0.364	0.565	0.584	0.463	0.487	0.526	0.757	1.129	0.764	0.735	0.471	0.753	0.676	
1.228	0.509	0.655	1.105	1.059	0.533	0.553	0.381	0.554	0.656	0.510	0.536	0.523	0.777	1.102	0.830	0.745	0.511	0.741	0.686	
1.335	0.509	0.632	1.027	1.015	0.513	0.594	0.367	0.580	0.613	0.470	0.497	0.521	0.812	1.116	0.782	0.753	0.467	0.807	0.678	
1.225	0.495	0.612	1.012	0.975	0.535	0.566	0.379	0.547	0.603	0.492	0.482	0.508	0.767	1.045	0.773	0.731	0.475	0.745	0.651	
1.249	0.524	0.626	1.079	1.072	0.573	0.573	0.349	0.592	0.594	0.507	0.501	0.524	0.854	1.070	0.810	0.769	0.480	0.794	0.672	
1.246	0.507	0.591	0.941	0.931	0.529	0.561	0.365	0.536	0.598	0.459	0.460	0.520	0.803	1.056	0.749	0.743	0.464	0.782	0.631	
1.281	0.521	0.626	1.068	1.045	0.555	0.581	0.365	0.588	0.617	0.511	0.510	0.523	0.842	1.063	0.802	0.771	0.479	0.799	0.692	
1.320	0.476	0.619	0.935	0.858	0.501	0.596	0.375	0.516	0.549	0.426	0.477	0.532	0.693	1.144	0.740	0.717	0.451	0.756	0.664	
1.218	0.498	0.603	0.982	0.923	0.585	0.546	0.369	0.584	0.611	0.513	0.468	0.569	0.827	1.015	0.735	0.795	0.497	0.811	0.691	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.008	0.002	0.000	0.000	0.007	0.003	0.139	0.034	0.002	0.764	0.011	0.005	0.009	0.000	0.000	0.001	0.012	0.001	0.002	
LSPI converged after 5 iterations in 0.080 seconds.
MEAN LOSS: 0.800
MEAN RF POLICY* IN MDP_m (loss): 0.730

Loss matrix
0.827	0.418	0.682	1.022	1.007	0.682	0.391	0.207	0.471	0.636	0.339	0.524	0.510	0.334	1.136	0.776	0.523	0.554	0.442	0.624	
1.197	0.757	1.032	1.281	1.211	1.121	0.688	0.449	0.849	1.068	0.491	0.911	0.927	0.563	1.743	1.056	0.675	0.981	0.646	1.082	
1.123	0.734	1.019	1.308	1.294	1.095	0.664	0.410	0.828	1.107	0.498	0.893	0.875	0.569	1.689	1.060	0.661	0.950	0.639	1.051	
1.165	0.728	1.022	1.252	1.162	1.107	0.666	0.436	0.822	1.056	0.489	0.889	0.916	0.536	1.720	1.041	0.670	0.973	0.632	1.053	
1.149	0.717	0.993	1.264	1.155	1.039	0.657	0.439	0.770	1.011	0.478	0.883	0.878	0.495	1.684	1.036	0.668	0.943	0.612	1.013	
1.094	0.700	0.974	1.285	1.201	1.064	0.636	0.415	0.798	1.003	0.492	0.854	0.878	0.559	1.604	1.019	0.686	0.930	0.627	1.002	
1.093	0.751	0.971	1.255	1.215	1.067	0.664	0.433	0.802	1.078	0.492	0.877	0.880	0.574	1.588	1.050	0.662	0.942	0.643	1.011	
1.128	0.707	0.983	1.252	1.184	1.046	0.646	0.410	0.783	1.011	0.485	0.861	0.854	0.520	1.648	1.035	0.665	0.919	0.619	0.998	
1.202	0.733	1.055	1.330	1.276	1.118	0.675	0.410	0.871	1.068	0.498	0.909	0.892	0.577	1.784	1.066	0.680	0.958	0.648	1.081	
1.041	0.711	0.944	1.167	1.141	1.064	0.633	0.396	0.766	1.036	0.458	0.833	0.841	0.541	1.544	1.018	0.616	0.908	0.605	0.963	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.003	0.000	0.000	0.000	0.000	0.007	0.895	0.001	0.000	0.030	0.001	0.001	0.055	0.000	0.000	0.002	0.000	0.004	0.000	
LSPI converged after 5 iterations in 0.080 seconds.
MEAN LOSS: 0.445
MEAN RF POLICY* IN MDP_m (loss): 0.543

Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.526	0.415	0.579	0.704	0.689	0.354	0.339	0.203	0.340	0.550	0.256	0.520	0.339	0.248	0.789	0.810	0.271	0.485	0.255	0.448	
0.508	0.425	0.619	0.752	0.802	0.367	0.338	0.197	0.336	0.611	0.277	0.525	0.339	0.336	0.847	0.835	0.287	0.479	0.263	0.471	
0.523	0.421	0.605	0.745	0.783	0.366	0.334	0.195	0.339	0.604	0.282	0.521	0.333	0.333	0.826	0.828	0.287	0.481	0.270	0.461	
0.497	0.421	0.604	0.729	0.772	0.360	0.331	0.198	0.326	0.599	0.277	0.516	0.334	0.348	0.821	0.823	0.284	0.476	0.262	0.457	
0.497	0.422	0.611	0.764	0.825	0.368	0.333	0.195	0.321	0.616	0.285	0.516	0.326	0.366	0.802	0.828	0.281	0.478	0.267	0.454	
0.516	0.425	0.604	0.752	0.809	0.369	0.336	0.193	0.324	0.614	0.289	0.519	0.333	0.357	0.821	0.829	0.278	0.478	0.264	0.459	
0.494	0.419	0.596	0.742	0.788	0.361	0.328	0.195	0.325	0.602	0.281	0.512	0.329	0.349	0.793	0.819	0.284	0.473	0.270	0.452	
0.503	0.421	0.608	0.743	0.786	0.379	0.331	0.198	0.332	0.610	0.283	0.517	0.332	0.349	0.820	0.826	0.281	0.481	0.260	0.461	
0.493	0.420	0.608	0.745	0.791	0.359	0.333	0.196	0.321	0.599	0.277	0.518	0.330	0.339	0.814	0.828	0.279	0.475	0.257	0.456	
0.499	0.423	0.627	0.742	0.795	0.378	0.338	0.199	0.332	0.599	0.276	0.520	0.331	0.355	0.839	0.832	0.283	0.480	0.269	0.465	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.002	0.006	0.001	0.000	0.000	0.011	0.016	0.734	0.019	0.001	0.046	0.002	0.017	0.030	0.000	0.000	0.043	0.003	0.068	0.004	
LSPI converged after 4 iterations in 0.650 seconds.
MEAN LOSS: 0.474
MEAN RF POLICY* IN MDP_m (loss): 0.428

Loss matrix
0.951	0.358	0.504	0.555	0.461	0.361	0.403	0.270	0.260	0.503	0.235	0.415	0.394	0.241	1.073	0.648	0.371	0.412	0.376	0.411	
0.972	0.403	0.550	0.800	0.758	0.415	0.404	0.268	0.322	0.615	0.366	0.459	0.391	0.423	1.010	0.727	0.507	0.427	0.468	0.463	
0.954	0.405	0.540	0.760	0.739	0.422	0.406	0.271	0.308	0.613	0.354	0.456	0.390	0.416	0.990	0.722	0.488	0.426	0.458	0.455	
0.968	0.399	0.559	0.813	0.768	0.409	0.405	0.268	0.312	0.597	0.363	0.464	0.381	0.412	1.020	0.732	0.493	0.425	0.459	0.459	
0.939	0.399	0.549	0.787	0.745	0.410	0.399	0.266	0.291	0.599	0.350	0.460	0.382	0.395	1.005	0.727	0.472	0.427	0.438	0.448	
0.949	0.397	0.543	0.801	0.747	0.404	0.398	0.270	0.311	0.592	0.361	0.456	0.380	0.403	0.990	0.722	0.493	0.421	0.454	0.448	
0.966	0.408	0.564	0.803	0.761	0.420	0.411	0.269	0.309	0.616	0.362	0.466	0.394	0.417	1.029	0.737	0.485	0.431	0.450	0.471	
0.947	0.407	0.558	0.790	0.750	0.426	0.402	0.267	0.310	0.617	0.359	0.462	0.396	0.410	1.014	0.731	0.492	0.431	0.448	0.465	
0.962	0.405	0.558	0.786	0.752	0.433	0.411	0.269	0.318	0.603	0.363	0.462	0.399	0.420	1.023	0.732	0.490	0.432	0.456	0.472	
0.973	0.396	0.552	0.770	0.710	0.405	0.405	0.269	0.296	0.594	0.343	0.458	0.387	0.375	1.042	0.721	0.477	0.425	0.436	0.457	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.012	0.002	0.000	0.001	0.009	0.010	0.203	0.095	0.001	0.601	0.005	0.014	0.023	0.000	0.000	0.004	0.007	0.006	0.005	
LSPI converged after 5 iterations in 0.780 seconds.
MEAN LOSS: 0.485
MEAN RF POLICY* IN MDP_m (loss): 0.303

Loss matrix
0.723	0.291	0.419	0.502	0.432	0.375	0.323	0.254	0.228	0.521	0.211	0.332	0.365	0.256	0.821	0.529	0.319	0.363	0.340	0.357	
0.747	0.340	0.471	0.713	0.728	0.465	0.345	0.247	0.310	0.564	0.328	0.380	0.373	0.451	0.807	0.621	0.408	0.386	0.409	0.430	
0.741	0.342	0.471	0.696	0.716	0.471	0.347	0.246	0.308	0.569	0.324	0.381	0.378	0.439	0.792	0.621	0.406	0.391	0.408	0.433	
0.731	0.333	0.464	0.703	0.695	0.434	0.339	0.249	0.279	0.566	0.316	0.380	0.363	0.406	0.788	0.616	0.402	0.381	0.405	0.412	
0.711	0.341	0.472	0.714	0.742	0.471	0.341	0.246	0.301	0.578	0.332	0.380	0.367	0.445	0.767	0.626	0.402	0.385	0.410	0.420	
0.742	0.333	0.474	0.688	0.700	0.456	0.347	0.247	0.301	0.564	0.315	0.376	0.374	0.417	0.800	0.611	0.406	0.380	0.417	0.426	
0.744	0.344	0.472	0.713	0.741	0.458	0.350	0.245	0.316	0.567	0.331	0.380	0.375	0.460	0.784	0.624	0.418	0.386	0.428	0.433	
0.728	0.335	0.469	0.698	0.712	0.448	0.341	0.250	0.293	0.572	0.318	0.376	0.372	0.424	0.781	0.615	0.404	0.384	0.408	0.421	
0.701	0.329	0.467	0.682	0.677	0.452	0.333	0.249	0.273	0.579	0.310	0.376	0.368	0.394	0.786	0.609	0.388	0.387	0.384	0.412	
0.722	0.338	0.466	0.725	0.740	0.458	0.335	0.246	0.310	0.564	0.334	0.381	0.361	0.449	0.780	0.623	0.417	0.382	0.416	0.417	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.027	0.004	0.000	0.001	0.005	0.022	0.240	0.084	0.001	0.545	0.012	0.013	0.012	0.000	0.001	0.009	0.010	0.008	0.007	
LSPI converged after 4 iterations in 0.660 seconds.
MEAN LOSS: 0.791
MEAN RF POLICY* IN MDP_m (loss): 0.686

Loss matrix
0.887	0.460	0.721	0.922	0.969	0.699	0.442	0.290	0.546	0.721	0.390	0.557	0.582	0.441	1.208	0.758	0.500	0.577	0.443	0.745	
0.979	0.541	0.794	0.964	1.072	0.809	0.555	0.343	0.660	0.921	0.425	0.639	0.681	0.552	1.314	0.800	0.549	0.691	0.583	0.834	
0.975	0.546	0.799	0.982	1.112	0.820	0.557	0.343	0.666	0.941	0.439	0.645	0.678	0.576	1.311	0.813	0.552	0.696	0.592	0.836	
0.964	0.541	0.803	0.997	1.116	0.809	0.562	0.341	0.658	0.929	0.440	0.642	0.674	0.575	1.302	0.814	0.549	0.692	0.597	0.826	
0.984	0.543	0.814	0.997	1.126	0.839	0.562	0.339	0.690	0.937	0.441	0.647	0.684	0.585	1.330	0.807	0.561	0.699	0.603	0.852	
0.960	0.536	0.795	0.988	1.093	0.799	0.553	0.344	0.650	0.917	0.432	0.638	0.666	0.557	1.300	0.808	0.543	0.683	0.580	0.820	
0.971	0.539	0.803	0.966	1.104	0.836	0.550	0.337	0.675	0.941	0.433	0.639	0.689	0.569	1.317	0.801	0.552	0.701	0.588	0.843	
0.980	0.545	0.810	0.989	1.134	0.829	0.559	0.341	0.677	0.950	0.441	0.648	0.685	0.588	1.324	0.813	0.555	0.701	0.600	0.846	
0.983	0.542	0.803	0.972	1.101	0.821	0.559	0.343	0.670	0.942	0.431	0.640	0.678	0.561	1.326	0.806	0.551	0.691	0.595	0.834	
0.965	0.542	0.804	0.989	1.125	0.833	0.546	0.340	0.679	0.937	0.439	0.644	0.686	0.581	1.308	0.803	0.560	0.701	0.589	0.847	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.007	0.000	0.000	0.000	0.000	0.005	0.927	0.001	0.000	0.045	0.001	0.001	0.004	0.000	0.000	0.005	0.001	0.003	0.000	
LSPI converged after 4 iterations in 0.670 seconds.
MEAN LOSS: 0.455
MEAN RF POLICY* IN MDP_m (loss): 0.700

Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.524	0.422	0.588	0.740	0.761	0.357	0.333	0.190	0.319	0.580	0.269	0.538	0.349	0.298	0.799	0.831	0.277	0.497	0.256	0.438	
0.518	0.424	0.592	0.745	0.772	0.367	0.334	0.189	0.318	0.588	0.273	0.539	0.349	0.312	0.801	0.834	0.278	0.497	0.254	0.440	
0.518	0.424	0.591	0.745	0.772	0.365	0.333	0.190	0.319	0.587	0.271	0.538	0.348	0.309	0.800	0.833	0.279	0.495	0.256	0.440	
0.519	0.423	0.591	0.743	0.768	0.361	0.333	0.189	0.317	0.585	0.270	0.538	0.349	0.308	0.801	0.833	0.279	0.496	0.255	0.439	
0.523	0.424	0.591	0.746	0.772	0.364	0.334	0.190	0.318	0.587	0.272	0.538	0.347	0.307	0.801	0.834	0.278	0.496	0.257	0.439	
0.521	0.424	0.591	0.746	0.772	0.364	0.333	0.190	0.317	0.587	0.272	0.538	0.348	0.310	0.800	0.834	0.279	0.496	0.256	0.439	
0.524	0.423	0.591	0.744	0.772	0.364	0.335	0.190	0.319	0.586	0.272	0.537	0.348	0.307	0.803	0.833	0.279	0.495	0.256	0.439	
0.520	0.424	0.591	0.747	0.773	0.362	0.333	0.189	0.316	0.587	0.272	0.537	0.348	0.309	0.801	0.834	0.279	0.495	0.255	0.439	
0.521	0.424	0.590	0.745	0.769	0.363	0.334	0.190	0.316	0.586	0.272	0.538	0.349	0.310	0.799	0.833	0.279	0.495	0.256	0.440	
0.521	0.423	0.591	0.743	0.769	0.363	0.333	0.191	0.319	0.584	0.271	0.537	0.349	0.305	0.801	0.833	0.279	0.495	0.256	0.440	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.002	0.005	0.001	0.000	0.000	0.011	0.016	0.725	0.020	0.001	0.048	0.001	0.013	0.025	0.000	0.000	0.040	0.002	0.085	0.004	
LSPI converged after 4 iterations in 6.820 seconds.
MEAN LOSS: 0.474
MEAN RF POLICY* IN MDP_m (loss): 0.495

Loss matrix
0.846	0.346	0.492	0.564	0.518	0.370	0.394	0.228	0.298	0.481	0.228	0.403	0.374	0.234	0.975	0.624	0.351	0.389	0.361	0.416	
0.839	0.352	0.496	0.603	0.560	0.376	0.390	0.226	0.301	0.495	0.248	0.409	0.370	0.263	0.954	0.634	0.367	0.391	0.371	0.420	
0.839	0.353	0.500	0.610	0.567	0.376	0.392	0.225	0.301	0.494	0.249	0.410	0.369	0.263	0.962	0.636	0.366	0.391	0.368	0.421	
0.841	0.354	0.501	0.608	0.567	0.380	0.391	0.225	0.303	0.496	0.250	0.410	0.372	0.266	0.963	0.637	0.369	0.392	0.371	0.423	
0.844	0.353	0.500	0.612	0.567	0.377	0.393	0.225	0.304	0.493	0.251	0.410	0.371	0.266	0.961	0.636	0.371	0.391	0.374	0.423	
0.836	0.352	0.498	0.613	0.567	0.376	0.389	0.226	0.301	0.492	0.251	0.409	0.368	0.265	0.956	0.636	0.369	0.389	0.369	0.419	
0.838	0.353	0.498	0.611	0.568	0.379	0.391	0.226	0.304	0.495	0.252	0.409	0.371	0.267	0.954	0.635	0.371	0.392	0.372	0.422	
0.848	0.352	0.502	0.608	0.562	0.378	0.394	0.225	0.305	0.493	0.248	0.410	0.371	0.260	0.968	0.635	0.370	0.391	0.372	0.424	
0.847	0.353	0.501	0.608	0.564	0.378	0.393	0.225	0.303	0.492	0.250	0.409	0.372	0.266	0.965	0.635	0.370	0.391	0.372	0.424	
0.842	0.352	0.498	0.612	0.564	0.377	0.392	0.225	0.305	0.490	0.250	0.408	0.370	0.265	0.960	0.635	0.371	0.390	0.373	0.421	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.017	0.002	0.001	0.001	0.009	0.008	0.669	0.042	0.003	0.112	0.006	0.010	0.084	0.000	0.000	0.011	0.008	0.011	0.005	
LSPI converged after 5 iterations in 8.430 seconds.
MEAN LOSS: 0.468
MEAN RF POLICY* IN MDP_m (loss): 0.293

Loss matrix
0.787	0.307	0.488	0.543	0.558	0.467	0.361	0.245	0.290	0.548	0.231	0.363	0.415	0.254	0.924	0.564	0.368	0.406	0.394	0.437	
0.802	0.336	0.514	0.564	0.571	0.505	0.384	0.258	0.309	0.578	0.255	0.392	0.452	0.276	0.944	0.588	0.390	0.440	0.414	0.478	
0.805	0.337	0.512	0.568	0.575	0.507	0.386	0.259	0.313	0.577	0.258	0.392	0.453	0.283	0.941	0.588	0.392	0.440	0.416	0.479	
0.810	0.339	0.517	0.571	0.581	0.511	0.389	0.261	0.316	0.578	0.259	0.395	0.456	0.281	0.949	0.592	0.395	0.443	0.420	0.484	
0.807	0.340	0.517	0.566	0.575	0.514	0.390	0.261	0.316	0.579	0.258	0.395	0.458	0.283	0.950	0.591	0.393	0.443	0.418	0.485	
0.809	0.341	0.518	0.569	0.577	0.514	0.390	0.262	0.318	0.581	0.261	0.396	0.459	0.283	0.951	0.591	0.395	0.445	0.420	0.485	
0.809	0.339	0.515	0.571	0.580	0.507	0.389	0.261	0.316	0.578	0.260	0.394	0.455	0.284	0.944	0.591	0.396	0.440	0.421	0.484	
0.811	0.339	0.516	0.566	0.570	0.509	0.390	0.263	0.315	0.578	0.258	0.395	0.457	0.280	0.949	0.590	0.395	0.442	0.420	0.484	
0.801	0.337	0.513	0.565	0.570	0.506	0.387	0.260	0.312	0.578	0.257	0.392	0.453	0.280	0.941	0.588	0.391	0.439	0.416	0.478	
0.801	0.336	0.512	0.568	0.578	0.508	0.385	0.260	0.313	0.578	0.258	0.392	0.452	0.280	0.940	0.588	0.393	0.439	0.417	0.478	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.030	0.002	0.001	0.001	0.003	0.013	0.118	0.043	0.001	0.662	0.011	0.005	0.080	0.000	0.001	0.011	0.006	0.008	0.004	
LSPI converged after 4 iterations in 6.850 seconds.
MEAN LOSS: 0.779
MEAN RF POLICY* IN MDP_m (loss): 0.645

Loss matrix
0.939	0.395	0.672	0.862	0.892	0.662	0.422	0.244	0.529	0.610	0.342	0.477	0.536	0.378	1.200	0.692	0.489	0.508	0.422	0.678	
0.929	0.408	0.671	0.893	0.934	0.676	0.423	0.258	0.532	0.670	0.351	0.493	0.549	0.401	1.195	0.699	0.493	0.532	0.434	0.685	
0.926	0.410	0.667	0.891	0.934	0.673	0.422	0.260	0.528	0.671	0.350	0.494	0.549	0.399	1.190	0.697	0.491	0.532	0.433	0.682	
0.916	0.409	0.665	0.894	0.931	0.669	0.417	0.263	0.523	0.678	0.350	0.498	0.542	0.398	1.184	0.698	0.487	0.527	0.428	0.675	
0.926	0.408	0.667	0.889	0.929	0.670	0.420	0.259	0.526	0.677	0.350	0.494	0.546	0.398	1.192	0.698	0.490	0.531	0.431	0.678	
0.926	0.407	0.666	0.889	0.932	0.679	0.420	0.257	0.531	0.672	0.351	0.491	0.549	0.401	1.189	0.695	0.493	0.530	0.433	0.681	
0.930	0.409	0.670	0.892	0.935	0.677	0.423	0.258	0.532	0.676	0.351	0.494	0.549	0.400	1.197	0.698	0.493	0.532	0.434	0.683	
0.927	0.409	0.667	0.889	0.931	0.677	0.421	0.259	0.529	0.670	0.350	0.493	0.549	0.397	1.193	0.698	0.490	0.532	0.432	0.681	
0.936	0.411	0.676	0.891	0.936	0.683	0.427	0.258	0.537	0.671	0.355	0.496	0.552	0.402	1.203	0.702	0.497	0.536	0.440	0.687	
0.928	0.409	0.671	0.875	0.908	0.681	0.423	0.259	0.534	0.665	0.351	0.492	0.552	0.396	1.196	0.698	0.493	0.534	0.435	0.686	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.000	0.016	0.000	0.000	0.000	0.000	0.012	0.860	0.003	0.001	0.064	0.005	0.002	0.020	0.000	0.000	0.004	0.003	0.010	0.000	
LSPI converged after 5 iterations in 8.530 seconds.
MEAN LOSS: 0.461
MEAN RF POLICY* IN MDP_m (loss): 0.619

