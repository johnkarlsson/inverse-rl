Generating LSTDQ demonstrations...
2162 unique states in 5000 demonstrations...
Optimal weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.165	0.259	0.926	-0.030	0.091	0.140	0.117	-0.249	-1.376	-0.150	0.104	-0.469	0.009	
Random weights X(s,d,t,x,c,f) O(s,d,t,x,c,f) : 
0.053	0.334	1.029	0.002	0.052	0.176	-0.164	-0.315	-1.133	-0.080	0.088	-0.457	-0.018	
Sampling 20 reward functions...
	Sampled reward functions:
	 (0)	0.237	0.061	1.069	-0.182	0.041	-0.073	-0.273	-0.515	-1.215	0.318	0.050	2.176	-0.179	
	 (1)	0.044	0.494	2.244	0.343	-0.159	-0.037	0.064	-1.123	-1.397	0.111	-0.038	3.405	-0.442	
	 (2)	-0.086	0.126	1.371	0.286	-0.066	0.063	0.044	0.110	-0.758	0.000	-0.054	0.334	-0.045	
	 (3)	0.347	0.769	1.150	-0.185	-0.137	-1.751	-0.201	-0.631	-0.351	-0.161	0.045	0.763	-0.113	
	 (4)	0.100	0.183	0.836	-0.393	-0.130	0.197	-0.137	-0.209	-0.635	0.144	0.144	-1.115	-0.193	
	 (5)	0.442	0.306	1.670	-0.134	-0.402	0.006	-0.350	-0.121	-0.525	0.001	0.104	3.348	-0.471	
	 (6)	0.470	0.982	2.302	0.011	-0.526	-0.918	-0.415	-0.948	-1.110	0.172	0.152	5.209	-0.678	
	 (7)	-0.088	0.375	0.947	0.120	-0.113	-1.181	0.001	-0.127	-0.858	-0.059	0.050	2.946	-0.116	
	 (8)	-0.076	0.098	0.879	0.034	0.162	-3.457	0.050	-0.512	-1.406	0.112	0.073	2.584	0.233	
	 (9)	0.065	0.126	1.055	-0.114	-0.035	0.015	-0.060	-0.236	-0.771	0.128	-0.028	2.175	0.135	
	 (10)	-0.012	-0.205	1.328	0.137	0.085	0.517	0.034	-0.086	-0.533	-0.031	-0.166	0.656	0.087	
	 (11)	-0.079	0.393	1.506	0.163	0.067	0.028	-0.055	-0.489	-1.495	-0.036	0.297	-1.615	-0.048	
	 (12)	-0.020	0.144	1.745	-0.137	-0.037	0.794	0.055	-0.199	-0.904	0.211	-0.006	1.361	-0.060	
	 (13)	0.005	0.071	0.886	-0.061	0.113	0.379	0.020	-0.064	-1.003	-0.061	-0.082	-0.272	-0.100	
	 (14)	-0.097	0.155	1.229	0.269	0.108	1.770	-0.015	0.155	-0.848	0.048	-0.128	-2.137	0.051	
	 (15)	0.121	0.207	1.326	0.089	-0.034	0.121	-0.194	-0.190	-0.792	0.219	-0.030	1.160	-0.083	
	 (16)	-0.102	-0.027	1.284	0.035	0.075	0.421	0.042	0.083	-0.791	-0.091	-0.071	3.304	-0.027	
	 (17)	-0.080	0.127	0.703	0.037	0.051	-0.435	0.055	-0.007	-1.112	-0.380	0.066	-1.277	-0.011	
	 (18)	0.025	0.006	0.932	-0.219	0.005	0.793	-0.057	0.002	-1.054	0.035	0.044	-1.969	0.020	
	 (19)	0.084	0.340	0.710	-0.097	-0.162	-1.319	-0.111	-0.093	-1.249	0.075	0.139	1.922	-0.213	
LSPI converged after 5 iterations in 3.240 seconds.
Creating 20 corresponding optimal policies...
Calculating corresponding average win rates (vs. random)...
	True { score } and optimal weights:
	{ 0.970 }	0.201	0.303	0.908	-0.047	0.093	0.264	0.314	-0.224	-1.532	-0.332	0.111	-0.750	0.007	
	{ -0.130 }	 (random)
	{ -0.112 }	 (random random)
	{ score } and optimal weights of the optimal policies (from sampled reward functions):
 (0)	{ 0.887 }	0.579	0.375	0.703	-0.362	0.163	0.081	-0.127	-0.776	-2.118	0.061	0.291	0.758	-0.355	
 (1)	{ 0.962 }	0.461	1.038	2.045	0.047	-0.075	0.054	0.610	-1.391	-2.530	-0.699	0.140	1.303	-0.730	
 (2)	{ 0.781 }	0.190	0.517	1.184	0.146	0.010	0.183	0.512	-0.133	-1.576	-0.453	0.053	-0.727	-0.075	
 (3)	{ 0.919 }	0.998	0.961	0.981	-0.705	-0.141	-1.154	0.131	-0.875	-1.079	-0.760	0.214	-0.016	-0.143	
 (4)	{ 0.499 }	0.191	0.527	0.761	-0.272	-0.170	0.078	0.050	-0.392	-1.220	-0.122	0.335	-1.022	-0.351	
 (5)	{ 0.977 }	0.694	0.658	1.884	-0.386	-0.601	0.007	0.138	-0.284	-1.271	-0.134	0.304	2.032	-0.724	
 (6)	{ 0.947 }	0.838	1.442	2.501	-0.337	-0.738	-0.715	0.046	-1.066	-2.031	-0.045	0.406	3.154	-1.058	
 (7)	{ 0.524 }	0.078	0.497	0.820	-0.117	-0.111	-0.820	0.409	-0.184	-1.494	-0.248	0.207	1.421	-0.200	
 (8)	{ 0.920 }	0.394	0.242	0.211	-0.396	0.354	-2.284	0.226	-0.722	-2.406	-0.307	0.402	0.657	0.273	
 (9)	{ 0.950 }	0.319	0.414	0.826	-0.194	0.025	0.311	0.269	-0.431	-1.384	-0.245	0.079	0.836	0.246	
 (10)	{ 0.956 }	0.304	0.211	1.001	0.035	0.276	0.699	0.289	-0.400	-1.279	-0.439	-0.111	-0.296	0.120	
 (11)	{ 0.882 }	0.717	0.973	0.673	-0.062	0.258	0.273	0.692	-1.003	-3.497	-1.132	0.845	-2.892	-0.183	
 (12)	{ 0.776 }	0.364	0.702	1.411	-0.242	0.073	0.762	0.872	-0.467	-2.182	-0.615	0.178	-0.328	-0.089	
 (13)	{ 0.902 }	0.340	0.342	0.512	-0.175	0.328	0.472	0.451	-0.437	-1.911	-0.670	0.037	-1.317	-0.216	
 (14)	{ 0.742 }	0.608	0.655	0.761	0.198	0.368	1.880	0.714	-0.405	-2.109	-0.946	-0.045	-3.097	0.112	
 (15)	{ 0.859 }	0.476	0.614	1.070	-0.020	0.061	0.331	0.094	-0.442	-1.629	-0.124	0.112	-0.015	-0.149	
 (16)	{ 0.707 }	0.182	0.337	0.882	-0.040	0.270	0.528	0.644	-0.131	-1.907	-0.443	0.073	1.350	-0.086	
 (17)	{ 0.827 }	0.220	0.383	0.344	-0.052	0.170	-0.183	0.262	-0.375	-1.931	-0.709	0.292	-1.769	-0.083	
 (18)	{ 0.909 }	0.272	0.452	0.656	-0.075	0.093	0.740	0.266	-0.392	-1.877	-0.444	0.215	-2.289	0.017	
 (19)	{ 0.420 }	0.193	0.421	0.658	-0.335	-0.212	-1.070	0.286	-0.224	-1.831	-0.184	0.336	0.594	-0.363	
Scores for the experts true policies:
	{ 0.949 }	 (expert 0)
	{ 0.935 }	 (expert 1)
	{ 0.948 }	 (expert 2)
	{ 0.633 }	 (expert 3)
Summed loss of reward-optimal policies w.r.t. true rho values rho_m == MDP_m == V_m == (expert weights)
	0.471	1.021	0.651	0.771	0.409	1.093	1.300	0.743	0.960	0.495	0.447	2.210	1.438	0.708	1.919	0.599	1.216	0.505	0.702	0.687	
	0.410	1.019	0.671	0.766	0.482	0.994	1.211	0.633	0.909	0.452	0.491	2.255	1.454	0.773	1.984	0.593	1.163	0.593	0.819	0.606	
	0.776	1.330	1.030	1.067	0.435	1.313	1.525	0.963	1.183	0.804	0.766	2.545	1.791	1.026	2.238	0.973	1.548	0.733	0.938	0.841	
	0.494	1.133	0.749	0.812	0.521	1.020	1.291	0.701	0.914	0.592	0.569	2.315	1.550	0.829	2.046	0.694	1.250	0.610	0.874	0.609	
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.350	1.030	0.536	1.017	0.254	0.877	0.980	0.313	0.489	0.493	0.594	1.044	0.854	0.572	1.318	0.502	0.566	0.318	0.441	0.352	
0.368	1.108	0.639	1.026	0.261	0.990	1.055	0.326	0.497	0.550	0.696	0.885	0.925	0.629	1.316	0.597	0.652	0.283	0.451	0.348	
0.335	1.043	0.598	0.979	0.244	0.889	0.943	0.334	0.621	0.536	0.658	0.899	0.893	0.615	1.290	0.558	0.627	0.282	0.433	0.380	
0.318	0.996	0.544	0.943	0.230	0.826	0.830	0.302	0.548	0.501	0.636	0.876	0.845	0.600	1.248	0.500	0.610	0.266	0.414	0.349	
0.354	1.070	0.581	0.985	0.255	0.901	0.971	0.305	0.534	0.519	0.658	0.864	0.862	0.606	1.259	0.547	0.613	0.266	0.406	0.349	
0.307	0.937	0.532	0.946	0.240	0.821	0.821	0.300	0.532	0.501	0.610	0.856	0.835	0.589	1.248	0.500	0.588	0.261	0.412	0.358	
0.310	0.965	0.555	0.966	0.232	0.866	0.875	0.290	0.510	0.511	0.615	0.872	0.849	0.589	1.234	0.508	0.593	0.275	0.416	0.349	
0.310	0.952	0.538	0.980	0.249	0.860	0.874	0.298	0.586	0.493	0.612	0.809	0.809	0.579	1.207	0.495	0.584	0.257	0.405	0.350	
0.290	0.897	0.525	0.932	0.245	0.883	0.801	0.280	0.459	0.475	0.603	0.728	0.822	0.578	1.204	0.471	0.595	0.232	0.401	0.324	
0.301	0.966	0.557	0.919	0.234	0.847	0.844	0.316	0.570	0.492	0.624	0.834	0.843	0.592	1.246	0.502	0.598	0.271	0.416	0.365	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.043	0.000	0.002	0.000	0.726	0.000	0.000	0.057	0.003	0.004	0.001	0.000	0.000	0.001	0.000	0.003	0.001	0.116	0.013	0.029	
LSPI converged after 5 iterations in 3.440 seconds.
MEAN LOSS: 0.458
MEAN RF POLICY* LOSS IN MDP_m: 0.318
MEAN RF POLICY* SCORE IN MDP: 0.684

Loss matrix
0.411	0.932	0.444	0.839	0.286	0.742	0.836	0.461	0.775	0.525	0.470	1.086	0.843	0.514	1.152	0.440	0.610	0.315	0.404	0.520	
0.492	1.181	0.673	0.955	0.286	1.048	1.132	0.544	0.894	0.762	0.762	1.058	1.069	0.642	1.345	0.634	0.826	0.354	0.504	0.512	
0.476	1.154	0.667	0.951	0.253	1.051	1.123	0.527	0.863	0.774	0.773	0.989	1.043	0.613	1.330	0.634	0.804	0.332	0.487	0.512	
0.455	1.083	0.601	0.923	0.240	0.919	0.990	0.511	0.956	0.678	0.674	1.014	0.986	0.620	1.273	0.561	0.763	0.342	0.443	0.502	
0.483	1.122	0.639	0.989	0.255	1.051	1.127	0.542	0.920	0.749	0.720	1.029	1.014	0.590	1.277	0.614	0.775	0.326	0.466	0.534	
0.478	1.111	0.648	0.991	0.248	1.033	1.108	0.519	0.953	0.850	0.789	1.012	1.023	0.584	1.336	0.655	0.778	0.323	0.493	0.528	
0.487	1.122	0.635	0.986	0.269	1.020	1.118	0.537	0.954	0.766	0.735	1.033	1.021	0.607	1.293	0.622	0.783	0.346	0.478	0.514	
0.418	1.019	0.636	0.939	0.224	0.984	1.000	0.503	0.974	0.792	0.741	0.962	0.986	0.571	1.346	0.601	0.756	0.329	0.483	0.510	
0.481	1.148	0.696	0.980	0.257	1.090	1.158	0.533	0.883	0.853	0.826	1.008	1.066	0.599	1.373	0.678	0.816	0.332	0.521	0.510	
0.454	1.067	0.617	0.916	0.256	0.989	1.021	0.525	0.877	0.687	0.693	0.982	0.988	0.597	1.264	0.569	0.772	0.337	0.449	0.521	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.012	0.000	0.002	0.000	0.867	0.000	0.000	0.005	0.000	0.001	0.001	0.000	0.000	0.002	0.000	0.003	0.000	0.092	0.012	0.005	
LSPI converged after 4 iterations in 2.860 seconds.
MEAN LOSS: 0.498
MEAN RF POLICY* LOSS IN MDP_m: 0.444
MEAN RF POLICY* SCORE IN MDP: 0.617

Loss matrix
0.367	0.595	0.258	0.751	0.232	0.637	0.695	0.554	1.116	0.339	0.231	0.912	0.586	0.389	0.759	0.242	0.513	0.286	0.255	0.627	
0.370	0.709	0.451	0.767	0.232	0.819	0.778	0.598	1.246	0.595	0.559	0.761	0.731	0.393	0.830	0.363	0.640	0.274	0.330	0.586	
0.395	0.777	0.479	0.755	0.238	0.930	0.870	0.609	1.215	0.602	0.606	0.688	0.737	0.406	0.797	0.391	0.649	0.250	0.315	0.602	
0.367	0.754	0.450	0.724	0.235	0.853	0.806	0.602	1.198	0.577	0.555	0.717	0.732	0.440	0.875	0.366	0.640	0.262	0.326	0.592	
0.386	0.767	0.477	0.787	0.232	0.874	0.841	0.609	1.262	0.603	0.592	0.732	0.753	0.422	0.865	0.396	0.649	0.269	0.328	0.606	
0.409	0.755	0.443	0.810	0.232	0.829	0.824	0.611	1.332	0.609	0.557	0.788	0.738	0.412	0.861	0.377	0.632	0.270	0.332	0.606	
0.397	0.804	0.467	0.731	0.222	0.831	0.798	0.580	1.173	0.583	0.587	0.726	0.771	0.443	0.889	0.389	0.673	0.256	0.336	0.557	
0.389	0.761	0.436	0.735	0.246	0.795	0.780	0.579	1.174	0.593	0.538	0.814	0.767	0.414	0.860	0.362	0.648	0.268	0.354	0.559	
0.386	0.747	0.448	0.747	0.255	0.836	0.796	0.592	1.245	0.634	0.591	0.716	0.743	0.417	0.842	0.371	0.639	0.260	0.346	0.570	
0.400	0.752	0.433	0.773	0.228	0.820	0.804	0.606	1.286	0.564	0.546	0.732	0.718	0.425	0.859	0.371	0.629	0.265	0.312	0.607	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.017	0.000	0.013	0.000	0.728	0.000	0.000	0.001	0.000	0.003	0.014	0.000	0.000	0.012	0.000	0.028	0.001	0.130	0.051	0.001	
LSPI converged after 4 iterations in 2.840 seconds.
MEAN LOSS: 0.544
MEAN RF POLICY* LOSS IN MDP_m: 0.435
MEAN RF POLICY* SCORE IN MDP: 0.828

Loss matrix
0.522	1.218	0.571	0.985	0.549	1.021	1.312	0.610	0.776	0.661	0.535	1.107	1.066	0.634	1.225	0.536	0.704	0.385	0.618	0.634	
1.091	2.145	0.829	0.716	0.590	1.015	1.425	0.793	1.018	0.683	0.939	1.132	1.398	1.327	1.570	0.775	1.237	0.626	0.804	0.859	
1.167	2.136	0.810	0.791	0.609	1.031	1.499	0.848	1.175	0.687	0.883	1.235	1.374	1.314	1.520	0.789	1.211	0.667	0.807	0.957	
1.117	2.137	0.838	0.807	0.603	1.065	1.509	0.834	1.172	0.716	0.932	1.185	1.382	1.291	1.524	0.793	1.203	0.659	0.813	0.924	
1.087	2.054	0.823	0.788	0.595	0.988	1.403	0.820	1.121	0.720	0.909	1.209	1.397	1.293	1.564	0.775	1.215	0.655	0.822	0.899	
1.118	2.070	0.818	0.826	0.656	1.034	1.500	0.837	1.111	0.716	0.870	1.293	1.396	1.289	1.554	0.793	1.191	0.683	0.852	0.942	
1.082	2.008	0.774	0.778	0.628	0.990	1.417	0.804	1.132	0.693	0.837	1.255	1.342	1.240	1.495	0.741	1.138	0.663	0.827	0.919	
1.016	1.951	0.788	0.805	0.576	1.015	1.410	0.780	1.091	0.683	0.855	1.167	1.319	1.198	1.483	0.741	1.129	0.608	0.770	0.870	
1.040	1.982	0.779	0.789	0.585	0.993	1.400	0.787	1.068	0.680	0.848	1.185	1.329	1.228	1.501	0.738	1.143	0.624	0.785	0.872	
1.120	2.106	0.838	0.791	0.587	1.036	1.463	0.816	1.111	0.712	0.926	1.216	1.389	1.303	1.565	0.801	1.220	0.650	0.813	0.904	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.009	0.000	0.001	0.000	0.014	0.000	0.000	0.001	0.000	0.002	0.001	0.000	0.000	0.000	0.000	0.001	0.000	0.970	0.001	0.000	
LSPI converged after 5 iterations in 3.420 seconds.
MEAN LOSS: 0.608
MEAN RF POLICY* LOSS IN MDP_m: 0.595
MEAN RF POLICY* SCORE IN MDP: 0.826

LSPI converged after 4 iterations in 2.850 seconds.
~~~ MAGIC SCORE ~~~	0.846
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.307	0.826	0.430	0.844	0.186	0.690	0.751	0.267	0.477	0.416	0.485	0.915	0.708	0.496	1.130	0.411	0.482	0.284	0.371	0.300	
0.312	0.857	0.455	0.840	0.192	0.700	0.751	0.272	0.489	0.422	0.519	0.848	0.721	0.519	1.133	0.435	0.509	0.272	0.362	0.306	
0.316	0.877	0.467	0.856	0.192	0.724	0.779	0.277	0.543	0.429	0.527	0.853	0.730	0.517	1.133	0.449	0.508	0.274	0.365	0.318	
0.306	0.836	0.441	0.846	0.196	0.698	0.739	0.271	0.505	0.420	0.507	0.841	0.712	0.510	1.115	0.423	0.497	0.268	0.360	0.309	
0.318	0.881	0.467	0.854	0.194	0.716	0.778	0.283	0.484	0.429	0.524	0.860	0.732	0.522	1.139	0.443	0.520	0.275	0.368	0.307	
0.315	0.867	0.457	0.856	0.199	0.699	0.759	0.280	0.527	0.426	0.521	0.849	0.722	0.519	1.133	0.439	0.509	0.272	0.361	0.318	
0.318	0.871	0.453	0.841	0.193	0.710	0.762	0.274	0.520	0.422	0.521	0.847	0.724	0.515	1.129	0.435	0.504	0.270	0.363	0.312	
0.311	0.854	0.452	0.845	0.194	0.704	0.761	0.267	0.460	0.423	0.515	0.847	0.719	0.517	1.132	0.430	0.508	0.271	0.363	0.300	
0.309	0.871	0.462	0.849	0.185	0.702	0.763	0.279	0.515	0.430	0.519	0.864	0.732	0.518	1.135	0.443	0.507	0.275	0.366	0.313	
0.308	0.852	0.452	0.839	0.193	0.699	0.752	0.274	0.487	0.421	0.511	0.849	0.719	0.513	1.127	0.431	0.503	0.271	0.363	0.306	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.030	0.000	0.004	0.000	0.761	0.000	0.000	0.074	0.002	0.006	0.002	0.000	0.000	0.002	0.000	0.006	0.002	0.061	0.016	0.033	
LSPI converged after 4 iterations in 2.910 seconds.
MEAN LOSS: 0.462
MEAN RF POLICY* LOSS IN MDP_m: 0.331
MEAN RF POLICY* SCORE IN MDP: 0.606

Loss matrix
0.393	0.690	0.347	0.739	0.229	0.615	0.745	0.407	0.865	0.487	0.365	0.955	0.669	0.406	0.902	0.366	0.515	0.263	0.341	0.421	
0.341	0.672	0.412	0.724	0.224	0.678	0.730	0.420	0.883	0.583	0.446	0.842	0.703	0.450	0.947	0.397	0.558	0.256	0.361	0.399	
0.357	0.701	0.429	0.736	0.224	0.699	0.756	0.431	0.910	0.595	0.471	0.848	0.721	0.454	0.952	0.413	0.572	0.260	0.368	0.403	
0.337	0.675	0.421	0.714	0.228	0.683	0.723	0.425	0.862	0.576	0.450	0.843	0.716	0.450	0.946	0.399	0.569	0.259	0.366	0.403	
0.348	0.703	0.418	0.705	0.234	0.692	0.742	0.423	0.842	0.566	0.448	0.842	0.719	0.455	0.942	0.397	0.570	0.256	0.365	0.402	
0.345	0.680	0.411	0.708	0.238	0.668	0.721	0.415	0.843	0.558	0.433	0.874	0.714	0.455	0.954	0.393	0.564	0.264	0.370	0.398	
0.342	0.682	0.421	0.721	0.227	0.667	0.723	0.421	0.870	0.587	0.456	0.871	0.730	0.451	0.976	0.403	0.575	0.263	0.378	0.390	
0.346	0.679	0.412	0.713	0.233	0.685	0.743	0.431	0.867	0.575	0.444	0.857	0.715	0.442	0.943	0.398	0.567	0.258	0.367	0.409	
0.349	0.681	0.416	0.726	0.229	0.673	0.731	0.424	0.883	0.577	0.448	0.868	0.717	0.460	0.962	0.402	0.566	0.263	0.368	0.404	
0.351	0.704	0.432	0.721	0.233	0.713	0.767	0.437	0.880	0.590	0.469	0.841	0.728	0.455	0.950	0.413	0.577	0.260	0.371	0.410	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.036	0.000	0.010	0.000	0.744	0.000	0.000	0.008	0.000	0.001	0.006	0.000	0.000	0.006	0.000	0.011	0.002	0.143	0.021	0.011	
LSPI converged after 4 iterations in 2.880 seconds.
MEAN LOSS: 0.512
MEAN RF POLICY* LOSS IN MDP_m: 0.405
MEAN RF POLICY* SCORE IN MDP: 0.828

Loss matrix
0.337	0.576	0.299	0.644	0.224	0.582	0.648	0.398	0.759	0.364	0.307	0.832	0.615	0.410	0.840	0.316	0.536	0.243	0.319	0.434	
0.321	0.627	0.371	0.633	0.226	0.666	0.687	0.416	0.745	0.441	0.401	0.731	0.674	0.422	0.866	0.355	0.581	0.230	0.343	0.419	
0.322	0.630	0.377	0.641	0.225	0.681	0.701	0.420	0.767	0.449	0.407	0.730	0.670	0.421	0.854	0.359	0.577	0.236	0.342	0.427	
0.337	0.658	0.374	0.654	0.233	0.662	0.707	0.419	0.750	0.455	0.406	0.771	0.691	0.436	0.880	0.367	0.585	0.239	0.353	0.420	
0.323	0.623	0.371	0.647	0.227	0.677	0.701	0.423	0.776	0.448	0.402	0.737	0.668	0.415	0.844	0.354	0.575	0.234	0.341	0.429	
0.321	0.634	0.380	0.640	0.229	0.690	0.710	0.424	0.755	0.439	0.403	0.729	0.676	0.426	0.861	0.359	0.584	0.233	0.343	0.432	
0.320	0.611	0.370	0.645	0.219	0.667	0.684	0.419	0.766	0.444	0.402	0.724	0.661	0.417	0.854	0.353	0.576	0.236	0.339	0.427	
0.329	0.623	0.366	0.651	0.223	0.671	0.699	0.422	0.776	0.443	0.398	0.736	0.658	0.413	0.842	0.354	0.567	0.238	0.335	0.431	
0.329	0.637	0.371	0.652	0.223	0.692	0.719	0.424	0.760	0.441	0.398	0.734	0.663	0.416	0.836	0.356	0.575	0.236	0.335	0.432	
0.326	0.622	0.361	0.653	0.219	0.667	0.696	0.421	0.791	0.439	0.393	0.731	0.654	0.414	0.835	0.350	0.567	0.232	0.331	0.430	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.033	0.001	0.023	0.000	0.684	0.000	0.000	0.007	0.000	0.006	0.012	0.000	0.000	0.007	0.000	0.020	0.001	0.173	0.025	0.006	
LSPI converged after 5 iterations in 3.580 seconds.
MEAN LOSS: 0.555
MEAN RF POLICY* LOSS IN MDP_m: 0.429
MEAN RF POLICY* SCORE IN MDP: 0.842

Loss matrix
0.513	1.081	0.532	0.841	0.465	0.931	1.262	0.584	0.791	0.610	0.506	1.047	0.942	0.539	1.081	0.539	0.681	0.379	0.525	0.583	
0.637	1.202	0.628	0.778	0.540	0.960	1.268	0.664	0.910	0.639	0.558	1.151	1.030	0.739	1.218	0.593	0.798	0.539	0.686	0.754	
0.626	1.184	0.629	0.780	0.528	0.972	1.268	0.660	0.890	0.632	0.556	1.125	1.017	0.734	1.210	0.591	0.792	0.528	0.668	0.752	
0.635	1.210	0.636	0.794	0.526	0.987	1.296	0.669	0.914	0.643	0.563	1.130	1.029	0.728	1.203	0.600	0.799	0.520	0.664	0.755	
0.639	1.214	0.628	0.781	0.528	0.966	1.277	0.655	0.879	0.636	0.560	1.135	1.035	0.736	1.216	0.597	0.803	0.519	0.671	0.740	
0.651	1.250	0.643	0.801	0.529	0.990	1.312	0.669	0.906	0.651	0.578	1.139	1.049	0.749	1.222	0.613	0.814	0.528	0.670	0.748	
0.647	1.236	0.640	0.781	0.530	0.988	1.309	0.672	0.882	0.643	0.571	1.131	1.045	0.742	1.222	0.609	0.812	0.520	0.669	0.755	
0.635	1.210	0.633	0.778	0.527	0.995	1.301	0.666	0.885	0.635	0.559	1.121	1.031	0.734	1.210	0.599	0.804	0.519	0.663	0.751	
0.626	1.189	0.626	0.782	0.525	0.964	1.267	0.659	0.902	0.632	0.553	1.147	1.024	0.731	1.213	0.593	0.793	0.528	0.667	0.742	
0.633	1.189	0.629	0.768	0.531	0.978	1.280	0.668	0.894	0.633	0.550	1.133	1.028	0.732	1.209	0.593	0.799	0.529	0.671	0.759	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.002	0.000	0.002	0.000	0.020	0.000	0.000	0.001	0.000	0.002	0.009	0.000	0.000	0.001	0.000	0.004	0.000	0.955	0.002	0.001	
LSPI converged after 5 iterations in 3.530 seconds.
MEAN LOSS: 0.609
MEAN RF POLICY* LOSS IN MDP_m: 0.590
MEAN RF POLICY* SCORE IN MDP: 0.835

LSPI converged after 5 iterations in 3.530 seconds.
~~~ MAGIC SCORE ~~~	0.868
Generating 4 expert demonstrations...
Initializing 4 policy posteriors...
Sampling 10 policies from 4 experts...
TESTING WITH 0th POLICY = TRUE EXPERT POLICY
Initializing 4 BMT objects...
Loss matrix
0.281	0.735	0.399	0.807	0.200	0.660	0.675	0.270	0.500	0.395	0.456	0.871	0.679	0.480	1.115	0.379	0.461	0.264	0.358	0.324	
0.281	0.740	0.403	0.807	0.200	0.661	0.676	0.271	0.506	0.397	0.460	0.860	0.681	0.482	1.115	0.383	0.464	0.262	0.356	0.325	
0.283	0.741	0.403	0.808	0.201	0.663	0.679	0.270	0.498	0.397	0.460	0.862	0.682	0.482	1.117	0.383	0.465	0.262	0.357	0.324	
0.282	0.740	0.403	0.808	0.201	0.662	0.678	0.272	0.509	0.397	0.460	0.860	0.681	0.482	1.115	0.383	0.464	0.263	0.357	0.326	
0.281	0.739	0.403	0.805	0.201	0.663	0.676	0.272	0.508	0.396	0.461	0.857	0.680	0.482	1.115	0.382	0.464	0.262	0.356	0.326	
0.282	0.739	0.401	0.806	0.200	0.661	0.676	0.273	0.509	0.396	0.460	0.858	0.680	0.482	1.114	0.381	0.464	0.262	0.356	0.327	
0.281	0.737	0.403	0.807	0.199	0.661	0.675	0.270	0.504	0.397	0.460	0.862	0.682	0.482	1.117	0.383	0.465	0.262	0.357	0.323	
0.281	0.738	0.402	0.806	0.200	0.661	0.676	0.271	0.506	0.396	0.459	0.859	0.681	0.482	1.115	0.381	0.465	0.262	0.356	0.326	
0.281	0.737	0.402	0.806	0.201	0.660	0.675	0.271	0.503	0.396	0.459	0.859	0.680	0.482	1.115	0.381	0.464	0.262	0.356	0.325	
0.283	0.742	0.405	0.808	0.200	0.666	0.681	0.271	0.505	0.398	0.462	0.862	0.684	0.483	1.118	0.384	0.466	0.263	0.358	0.324	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.048	0.000	0.007	0.000	0.729	0.001	0.000	0.058	0.002	0.008	0.004	0.000	0.000	0.003	0.000	0.010	0.003	0.084	0.015	0.026	
LSPI converged after 5 iterations in 4.010 seconds.
MEAN LOSS: 0.461
MEAN RF POLICY* LOSS IN MDP_m: 0.313
MEAN RF POLICY* SCORE IN MDP: 0.696

Loss matrix
0.359	0.590	0.307	0.703	0.229	0.565	0.675	0.378	0.846	0.443	0.321	0.895	0.592	0.365	0.836	0.328	0.463	0.239	0.311	0.400	
0.354	0.585	0.319	0.707	0.228	0.577	0.677	0.382	0.866	0.465	0.336	0.877	0.600	0.371	0.847	0.337	0.471	0.238	0.316	0.399	
0.352	0.582	0.317	0.707	0.230	0.578	0.678	0.383	0.867	0.467	0.333	0.874	0.598	0.368	0.841	0.334	0.467	0.236	0.315	0.400	
0.348	0.581	0.318	0.700	0.228	0.577	0.671	0.379	0.854	0.465	0.333	0.870	0.598	0.369	0.842	0.334	0.468	0.236	0.316	0.396	
0.354	0.588	0.317	0.704	0.230	0.579	0.682	0.383	0.860	0.467	0.335	0.876	0.601	0.368	0.843	0.337	0.470	0.237	0.317	0.399	
0.349	0.582	0.318	0.702	0.229	0.578	0.675	0.381	0.855	0.467	0.334	0.872	0.599	0.368	0.843	0.335	0.469	0.236	0.316	0.397	
0.350	0.581	0.316	0.702	0.228	0.576	0.672	0.380	0.856	0.461	0.331	0.873	0.597	0.368	0.841	0.332	0.468	0.236	0.314	0.398	
0.350	0.581	0.318	0.704	0.229	0.576	0.672	0.381	0.860	0.469	0.335	0.875	0.600	0.369	0.844	0.335	0.470	0.237	0.317	0.397	
0.352	0.585	0.318	0.700	0.229	0.579	0.677	0.382	0.860	0.468	0.337	0.869	0.600	0.368	0.842	0.336	0.470	0.235	0.316	0.398	
0.351	0.583	0.318	0.700	0.230	0.578	0.675	0.382	0.858	0.460	0.333	0.873	0.598	0.370	0.842	0.334	0.469	0.237	0.315	0.400	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.016	0.001	0.037	0.000	0.667	0.001	0.000	0.011	0.000	0.004	0.022	0.000	0.001	0.013	0.000	0.021	0.004	0.162	0.031	0.009	
LSPI converged after 5 iterations in 3.930 seconds.
MEAN LOSS: 0.530
MEAN RF POLICY* LOSS IN MDP_m: 0.386
MEAN RF POLICY* SCORE IN MDP: 0.826

Loss matrix
0.366	0.599	0.284	0.621	0.261	0.586	0.674	0.414	0.701	0.371	0.288	0.860	0.629	0.397	0.794	0.313	0.544	0.248	0.329	0.438	
0.362	0.602	0.295	0.622	0.261	0.597	0.678	0.417	0.700	0.386	0.300	0.843	0.637	0.396	0.792	0.319	0.549	0.246	0.332	0.436	
0.362	0.603	0.296	0.622	0.260	0.600	0.682	0.418	0.701	0.386	0.301	0.839	0.634	0.394	0.791	0.320	0.547	0.245	0.331	0.438	
0.362	0.605	0.297	0.623	0.260	0.600	0.682	0.418	0.704	0.386	0.302	0.840	0.637	0.396	0.792	0.320	0.550	0.245	0.332	0.437	
0.361	0.605	0.297	0.619	0.261	0.599	0.679	0.416	0.693	0.385	0.302	0.843	0.639	0.397	0.797	0.320	0.550	0.245	0.333	0.435	
0.361	0.603	0.296	0.621	0.261	0.597	0.678	0.416	0.696	0.384	0.300	0.845	0.639	0.399	0.799	0.320	0.551	0.246	0.334	0.436	
0.359	0.598	0.294	0.620	0.260	0.596	0.674	0.417	0.703	0.383	0.299	0.839	0.635	0.395	0.792	0.317	0.549	0.246	0.331	0.437	
0.364	0.605	0.296	0.624	0.260	0.598	0.681	0.417	0.698	0.387	0.301	0.846	0.638	0.397	0.795	0.320	0.549	0.246	0.333	0.436	
0.362	0.604	0.297	0.620	0.261	0.600	0.680	0.417	0.698	0.384	0.301	0.841	0.638	0.397	0.796	0.320	0.550	0.246	0.333	0.436	
0.362	0.604	0.296	0.620	0.261	0.597	0.679	0.417	0.696	0.385	0.301	0.843	0.638	0.397	0.796	0.320	0.550	0.246	0.333	0.436	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.016	0.001	0.050	0.001	0.108	0.001	0.000	0.008	0.000	0.012	0.043	0.000	0.001	0.010	0.000	0.031	0.002	0.686	0.025	0.006	
LSPI converged after 5 iterations in 4.020 seconds.
MEAN LOSS: 0.741
MEAN RF POLICY* LOSS IN MDP_m: 0.630
MEAN RF POLICY* SCORE IN MDP: 0.924

Loss matrix
0.456	0.903	0.464	0.767	0.452	0.798	1.081	0.525	0.712	0.527	0.437	1.013	0.814	0.478	0.985	0.461	0.576	0.340	0.488	0.559	
0.453	0.897	0.481	0.735	0.444	0.788	1.040	0.523	0.711	0.526	0.462	0.997	0.825	0.522	1.024	0.467	0.608	0.356	0.512	0.563	
0.452	0.895	0.481	0.731	0.444	0.785	1.033	0.519	0.705	0.524	0.460	0.996	0.822	0.523	1.026	0.465	0.607	0.357	0.513	0.557	
0.454	0.901	0.484	0.735	0.442	0.789	1.037	0.521	0.711	0.529	0.461	0.999	0.827	0.527	1.029	0.469	0.610	0.360	0.516	0.557	
0.448	0.887	0.483	0.730	0.442	0.779	1.019	0.517	0.708	0.523	0.468	0.996	0.820	0.529	1.029	0.465	0.609	0.358	0.514	0.556	
0.460	0.907	0.482	0.734	0.446	0.789	1.043	0.523	0.715	0.526	0.459	1.001	0.825	0.524	1.024	0.467	0.608	0.359	0.513	0.565	
0.454	0.899	0.483	0.733	0.443	0.781	1.031	0.519	0.708	0.525	0.465	0.999	0.824	0.524	1.028	0.467	0.608	0.357	0.514	0.559	
0.453	0.894	0.484	0.734	0.442	0.779	1.025	0.519	0.711	0.528	0.466	1.000	0.824	0.529	1.031	0.468	0.610	0.358	0.515	0.556	
0.454	0.897	0.482	0.734	0.445	0.785	1.036	0.523	0.708	0.526	0.462	1.001	0.825	0.525	1.026	0.467	0.609	0.358	0.513	0.562	
0.453	0.900	0.482	0.737	0.448	0.793	1.046	0.524	0.703	0.526	0.461	1.000	0.826	0.521	1.024	0.467	0.607	0.356	0.512	0.562	
Reward function probabilities:
0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	
0.013	0.000	0.007	0.000	0.016	0.000	0.000	0.004	0.001	0.004	0.017	0.000	0.000	0.004	0.000	0.009	0.002	0.916	0.005	0.003	
LSPI converged after 5 iterations in 4.270 seconds.
MEAN LOSS: 0.612
MEAN RF POLICY* LOSS IN MDP_m: 0.576
MEAN RF POLICY* SCORE IN MDP: 0.826

LSPI converged after 5 iterations in 6.940 seconds.
~~~ MAGIC SCORE ~~~	0.869
